
import os
import numpy as np
import cv2

# ----------------------- é…ç½® -----------------------
yolo_net_cfg = r"C:\Users\DELL\Desktop\æ•°æ®ç­›é€‰\yolov3.cfg"
yolo_net_weights = r"C:\Users\DELL\Desktop\æ•°æ®ç­›é€‰\yolov3.weights"
yolo_net_names = r"C:\Users\DELL\Desktop\æ•°æ®ç­›é€‰\coco.names"
video_path = r"X:\è§†é¢‘æ ·ä¾‹\ä¸€èˆ¬ç±»\å…¶ä»–ç±»\è¡—å¤´è¡¨æ¼”\354852_segment_1.mp4"
# ----------------------------------------------------

# æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
print("æ­£åœ¨æ£€æŸ¥è·¯å¾„...")
for path in [yolo_net_cfg, yolo_net_weights, yolo_net_names, video_path]:
    if not os.path.isfile(path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        exit(1)
print("âœ… æ‰€æœ‰æ–‡ä»¶è·¯å¾„æ­£ç¡®")

# åŠ è½½ YOLO æ¨¡å‹
try:
    net = cv2.dnn.readNet(yolo_net_weights, yolo_net_cfg)
except cv2.error as e:
    print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    exit(1)

# è·å–è¾“å‡ºå±‚
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# åŠ è½½ COCO ç±»åˆ«
with open(yolo_net_names, 'r') as f:
    classes = [line.strip() for line in f.readlines()]

def generate_video_description(video_path):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ï¼š{video_path}")
        return "è§†é¢‘è¯»å–å¤±è´¥"

    frame_count = 0
    object_counts = {}

    print("ğŸš€ å¼€å§‹è§†é¢‘åˆ†æ...")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        height, width, _ = frame.shape
        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
        net.setInput(blob)
        outputs = net.forward(output_layers)

        detected_objects = []
        for out in outputs:
            for detection in out:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                if confidence > 0.5:
                    detected_objects.append(classes[class_id])

        for obj in detected_objects:
            object_counts[obj] = object_counts.get(obj, 0) + 1

        frame_count += 1
        if frame_count % 30 == 0:
            print(f"å·²å¤„ç†å¸§æ•°: {frame_count}")

    cap.release()

    # ç”Ÿæˆç»Ÿè®¡æè¿°
    description = f"è¯¥è§†é¢‘å…±æœ‰ {frame_count} å¸§ï¼Œæ€»å…±æ£€æµ‹åˆ°ä»¥ä¸‹ç‰©ä½“ï¼š"
    for obj, count in object_counts.items():
        description += f"\n- {obj}: {count} æ¬¡"

    # æ·»åŠ è‡ªç„¶è¯­è¨€æ€»ç»“
    description += "\n\nç®€è¦æè¿°ï¼š\n"
    if not object_counts:
        description += "è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ç‰©ä½“ï¼Œå¯èƒ½ä¸ºé™æ€æˆ–ä½äº®åº¦åœºæ™¯ã€‚"
    else:
        top_objs = sorted(object_counts.items(), key=lambda x: -x[1])[:3]
        summary = "è§†é¢‘ä¸­ä¸»è¦åŒ…å« "
        summary += "ã€".join([f"{obj}ï¼ˆå‡ºç°çº¦{cnt}æ¬¡ï¼‰" for obj, cnt in top_objs])
        summary += " ç­‰å†…å®¹ã€‚"
        description += summary

    return description

# ç”Ÿæˆå¹¶æ‰“å°è§†é¢‘æè¿°
desc = generate_video_description(video_path)
print("\nğŸ“ è§†é¢‘å†…å®¹åˆ†æç»“æœï¼š\n")
print(desc)
