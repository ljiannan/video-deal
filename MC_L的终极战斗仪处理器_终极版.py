# _*_ coding: utf-8 _*_
"""
MC_Lçš„ç»ˆææˆ˜æ–—ä»ª - NASæé™ä¼˜åŒ–ç‰ˆ
æ•´åˆæ‰¹é‡åˆ‡å¤´å°¾2.1.pyå’Œæ‰¹é‡è£å‰ª2.1.pyçš„æ‰€æœ‰åŠŸèƒ½
æ”¯æŒï¼šåˆ‡å¤´å°¾ + è£å‰ª + ç¼©æ”¾ + ç¡¬ä»¶åŠ é€Ÿ + ROIé€‰æ‹© + æ–­ç‚¹ç»­ä¼ 

ğŸš€ MC_Lçš„é¡¶å°–ä¼˜åŒ–ç­–ç•¥ï¼š
- æ™ºèƒ½æœ¬åœ°ç¼“å­˜ç³»ç»Ÿ (500TB+ NASä¼˜åŒ–)
- å¼‚æ­¥é¢„è¯»å–å’Œæ–­ç‚¹ç»­ä¼ 
- i9å¤„ç†å™¨æè‡´æ€§èƒ½è°ƒä¼˜
- ç½‘ç»œI/Oç“¶é¢ˆçªç ´
- å†…å­˜å’Œå­˜å‚¨æ™ºèƒ½ç®¡ç†

åŸºäºæ‰¹é‡åˆ‡å¤´å°¾2.1.pyå’Œæ‰¹é‡è£å‰ª2.1.pyæ•´åˆä¼˜åŒ–
ç‰ˆæœ¬: MC_Lçš„NAS Extreme V2.0
æ—¥æœŸ: 2025å¹´
"""

# ==================== START: ç”¨æˆ·é…ç½®åŒºåŸŸ ====================
# !!! è¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µä¿®æ”¹ä»¥ä¸‹é…ç½® !!!

# --- å¤„ç†äººå‘˜é…ç½® ---
PROCESSOR_NAME = "Ljn"  # å¤„ç†äººåç§°ï¼Œå¯ä»¥ä¿®æ”¹ä¸ºä½ çš„åå­—
COMPUTER_NAME = "å¤§09"  # ç”µè„‘å·ï¼Œç•™ç©ºå°†è‡ªåŠ¨è·å–
COMPUTER_IP = ""   # ç”µè„‘IPï¼Œç•™ç©ºå°†è‡ªåŠ¨è·å–

# --- FFmpeg è·¯å¾„é…ç½® ---
FFMPEG_PATH = r"D:\ffmpeg-master-latest-win64-gpl\ffmpeg-master-latest-win64-gpl\bin\ffmpeg.exe"
FFPROBE_PATH = r"D:\ffmpeg-master-latest-win64-gpl\ffmpeg-master-latest-win64-gpl\bin\ffprobe.exe"

# --- è¾“å…¥è¾“å‡ºè·¯å¾„é…ç½® ---
INPUT_DIR = r"Z:\aé¡¹ç›®\èˆªæ‹ç‰¹å†™\æå»ºæ¥ \9.22\é‡è¥"
OUTPUT_DIR = r"D:\9.22é‡è¥"

# --- è¿›åº¦è®°å½•é…ç½® ---
PROGRESS_FOLDER = r"Z:\personal_folder\L\å¤„ç†å®Œæ•°æ®è®°å½•"

# --- åŠŸèƒ½å¼€å…³é…ç½® ---
ENABLE_HEAD_TAIL_CUT = True   # å¯ç”¨åˆ‡å¤´å°¾åŠŸèƒ½
ENABLE_CROPPING = True        # å¯ç”¨è£å‰ªåŠŸèƒ½

# --- åˆ‡å¤´å°¾æ—¶é—´é…ç½® ---
HEAD_CUT_TIME = 42    # ç‰‡å¤´æ—¶é—´ï¼ˆç§’ï¼‰
TAIL_CUT_TIME = 42    # ç‰‡å°¾æ—¶é—´ï¼ˆç§’ï¼‰

# --- è£å‰ªé…ç½® ---
TARGET_RESOLUTION = (1920, 1080)  # ç›®æ ‡åˆ†è¾¨ç‡ (å¿…é¡»æ˜¯16:9æ¯”ä¾‹)

# --- ä½åˆ†è¾¨ç‡è§†é¢‘è·³è¿‡é…ç½® ---
SKIP_LOW_RESOLUTION_VIDEOS = True
MIN_RESOLUTION_WIDTH = 1920
SKIP_VIDEOS_MOVE_DIR = r"Z:\personal_folder\L\æµ‹è¯•\è·³è¿‡çš„ä½åˆ†è¾¨ç‡è§†é¢‘"

# --- æ”¯æŒçš„è§†é¢‘æ ¼å¼ ---
SUPPORTED_VIDEO_FORMATS = ['.mp4', '.mov', '.avi', '.mkv', '.wmv', '.flv', '.webm', '.ts', '.m4v', '.3gp', '.f4v']

# --- ç¡¬ä»¶é…ç½® ---
MAX_PARALLEL_WORKERS_OVERRIDE = 0
MAX_PARALLEL_WORKERS = 16  # ç¿»å€ä¼˜åŒ–å¹¶è¡Œå¤„ç†æ•°é‡
QUALITY_MODE = 'highest'
AUTO_BITRATE = True
VIDEO_BITRATE = "10M"
MAX_BITRATE = "20M"
BUFFER_SIZE = "20M"

# --- NASæé™ä¼˜åŒ–é…ç½® (2025å¹´é¡¶å°–ç­–ç•¥) ---
# æœ¬åœ°ç¼“å­˜é…ç½® - ğŸ”§ ä¸´æ—¶ç¦ç”¨ä»¥è§£å†³å¡æ­»é—®é¢˜
LOCAL_CACHE_DIR = r"D:\NAS_VideoCache"  # æœ¬åœ°SSDç¼“å­˜ç›®å½•
MAX_CACHE_SIZE_GB = 200  # æœ€å¤§ç¼“å­˜å¤§å°(GB) - æ ¹æ®æœ¬åœ°SSDç©ºé—´è°ƒæ•´
PRELOAD_COUNT = 0  # ğŸ”§ ç¦ç”¨é¢„åŠ è½½ä»¥é¿å…å¡æ­»
ASYNC_DOWNLOAD_THREADS = 8  # å¼‚æ­¥ä¸‹è½½çº¿ç¨‹æ•°

# ğŸ”§ ç·Šæ€¥ä¿®å¾©ï¼šç¦ç”¨ç·©å­˜åŠŸèƒ½ä»¥è§£æ±ºå¡æ­»å•é¡Œ
ENABLE_CACHE = False  # è¨­ç‚ºFalseä¾†ç¦ç”¨ç·©å­˜

# ç½‘ç»œä¼˜åŒ–é…ç½® - å·²æ¸…ç†æœªä½¿ç”¨çš„é…ç½®

# i9æ€§èƒ½ä¼˜åŒ–é…ç½®
ENABLE_I9_TURBO = True  # å¯ç”¨i9æ¶¡è½®ä¼˜åŒ–
CPU_AFFINITY_OPTIMIZATION = True  # CPUäº²å’Œæ€§ä¼˜åŒ–

# å­˜å‚¨ä¼˜åŒ–é…ç½®
TEMP_PROCESSING_DIR = r"D:\VideoProcessing_Temp"  # ä¸´æ—¶å¤„ç†ç›®å½•
AUTO_CLEANUP_CACHE = True  # è‡ªåŠ¨æ¸…ç†ç¼“å­˜
MONITOR_DISK_SPACE = True  # ç›‘æ§ç£ç›˜ç©ºé—´
MIN_FREE_SPACE_GB = 50  # æœ€å°å‰©ä½™ç©ºé—´(GB)
MEMORY_POOL_SIZE_GB = 8  # å†…å­˜æ± å¤§å°(GB)

# --- æ•°æ®åº“é…ç½® (å¤šç”µè„‘åä½œ) ---
MYSQL_CONFIG = {
    'host': '192.168.10.70',
    'user': 'root',
    'password': 'zq828079',
    'database': 'data_sql',
}

# å¯ç”¨å¤šç”µè„‘åä½œåŠŸèƒ½
ENABLE_MULTI_COMPUTER_SYNC = True  # è®¾ä¸ºTrueå¯ç”¨æ•°æ®åº“åŒæ­¥

# ===================== END: ç”¨æˆ·é…ç½®åŒºåŸŸ =====================

import time
import os
import subprocess
import logging
import json
import re
import concurrent.futures
import threading
import psutil
import platform
import hashlib
import uuid
from collections import deque
import socket
import numpy as np
import cv2
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any, Set
from tqdm import tqdm
from datetime import datetime
import multiprocessing
import queue
from dataclasses import dataclass, field
from collections import deque, defaultdict
import mysql.connector
from mysql.connector import Error, pooling
from loguru import logger
import enum
import signal
import atexit
import shutil
from threading import Lock, Event, Thread, RLock, Condition

# ==================== ç¼–ç é…ç½® ====================
os.environ['PYTHONIOENCODING'] = 'utf-8'
if platform.system() == "Windows":
    os.environ['PYTHONUTF8'] = '1'

# ==================== æµæ°´çº¿çŠ¶æ€æšä¸¾ ====================
class PipelineStage(enum.Enum):
    """æµæ°´çº¿é˜¶æ®µçŠ¶æ€"""
    PENDING = "pending"        # ç­‰å¾…å¤„ç†
    CACHING = "caching"        # æ­£åœ¨ç¼“å­˜
    CACHED = "cached"          # ç¼“å­˜å®Œæˆ
    CHECKING = "checking"      # æ­£åœ¨æŸ¥é‡
    DUPLICATE = "duplicate"    # å‘ç°é‡å¤
    PROCESSING = "processing"  # æ­£åœ¨å¤„ç†
    COMPLETED = "completed"    # å¤„ç†å®Œæˆ
    FAILED = "failed"          # å¤„ç†å¤±è´¥
    TIMEOUT = "timeout"        # è¶…æ—¶å¤±è´¥
    CANCELLED = "cancelled"    # å·²å–æ¶ˆ

@dataclass
class PipelineTask:
    """æµæ°´çº¿ä»»åŠ¡å•å…ƒ"""
    video_path: str
    task_id: str
    stage: PipelineStage = PipelineStage.PENDING
    cache_path: Optional[str] = None
    hash_value: Optional[str] = None
    error_msg: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3
    created_time: float = field(default_factory=time.time)
    stage_start_time: float = field(default_factory=time.time)
    timeout_seconds: Dict[PipelineStage, float] = field(default_factory=lambda: {
        PipelineStage.CACHING: 7200.0,    # 2å°æ—¶ç¼“å­˜è¶…æ—¶ï¼ˆå¤§æ–‡ä»¶éœ€è¦æ›´å¤šæ—¶é—´ï¼‰
        PipelineStage.CHECKING: 600.0,    # 10åˆ†é’ŸæŸ¥é‡è¶…æ—¶ï¼ˆå¢åŠ å®¹é”™ï¼‰
        PipelineStage.PROCESSING: 7200.0  # 2å°æ—¶å¤„ç†è¶…æ—¶ï¼ˆ4Kè§†é¢‘éœ€è¦æ›´å¤šæ—¶é—´ï¼‰
    })
    
    def is_timeout(self) -> bool:
        """æ£€æŸ¥å½“å‰é˜¶æ®µæ˜¯å¦è¶…æ—¶ - æ™ºèƒ½è¶…æ—¶æœºåˆ¶"""
        if self.stage in self.timeout_seconds:
            elapsed = time.time() - self.stage_start_time
            base_timeout = self.timeout_seconds[self.stage]
            
            # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
            try:
                if os.path.exists(self.video_path):
                    file_size_gb = os.path.getsize(self.video_path) / (1024**3)
                    # æ¯GBå¢åŠ 30åˆ†é’Ÿè¶…æ—¶æ—¶é—´ï¼ˆæœ€å¤šé¢å¤–å¢åŠ 4å°æ—¶ï¼‰
                    size_bonus = min(file_size_gb * 1800, 14400)  
                    adjusted_timeout = base_timeout + size_bonus
                    return elapsed > adjusted_timeout
            except Exception:
                pass  # å¦‚æœè·å–æ–‡ä»¶å¤§å°å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è¶…æ—¶
            
            return elapsed > base_timeout
        return False
    
    def update_stage(self, new_stage: PipelineStage, error_msg: str = None):
        """æ›´æ–°ä»»åŠ¡é˜¶æ®µ"""
        self.stage = new_stage
        self.stage_start_time = time.time()
        if error_msg:
            self.error_msg = error_msg
    
    def can_retry(self) -> bool:
        """æ˜¯å¦å¯ä»¥é‡è¯•"""
        return self.retry_count < self.max_retries
    
    def increment_retry(self):
        """å¢åŠ é‡è¯•æ¬¡æ•°"""
        self.retry_count += 1

class VideoPipelineManager:
    """è§†é¢‘å¤„ç†æµæ°´çº¿ç®¡ç†å™¨ - é˜²å¡æ­»åŠ å›ºç‰ˆ"""
    
    def __init__(self, max_concurrent_cache: int = 6, max_concurrent_check: int = 4, max_concurrent_process: int = 4):
        # é˜Ÿåˆ—ç³»ç»Ÿ
        self.pending_queue = queue.Queue()           # å¾…ç¼“å­˜é˜Ÿåˆ—
        self.cache_queue = queue.Queue()             # ç¼“å­˜å®Œæˆé˜Ÿåˆ—ï¼ˆå¾…æŸ¥é‡ï¼‰
        self.process_queue = queue.Queue()           # æŸ¥é‡å®Œæˆé˜Ÿåˆ—ï¼ˆå¾…å¤„ç†ï¼‰
        self.completed_queue = queue.Queue()         # å®Œæˆé˜Ÿåˆ—
        self.failed_queue = queue.Queue()            # å¤±è´¥é˜Ÿåˆ—
        
        # ä»»åŠ¡è·Ÿè¸ª
        self.tasks: Dict[str, PipelineTask] = {}     # æ‰€æœ‰ä»»åŠ¡
        self.active_tasks: Dict[PipelineStage, Set[str]] = defaultdict(set)  # æ´»è·ƒä»»åŠ¡åˆ†ç»„
        
        # å¹¶å‘æ§åˆ¶
        self.max_concurrent_cache = max_concurrent_cache
        self.max_concurrent_check = max_concurrent_check  
        self.max_concurrent_process = max_concurrent_process
        
        # çº¿ç¨‹æ§åˆ¶
        self.cache_threads: List[Thread] = []
        self.check_threads: List[Thread] = []
        self.process_threads: List[Thread] = []
        self.monitor_thread: Optional[Thread] = None
        
        # åŒæ­¥æ§åˆ¶
        self.task_lock = RLock()
        self.shutdown_event = Event()
        self.pipeline_condition = Condition()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'duplicate_tasks': 0,
            'cache_hits': 0,
            'avg_cache_time': 0.0,
            'avg_check_time': 0.0,
            'avg_process_time': 0.0
        }
        
        # é”™è¯¯æ¢å¤
        self.error_recovery_enabled = True
        self.deadlock_detection_enabled = True
        self.last_activity_time = time.time()
        
        # æ³¨å†Œé€€å‡ºå¤„ç†
        atexit.register(self.cleanup)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logging.info("ğŸš€ è§†é¢‘å¤„ç†æµæ°´çº¿ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        logging.info(f"ğŸ“¡ æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
        self.shutdown()
    
    def bulk_database_check(self, video_paths: List[str], progress_manager) -> Tuple[List[str], List[Dict]]:
        """æ‰¹é‡æ•°æ®åº“å¿«é€Ÿæ£€æŸ¥ - åœ¨ç¼“å­˜å‰è¿‡æ»¤å·²å¤„ç†è§†é¢‘"""
        videos_to_process = []
        skipped_videos = []
        
        if not progress_manager or not hasattr(progress_manager, 'video_record_manager'):
            return video_paths, []
            
        video_record_manager = progress_manager.video_record_manager
        if not video_record_manager or not video_record_manager.db_manager.is_available():
            return video_paths, []
        
        logging.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ•°æ®åº“å¿«é€Ÿæ£€æŸ¥ ({len(video_paths)} ä¸ªè§†é¢‘)...")
        
        for video_path in video_paths:
            try:
                is_processed, db_record = video_record_manager.quick_database_check_before_cache(video_path)
                if is_processed:
                    video_name = os.path.basename(video_path)
                    processor = db_record.get('computer_name', 'unknown') if db_record else 'unknown'
                    logging.info(f"ğŸ” è·³è¿‡å·²å¤„ç†è§†é¢‘: {video_name} (å¤„ç†è€…: {processor})")
                    skipped_videos.append({
                        'video_path': video_path,
                        'video_name': video_name,
                        'processor': processor,
                        'record': db_record
                    })
                    # ç›´æ¥æ ‡è®°ä¸ºå·²å®Œæˆ
                    progress_manager.mark_completed(video_path, db_record.get('output_path', '') if db_record else '', 0)
                else:
                    videos_to_process.append(video_path)
                    
            except Exception as e:
                logging.warning(f"âš ï¸ æ•°æ®åº“æ£€æŸ¥å¤±è´¥ï¼Œå°†ç»§ç»­å¤„ç†: {os.path.basename(video_path)} - {e}")
                videos_to_process.append(video_path)
        
        logging.info(f"âœ… æ‰¹é‡æ•°æ®åº“æ£€æŸ¥å®Œæˆ: è·³è¿‡ {len(skipped_videos)} ä¸ªï¼Œéœ€å¤„ç† {len(videos_to_process)} ä¸ª")
        return videos_to_process, skipped_videos

    def add_task(self, video_path: str) -> str:
        """æ·»åŠ ä»»åŠ¡åˆ°æµæ°´çº¿"""
        task_id = f"{int(time.time() * 1000)}_{hash(video_path) % 10000}"
        task = PipelineTask(video_path=video_path, task_id=task_id)
        
        with self.task_lock:
            self.tasks[task_id] = task
            self.active_tasks[PipelineStage.PENDING].add(task_id)
            self.pending_queue.put(task_id)
            self.stats['total_tasks'] += 1
            
        with self.pipeline_condition:
            self.pipeline_condition.notify_all()
            
        logging.debug(f"ğŸ“ ä»»åŠ¡å·²æ·»åŠ : {os.path.basename(video_path)} (ID: {task_id})")
        return task_id
    
    def start_pipeline(self, cache_manager, progress_manager, video_processor):
        """å¯åŠ¨æµæ°´çº¿å¤„ç†"""
        logging.info("ğŸ”„ å¯åŠ¨è§†é¢‘å¤„ç†æµæ°´çº¿...")
        
        # å¯åŠ¨ç¼“å­˜çº¿ç¨‹
        for i in range(self.max_concurrent_cache):
            thread = Thread(
                target=self._cache_worker,
                args=(cache_manager,),
                name=f"CacheWorker-{i}",
                daemon=True
            )
            thread.start()
            self.cache_threads.append(thread)
        
        # å¯åŠ¨æŸ¥é‡çº¿ç¨‹
        for i in range(self.max_concurrent_check):
            thread = Thread(
                target=self._check_worker,
                args=(progress_manager,),
                name=f"CheckWorker-{i}",
                daemon=True
            )
            thread.start()
            self.check_threads.append(thread)
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        for i in range(self.max_concurrent_process):
            thread = Thread(
                target=self._process_worker,
                args=(video_processor,),
                name=f"ProcessWorker-{i}",
                daemon=True
            )
            thread.start()
            self.process_threads.append(thread)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = Thread(
            target=self._monitor_worker,
            name="PipelineMonitor",
            daemon=True
        )
        self.monitor_thread.start()
        
        logging.info(f"âœ… æµæ°´çº¿å·²å¯åŠ¨: ç¼“å­˜Ã—{self.max_concurrent_cache}, æŸ¥é‡Ã—{self.max_concurrent_check}, å¤„ç†Ã—{self.max_concurrent_process}")
    
    def _cache_worker(self, cache_manager):
        """ç¼“å­˜å·¥ä½œçº¿ç¨‹"""
        thread_name = threading.current_thread().name
        logging.debug(f"ğŸ”„ {thread_name} å¯åŠ¨")
        
        while not self.shutdown_event.is_set():
            try:
                # ç­‰å¾…ä»»åŠ¡
                task_id = self.pending_queue.get(timeout=1.0)
                
                with self.task_lock:
                    if task_id not in self.tasks:
                        continue
                    task = self.tasks[task_id]
                    
                    # æ›´æ–°çŠ¶æ€
                    self.active_tasks[PipelineStage.PENDING].discard(task_id)
                    self.active_tasks[PipelineStage.CACHING].add(task_id)
                    task.update_stage(PipelineStage.CACHING)
                
                print(f"ğŸ“¥ å¼€å§‹ç¼“å­˜: {os.path.basename(task.video_path)}")
                logging.debug(f"ğŸ“¥ {thread_name} å¼€å§‹ç¼“å­˜: {os.path.basename(task.video_path)}")
                
                # æ‰§è¡Œç¼“å­˜
                start_time = time.time()
                cache_path = cache_manager.get_cached_path(task.video_path)
                
                # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œæ‰§è¡Œä¸‹è½½
                if not cache_path:
                    logging.info(f"ğŸ”„ ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½: {os.path.basename(task.video_path)}")
                    download_success = cache_manager._download_video_to_cache(task.video_path)
                    if download_success:
                        # ä¸‹è½½æˆåŠŸåé‡æ–°è·å–ç¼“å­˜è·¯å¾„
                        cache_path = cache_manager.get_cached_path(task.video_path)
                
                cache_time = time.time() - start_time
                
                with self.task_lock:
                    if cache_path:
                        # ç¼“å­˜æˆåŠŸ
                        task.cache_path = cache_path
                        self.active_tasks[PipelineStage.CACHING].discard(task_id)
                        self.active_tasks[PipelineStage.CACHED].add(task_id)
                        task.update_stage(PipelineStage.CACHED)
                        
                        # æ·»åŠ åˆ°æŸ¥é‡é˜Ÿåˆ—
                        self.cache_queue.put(task_id)
                        self.stats['cache_hits'] += 1
                        self.stats['avg_cache_time'] = (self.stats['avg_cache_time'] + cache_time) / 2
                        
                        print(f"ğŸ’¾ ç¼“å­˜å®Œæˆ: {os.path.basename(task.video_path)} ({cache_time:.1f}s)")
                        logging.info(f"âœ… {thread_name} ç¼“å­˜å®Œæˆ: {os.path.basename(task.video_path)} ({cache_time:.1f}s)")
                    else:
                        # ç¼“å­˜å¤±è´¥
                        self._handle_task_failure(task_id, "ç¼“å­˜å¤±è´¥", can_retry=True)
                
                with self.pipeline_condition:
                    self.pipeline_condition.notify_all()
                    
            except queue.Empty:
                continue
            except Exception as e:
                logging.error(f"âŒ {thread_name} ç¼“å­˜å¼‚å¸¸: {e}")
                if 'task_id' in locals():
                    self._handle_task_failure(task_id, f"ç¼“å­˜å¼‚å¸¸: {e}")
                    
        logging.debug(f"ğŸ›‘ {thread_name} å·²åœæ­¢")
    
    def _check_worker(self, progress_manager):
        """æŸ¥é‡å·¥ä½œçº¿ç¨‹"""
        thread_name = threading.current_thread().name
        logging.debug(f"ğŸ”„ {thread_name} å¯åŠ¨")
        
        while not self.shutdown_event.is_set():
            try:
                # ç­‰å¾…ä»»åŠ¡
                task_id = self.cache_queue.get(timeout=1.0)
                
                with self.task_lock:
                    if task_id not in self.tasks:
                        continue
                    task = self.tasks[task_id]
                    
                    # æ›´æ–°çŠ¶æ€
                    self.active_tasks[PipelineStage.CACHED].discard(task_id)
                    self.active_tasks[PipelineStage.CHECKING].add(task_id)
                    task.update_stage(PipelineStage.CHECKING)
                
                logging.debug(f"ğŸ” {thread_name} å¼€å§‹æŸ¥é‡: {os.path.basename(task.video_path)}")
                
                # æ‰§è¡ŒæŸ¥é‡
                start_time = time.time()
                is_duplicate = self._perform_duplicate_check(task, progress_manager)
                check_time = time.time() - start_time
                
                with self.task_lock:
                    self.active_tasks[PipelineStage.CHECKING].discard(task_id)
                    
                    if is_duplicate:
                        # å‘ç°é‡å¤
                        task.update_stage(PipelineStage.DUPLICATE)
                        self.failed_queue.put(task_id)
                        self.stats['duplicate_tasks'] += 1
                        logging.info(f"ğŸ”„ {thread_name} å‘ç°é‡å¤: {os.path.basename(task.video_path)}")
                    else:
                        # æ— é‡å¤ï¼Œè¿›å…¥å¤„ç†é˜Ÿåˆ—
                        self.active_tasks[PipelineStage.CACHED].add(task_id)
                        task.update_stage(PipelineStage.CACHED)
                        self.process_queue.put(task_id)
                        print(f"ğŸ” æŸ¥é‡é€šè¿‡: {os.path.basename(task.video_path)} ({check_time:.1f}s)")
                        logging.info(f"âœ… {thread_name} æŸ¥é‡é€šè¿‡: {os.path.basename(task.video_path)} ({check_time:.1f}s)")
                    
                    self.stats['avg_check_time'] = (self.stats['avg_check_time'] + check_time) / 2
                
                with self.pipeline_condition:
                    self.pipeline_condition.notify_all()
                    
            except queue.Empty:
                continue
            except Exception as e:
                logging.error(f"âŒ {thread_name} æŸ¥é‡å¼‚å¸¸: {e}")
                if 'task_id' in locals():
                    self._handle_task_failure(task_id, f"æŸ¥é‡å¼‚å¸¸: {e}")
                    
        logging.debug(f"ğŸ›‘ {thread_name} å·²åœæ­¢")
    
    def _process_worker(self, video_processor):
        """å¤„ç†å·¥ä½œçº¿ç¨‹"""
        thread_name = threading.current_thread().name
        logging.debug(f"ğŸ”„ {thread_name} å¯åŠ¨")
        
        while not self.shutdown_event.is_set():
            try:
                # ç­‰å¾…ä»»åŠ¡
                task_id = self.process_queue.get(timeout=1.0)
                
                with self.task_lock:
                    if task_id not in self.tasks:
                        continue
                    task = self.tasks[task_id]
                    
                    # æ›´æ–°çŠ¶æ€
                    self.active_tasks[PipelineStage.CACHED].discard(task_id)
                    self.active_tasks[PipelineStage.PROCESSING].add(task_id)
                    task.update_stage(PipelineStage.PROCESSING)
                
                print(f"âš™ï¸ å¼€å§‹å¤„ç†: {os.path.basename(task.video_path)}")
                logging.info(f"âš™ï¸ {thread_name} å¼€å§‹å¤„ç†: {os.path.basename(task.video_path)}")
                
                # æ‰§è¡Œå¤„ç†
                start_time = time.time()
                success = self._perform_video_processing(task, video_processor)
                process_time = time.time() - start_time
                
                with self.task_lock:
                    self.active_tasks[PipelineStage.PROCESSING].discard(task_id)
                    
                    if success:
                        # å¤„ç†æˆåŠŸ
                        task.update_stage(PipelineStage.COMPLETED)
                        self.completed_queue.put(task_id)
                        self.stats['completed_tasks'] += 1
                        print(f"ğŸ¬ å¤„ç†å®Œæˆ: {os.path.basename(task.video_path)} ({process_time:.1f}s)")
                        logging.info(f"ğŸ‰ {thread_name} å¤„ç†å®Œæˆ: {os.path.basename(task.video_path)} ({process_time:.1f}s)")
                    else:
                        # å¤„ç†å¤±è´¥
                        self._handle_task_failure(task_id, "å¤„ç†å¤±è´¥", can_retry=True)
                    
                    self.stats['avg_process_time'] = (self.stats['avg_process_time'] + process_time) / 2
                
                with self.pipeline_condition:
                    self.pipeline_condition.notify_all()
                    
            except queue.Empty:
                continue
            except Exception as e:
                logging.error(f"âŒ {thread_name} å¤„ç†å¼‚å¸¸: {e}")
                if 'task_id' in locals():
                    self._handle_task_failure(task_id, f"å¤„ç†å¼‚å¸¸: {e}")
                    
        logging.debug(f"ğŸ›‘ {thread_name} å·²åœæ­¢")
    
    def _monitor_worker(self):
        """ç›‘æ§å·¥ä½œçº¿ç¨‹ - é˜²å¡æ­»æ£€æµ‹"""
        logging.debug("ğŸ”„ Pipelineç›‘æ§çº¿ç¨‹å¯åŠ¨")
        
        while not self.shutdown_event.is_set():
            try:
                time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
                current_time = time.time()
                tasks_to_timeout = []
                
                with self.task_lock:
                    # æ£€æŸ¥è¶…æ—¶ä»»åŠ¡
                    for task_id, task in self.tasks.items():
                        if task.stage in [PipelineStage.CACHING, PipelineStage.CHECKING, PipelineStage.PROCESSING]:
                            if task.is_timeout():
                                tasks_to_timeout.append(task_id)
                    
                    # å¤„ç†è¶…æ—¶ä»»åŠ¡
                    for task_id in tasks_to_timeout:
                        self._handle_task_timeout(task_id)
                    
                    # æ­»é”æ£€æµ‹
                    if self.deadlock_detection_enabled:
                        self._detect_deadlock()
                    
                    # æ›´æ–°æ´»è·ƒæ—¶é—´
                    if tasks_to_timeout or self._has_active_tasks():
                        self.last_activity_time = current_time
                
                # æ‰“å°çŠ¶æ€
                if current_time - self.last_activity_time < 60:  # æœ€è¿‘1åˆ†é’Ÿæœ‰æ´»åŠ¨
                    self._print_pipeline_status()
                    
            except Exception as e:
                logging.error(f"âŒ ç›‘æ§çº¿ç¨‹å¼‚å¸¸: {e}")
                
        logging.debug("ğŸ›‘ Pipelineç›‘æ§çº¿ç¨‹å·²åœæ­¢")
    
    def _handle_task_failure(self, task_id: str, error_msg: str, can_retry: bool = False):
        """å¤„ç†ä»»åŠ¡å¤±è´¥"""
        with self.task_lock:
            if task_id not in self.tasks:
                return
                
            task = self.tasks[task_id]
            
            # ä»æ´»è·ƒä»»åŠ¡ä¸­ç§»é™¤
            for stage_set in self.active_tasks.values():
                stage_set.discard(task_id)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
            if can_retry and task.can_retry():
                task.increment_retry()
                task.update_stage(PipelineStage.PENDING, error_msg)
                self.active_tasks[PipelineStage.PENDING].add(task_id)
                self.pending_queue.put(task_id)
                logging.warning(f"ğŸ”„ ä»»åŠ¡é‡è¯• ({task.retry_count}/{task.max_retries}): {os.path.basename(task.video_path)} - {error_msg}")
            else:
                # æ ‡è®°å¤±è´¥
                task.update_stage(PipelineStage.FAILED, error_msg)
                self.failed_queue.put(task_id)
                self.stats['failed_tasks'] += 1
                logging.error(f"âŒ ä»»åŠ¡å¤±è´¥: {os.path.basename(task.video_path)} - {error_msg}")
    
    def _handle_task_timeout(self, task_id: str):
        """å¤„ç†ä»»åŠ¡è¶…æ—¶"""
        with self.task_lock:
            if task_id not in self.tasks:
                return
                
            task = self.tasks[task_id]
            stage_name = task.stage.value
            elapsed = time.time() - task.stage_start_time
            
            logging.warning(f"â° ä»»åŠ¡è¶…æ—¶: {os.path.basename(task.video_path)} åœ¨ {stage_name} é˜¶æ®µè¶…æ—¶ ({elapsed:.1f}s)")
            
            # ä»æ´»è·ƒä»»åŠ¡ä¸­ç§»é™¤
            for stage_set in self.active_tasks.values():
                stage_set.discard(task_id)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
            if task.can_retry():
                task.increment_retry()
                task.update_stage(PipelineStage.PENDING, f"é˜¶æ®µ {stage_name} è¶…æ—¶")
                self.active_tasks[PipelineStage.PENDING].add(task_id)
                self.pending_queue.put(task_id)
                logging.info(f"ğŸ”„ è¶…æ—¶ä»»åŠ¡é‡è¯•: {os.path.basename(task.video_path)}")
            else:
                task.update_stage(PipelineStage.TIMEOUT, f"é˜¶æ®µ {stage_name} è¶…æ—¶")
                self.failed_queue.put(task_id)
                self.stats['failed_tasks'] += 1
    
    def _detect_deadlock(self):
        """æ­»é”æ£€æµ‹"""
        # ç®€å•çš„æ­»é”æ£€æµ‹ï¼šå¦‚æœæ‰€æœ‰é˜Ÿåˆ—éƒ½ç©ºä¸”æœ‰æ´»è·ƒä»»åŠ¡ä½†å¾ˆä¹…æ²¡æœ‰è¿›å±•
        if (self.pending_queue.empty() and self.cache_queue.empty() and 
            self.process_queue.empty() and self._has_active_tasks() and
            time.time() - self.last_activity_time > 120):  # 2åˆ†é’Ÿæ— è¿›å±•
            
            logging.warning("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„æ­»é”æƒ…å†µï¼Œå°è¯•æ¢å¤...")
            
            # é‡ç½®è¶…æ—¶ä»»åŠ¡
            with self.task_lock:
                for task_id, task in self.tasks.items():
                    if task.stage in [PipelineStage.CACHING, PipelineStage.CHECKING, PipelineStage.PROCESSING]:
                        if time.time() - task.stage_start_time > 60:  # è¶…è¿‡1åˆ†é’Ÿçš„ä»»åŠ¡
                            self._handle_task_timeout(task_id)
    
    def _has_active_tasks(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒä»»åŠ¡"""
        return any(len(tasks) > 0 for stage, tasks in self.active_tasks.items() 
                  if stage != PipelineStage.COMPLETED)
    
    def _print_pipeline_status(self):
        """æ‰“å°æµæ°´çº¿çŠ¶æ€ - å·²ç¦ç”¨ï¼Œä½¿ç”¨tqdmç»Ÿä¸€æ˜¾ç¤º"""
        pass  # ä¸å†è¾“å‡ºï¼Œé¿å…ä¸tqdmè¿›åº¦æ¡å†²çª
    
    def _perform_duplicate_check(self, task: PipelineTask, progress_manager) -> bool:
        """æ‰§è¡ŒæŸ¥é‡æ£€æŸ¥ - ä½¿ç”¨å¢å¼ºçš„æ™ºèƒ½æŸ¥é‡ç³»ç»Ÿ ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è·¯å¾„è¿›è¡ŒæŸ¥é‡"""
        try:
            if not progress_manager.video_record_manager:
                return False
                
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è§†é¢‘è·¯å¾„è€Œä¸æ˜¯ç¼“å­˜è·¯å¾„è¿›è¡ŒæŸ¥é‡æ£€æŸ¥
            # è¿™ç¡®ä¿æ•°æ®åº“ä¸­è®°å½•çš„æ˜¯åŸå§‹NASè·¯å¾„çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æœ¬åœ°ç¼“å­˜è·¯å¾„
            is_duplicate, message = progress_manager.video_record_manager.enhanced_duplicate_check(task.video_path)
            
            if is_duplicate:
                logging.info(f"ğŸ” æ™ºèƒ½æŸ¥é‡å‘ç°é‡å¤: {os.path.basename(task.video_path)} - {message}")
                
                # æ¸…ç†ç¼“å­˜æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    if task.cache_path and os.path.exists(task.cache_path):
                        os.remove(task.cache_path)
                        logging.debug(f"ğŸ—‘ï¸ æ¸…ç†é‡å¤æ–‡ä»¶ç¼“å­˜: {task.cache_path}")
                except Exception as e:
                    logging.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
                return True
            else:
                logging.debug(f"ğŸ“ æ™ºèƒ½æŸ¥é‡æ£€æŸ¥å®Œæˆ: {os.path.basename(task.video_path)} - {message}")
                return False
                
        except Exception as e:
            logging.error(f"æ™ºèƒ½æŸ¥é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _perform_video_processing(self, task: PipelineTask, video_processor) -> bool:
        """æ‰§è¡Œè§†é¢‘å¤„ç†"""
        try:
            if not task.cache_path or not os.path.exists(task.cache_path):
                return False
                
            # è¿™é‡Œè°ƒç”¨å®é™…çš„è§†é¢‘å¤„ç†å‡½æ•°
            # éœ€è¦æ ¹æ®å…·ä½“çš„è§†é¢‘å¤„ç†å™¨æ¥å£è°ƒæ•´
            result = video_processor.process_single_video(
                input_path=task.cache_path,
                original_path=task.video_path
            )
            
            return result is not None and result
            
        except Exception as e:
            logging.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            return False
    
    def wait_completion(self, timeout: Optional[float] = None) -> bool:
        """ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ - å¢å¼ºç¨³å®šæ€§ç‰ˆæœ¬"""
        start_time = time.time()
        last_progress_time = start_time
        last_completed_count = 0
        
        while self._has_active_tasks():
            current_time = time.time()
            
            # æ£€æŸ¥æ€»ä½“è¶…æ—¶ï¼ˆå¦‚æœè®¾ç½®äº†ï¼‰
            if timeout and (current_time - start_time) > timeout:
                logging.warning(f"â° ç­‰å¾…å®Œæˆè¶…æ—¶ ({timeout}s)")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿›å±•ï¼ˆé˜²æ­¢å¡æ­»ï¼‰
            current_completed = self.stats['completed_tasks']
            if current_completed > last_completed_count:
                last_progress_time = current_time
                last_completed_count = current_completed
                logging.info(f"ğŸ“ˆ å¤„ç†è¿›åº¦: {current_completed}/{self.stats['total_tasks']} å®Œæˆ")
            
            # å¦‚æœ30åˆ†é’Ÿæ²¡æœ‰ä»»ä½•è¿›å±•ï¼Œè­¦å‘Šä½†ä¸é€€å‡ºï¼ˆè®©ä»»åŠ¡è‡ªç„¶è¶…æ—¶ï¼‰
            elif current_time - last_progress_time > 1800:  # 30åˆ†é’Ÿ
                logging.warning(f"âš ï¸ 30åˆ†é’Ÿæ— è¿›å±•ï¼Œå½“å‰çŠ¶æ€: {current_completed}/{self.stats['total_tasks']} å®Œæˆ")
                logging.warning(f"   æ´»è·ƒä»»åŠ¡: ç¼“å­˜{len(self.active_tasks[PipelineStage.CACHING])}, "
                              f"å¤„ç†{len(self.active_tasks[PipelineStage.PROCESSING])}")
                last_progress_time = current_time  # é‡ç½®æ—¶é—´ï¼Œé¿å…é‡å¤è­¦å‘Š
                
            with self.pipeline_condition:
                self.pipeline_condition.wait(timeout=10.0)  # å¢åŠ ç­‰å¾…é—´éš”
                
            if self.shutdown_event.is_set():
                logging.info("ğŸ›‘ æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œé€€å‡ºç­‰å¾…")
                break
                
        logging.info("âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        return True
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.task_lock:
            active_stats = {stage.value: len(tasks) for stage, tasks in self.active_tasks.items()}
            
        return {
            **self.stats,
            'active_by_stage': active_stats,
            'total_active': sum(active_stats.values()),
            'queue_sizes': {
                'pending': self.pending_queue.qsize(),
                'cache': self.cache_queue.qsize(),
                'process': self.process_queue.qsize(),
                'completed': self.completed_queue.qsize(),
                'failed': self.failed_queue.qsize()
            }
        }
    
    def shutdown(self):
        """å…³é—­æµæ°´çº¿"""
        logging.info("ğŸ›‘ å¼€å§‹å…³é—­è§†é¢‘å¤„ç†æµæ°´çº¿...")
        
        self.shutdown_event.set()
        
        with self.pipeline_condition:
            self.pipeline_condition.notify_all()
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        all_threads = self.cache_threads + self.check_threads + self.process_threads
        if self.monitor_thread:
            all_threads.append(self.monitor_thread)
            
        for thread in all_threads:
            if thread.is_alive():
                thread.join(timeout=5.0)
                
        logging.info("âœ… è§†é¢‘å¤„ç†æµæ°´çº¿å·²å…³é—­")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if not self.shutdown_event.is_set():
            self.shutdown()


# ==================== 2025å¹´é¡¶å°–NASä¼˜åŒ–æ ¸å¿ƒç±» ====================

class DatabaseManager:
    """æ•°æ®åº“è¿æ¥ç®¡ç†å™¨ - å¤šç”µè„‘åä½œæ ¸å¿ƒ"""
    
    def __init__(self):
        self.connection_pool = None
        self.is_enabled = ENABLE_MULTI_COMPUTER_SYNC
        if self.is_enabled:
            self._create_connection_pool()
    
    def _create_connection_pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        try:
            self.connection_pool = mysql.connector.pooling.MySQLConnectionPool(
                pool_name="video_editing_pool",
                pool_size=5,
                **MYSQL_CONFIG
            )
            logger.info("ğŸ”— æ•°æ®åº“è¿æ¥æ± åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•è¿æ¥
            with self.get_connection() as conn:
                if conn and conn.is_connected():
                    cursor = conn.cursor()
                    cursor.execute("SELECT 1")
                    cursor.fetchone()
                    logger.info("âœ… æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ")
                else:
                    raise Exception("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
                    
        except Error as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥æ± åˆ›å»ºå¤±è´¥: {e}")
            self.is_enabled = False
            self.connection_pool = None
    
    def get_connection(self, retries=3):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        if not self.is_enabled or not self.connection_pool:
            return None
            
        for attempt in range(retries):
            try:
                connection = self.connection_pool.get_connection()
                if connection and connection.is_connected():
                    # æµ‹è¯•è¿æ¥å¥åº·çŠ¶æ€
                    try:
                        connection.ping(reconnect=True)
                        return connection
                    except Error:
                        connection.close()
                        continue
                        
            except Error as e:
                if attempt == retries - 1:  # æœ€åä¸€æ¬¡å°è¯•
                    logger.error(f"âŒ è·å–æ•°æ®åº“è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                    # å°è¯•é‡æ–°åˆ›å»ºè¿æ¥æ± 
                    if e.errno in [2006, 2013]:  # MySQL server has gone away / Lost connection
                        logger.info("ğŸ”„ å°è¯•é‡æ–°åˆ›å»ºæ•°æ®åº“è¿æ¥æ± ...")
                        self._create_connection_pool()
                else:
                    logger.warning(f"âš ï¸ è·å–æ•°æ®åº“è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                    time.sleep(0.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    
        return None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å¯ç”¨"""
        return self.is_enabled and self.connection_pool is not None
    
    def health_check(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± å¥åº·çŠ¶æ€"""
        if not self.is_available():
            return False
            
        try:
            with self.get_connection() as conn:
                if conn and conn.is_connected():
                    cursor = conn.cursor()
                    cursor.execute("SELECT 1")
                    cursor.fetchone()
                    return True
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            
        return False
    
    def rebuild_pool_if_needed(self) -> bool:
        """å¦‚æœè¿æ¥æ± ä¸å¥åº·ï¼Œå°è¯•é‡å»º"""
        if not self.health_check():
            logger.info("ğŸ”„ æ£€æµ‹åˆ°æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œæ­£åœ¨é‡å»ºè¿æ¥æ± ...")
            self._create_connection_pool()
            return self.health_check()
        return True
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨"""
        try:
            # å¦‚æœæ•°æ®åº“åŠŸèƒ½æœªå¯ç”¨ï¼Œç›´æ¥è¿”å›True
            if not self.is_enabled:
                logger.info("ğŸ”§ æ•°æ®åº“åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
                return True
            
            # æ£€æŸ¥è¿æ¥æ± æ˜¯å¦å·²åˆ›å»º
            if not self.connection_pool:
                logger.info("ğŸ”§ åˆ›å»ºæ•°æ®åº“è¿æ¥æ± ...")
                self._create_connection_pool()
            
            # éªŒè¯è¿æ¥æ± çŠ¶æ€
            if not self.is_available():
                logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨")
                return False
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            if self.health_check():
                logger.info("âœ… æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.warning("âš ï¸ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def execute_with_retry(self, query: str, params: tuple = None, retries: int = 3, fetch: bool = False) -> bool:
        """æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆå¸¦é‡è¯•å’Œäº‹åŠ¡ç®¡ç†ï¼‰"""
        if not self.is_available():
            return False
            
        for attempt in range(retries):
            connection = None
            try:
                connection = self.get_connection()
                if not connection:
                    continue
                    
                cursor = connection.cursor()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºDDLè¯­å¥ï¼ˆCREATE, ALTER, DROPç­‰ï¼‰
                query_upper = query.strip().upper()
                is_ddl = query_upper.startswith(('CREATE', 'ALTER', 'DROP'))
                
                # DDLè¯­å¥ä¸éœ€è¦æ˜¾å¼äº‹åŠ¡ç®¡ç†
                if not is_ddl:
                    connection.start_transaction()
                
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                result = None
                if fetch:
                    result = cursor.fetchall()
                
                # DDLè¯­å¥è‡ªåŠ¨æäº¤ï¼Œå…¶ä»–è¯­å¥éœ€è¦æ‰‹åŠ¨æäº¤
                if not is_ddl:
                    connection.commit()
                
                cursor.close()
                return result if fetch else True
                
            except Error as e:
                if connection:
                    try:
                        connection.rollback()
                    except:
                        pass
                        
                if attempt == retries - 1:
                    logger.error(f"âŒ SQLæ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                else:
                    logger.warning(f"âš ï¸ SQLæ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                    time.sleep(0.5 * (attempt + 1))
                    
            except Exception as e:
                if connection:
                    try:
                        connection.rollback()
                    except:
                        pass
                logger.error(f"âŒ SQLæ‰§è¡Œæ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
                break
                
            finally:
                if connection:
                    try:
                        connection.close()
                    except:
                        pass
                        
        return False
    
    def execute_batch_with_retry(self, queries: List[Tuple[str, tuple]], retries: int = 3) -> bool:
        """æ‰¹é‡æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆå¸¦é‡è¯•å’Œäº‹åŠ¡ç®¡ç†ï¼‰"""
        if not self.is_available() or not queries:
            return False
            
        for attempt in range(retries):
            connection = None
            try:
                connection = self.get_connection()
                if not connection:
                    continue
                    
                cursor = connection.cursor()
                
                # å¼€å§‹äº‹åŠ¡
                connection.start_transaction()
                
                # æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
                for query, params in queries:
                    if params:
                        cursor.execute(query, params)
                    else:
                        cursor.execute(query)
                
                # æäº¤äº‹åŠ¡
                connection.commit()
                cursor.close()
                logger.info(f"âœ… æ‰¹é‡æ‰§è¡Œ {len(queries)} æ¡SQLè¯­å¥æˆåŠŸ")
                return True
                
            except Error as e:
                if connection:
                    try:
                        connection.rollback()
                    except:
                        pass
                        
                if attempt == retries - 1:
                    logger.error(f"âŒ æ‰¹é‡SQLæ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                else:
                    logger.warning(f"âš ï¸ æ‰¹é‡SQLæ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                    time.sleep(0.5 * (attempt + 1))
                    
            except Exception as e:
                if connection:
                    try:
                        connection.rollback()
                    except:
                        pass
                logger.error(f"âŒ æ‰¹é‡SQLæ‰§è¡Œæ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
                break
                
            finally:
                if connection:
                    try:
                        connection.close()
                    except:
                        pass
                        
        return False
    
    def fetch_all_with_retry(self, query: str, params: tuple = None, retries: int = 3) -> list:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è·å–æ‰€æœ‰ç»“æœï¼ˆå¸¦é‡è¯•ï¼‰"""
        if not self.is_available():
            return []
            
        for attempt in range(retries):
            try:
                with self.get_connection() as conn:
                    if conn and conn.is_connected():
                        cursor = conn.cursor(dictionary=True)
                        
                        if params:
                            cursor.execute(query, params)
                        else:
                            cursor.execute(query)
                        
                        result = cursor.fetchall()
                        return result
                        
            except Exception as e:
                logger.warning(f"âš ï¸ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                if attempt == retries - 1:
                    logger.error(f"âŒ æŸ¥è¯¢æœ€ç»ˆå¤±è´¥: {query}")
                    return []
                time.sleep(0.5 * (attempt + 1))
        
        return []

    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
        if not self.is_available():
            return {}
            
        metrics = {}
        try:
            with self.get_connection() as conn:
                if conn and conn.is_connected():
                    cursor = conn.cursor()
                    
                    # è·å–è¿æ¥æ•°
                    cursor.execute("SHOW STATUS LIKE 'Threads_connected'")
                    result = cursor.fetchone()
                    if result:
                        metrics['active_connections'] = int(result[1])
                    
                    # è·å–æŸ¥è¯¢ç»Ÿè®¡
                    cursor.execute("SHOW STATUS LIKE 'Questions'")
                    result = cursor.fetchone()
                    if result:
                        metrics['total_queries'] = int(result[1])
                    
                    # è·å–ç¼“å†²æ± ä½¿ç”¨ç‡
                    cursor.execute("SHOW STATUS LIKE 'Innodb_buffer_pool_pages_total'")
                    total_pages = cursor.fetchone()
                    cursor.execute("SHOW STATUS LIKE 'Innodb_buffer_pool_pages_free'")
                    free_pages = cursor.fetchone()
                    
                    if total_pages and free_pages:
                        total = int(total_pages[1])
                        free = int(free_pages[1])
                        used = total - free
                        metrics['buffer_pool_usage_percent'] = round((used / total) * 100, 2) if total > 0 else 0
                    
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            
        return metrics
    
    def cleanup(self):
        """æ¸…ç†æ•°æ®åº“è¿æ¥æ± èµ„æº"""
        try:
            if self.connection_pool:
                # å…³é—­è¿æ¥æ± ä¸­çš„æ‰€æœ‰è¿æ¥
                self.connection_pool.close()
                self.connection_pool = None
                logger.info("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“è¿æ¥æ± æ¸…ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        finally:
            self.is_enabled = False

class DatabaseMonitor:
    """æ•°æ®åº“ç›‘æ§å’Œè¯Šæ–­ç±»"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.last_check_time = 0
        self.health_history = []
        
    def get_comprehensive_status(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»¼åˆçŠ¶æ€æŠ¥å‘Š"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'basic_health': self.db_manager.health_check(),
            'connection_pool_status': self.db_manager.is_available(),
            'performance_metrics': self.db_manager.get_performance_metrics(),
            'table_statistics': self._get_table_statistics(),
            'recent_errors': self._get_recent_errors(),
            'recommendations': []
        }
        
        # æ·»åŠ å»ºè®®
        if not status['basic_health']:
            status['recommendations'].append("æ•°æ®åº“è¿æ¥å¼‚å¸¸ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå’Œæ•°æ®åº“æœåŠ¡çŠ¶æ€")
        
        perf = status.get('performance_metrics', {})
        if perf.get('buffer_pool_usage_percent', 0) > 90:
            status['recommendations'].append("ç¼“å†²æ± ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ innodb_buffer_pool_size")
        
        if perf.get('active_connections', 0) > 80:
            status['recommendations'].append("æ´»è·ƒè¿æ¥æ•°è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥æ± é…ç½®")
            
        return status
    
    def _get_table_statistics(self) -> Dict[str, Any]:
        """è·å–è§†é¢‘ç¼–è¾‘è¡¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        if not self.db_manager.is_available():
            return {}
            
        stats = {}
        try:
            with self.db_manager.get_connection() as conn:
                if conn and conn.is_connected():
                    cursor = conn.cursor()
                    
                    # æ€»è®°å½•æ•°
                    cursor.execute("SELECT COUNT(*) FROM Video_Editing")
                    result = cursor.fetchone()
                    stats['total_records'] = result[0] if result else 0
                    
                    # æˆåŠŸå¤„ç†æ•°
                    cursor.execute("SELECT COUNT(*) FROM Video_Editing WHERE status = 1")
                    result = cursor.fetchone()
                    stats['successful_processes'] = result[0] if result else 0
                    
                    # å¤±è´¥å¤„ç†æ•°
                    cursor.execute("SELECT COUNT(*) FROM Video_Editing WHERE status = 0")
                    result = cursor.fetchone()
                    stats['failed_processes'] = result[0] if result else 0
                    
                    # ä»Šæ—¥å¤„ç†æ•°
                    cursor.execute("""
                        SELECT COUNT(*) FROM Video_Editing 
                        WHERE DATE(updated_time) = CURDATE()
                    """)
                    result = cursor.fetchone()
                    stats['today_processes'] = result[0] if result else 0
                    
                    # å„è®¡ç®—æœºå¤„ç†ç»Ÿè®¡
                    cursor.execute("""
                        SELECT computer_name, COUNT(*) as count, 
                               SUM(CASE WHEN status = 1 THEN 1 ELSE 0 END) as success_count
                        FROM Video_Editing 
                        GROUP BY computer_name
                        ORDER BY count DESC
                    """)
                    results = cursor.fetchall()
                    stats['computer_statistics'] = [
                        {
                            'computer_name': row[0],
                            'total_count': row[1],
                            'success_count': row[2],
                            'success_rate': round(row[2] / row[1] * 100, 2) if row[1] > 0 else 0
                        }
                        for row in results
                    ]
                    
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            
        return stats
    
    def _get_recent_errors(self) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„é”™è¯¯è®°å½•"""
        if not self.db_manager.is_available():
            return []
            
        errors = []
        try:
            with self.db_manager.get_connection() as conn:
                if conn and conn.is_connected():
                    cursor = conn.cursor(dictionary=True)
                    
                    cursor.execute("""
                        SELECT video_name, log_path, updated_time, computer_name
                        FROM Video_Editing 
                        WHERE status = 0 AND updated_time >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                        ORDER BY updated_time DESC
                        LIMIT 10
                    """)
                    
                    results = cursor.fetchall()
                    errors = [
                        {
                            'video_name': row['video_name'],
                            'error_message': row['log_path'],
                            'time': row['updated_time'].isoformat() if row['updated_time'] else None,
                            'computer': row['computer_name']
                        }
                        for row in results
                    ]
                    
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–é”™è¯¯è®°å½•å¤±è´¥: {e}")
            
        return errors
    
    def log_health_status(self):
        """è®°å½•å¥åº·çŠ¶æ€å†å²"""
        current_time = time.time()
        if current_time - self.last_check_time > 300:  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            health_status = {
                'timestamp': current_time,
                'healthy': self.db_manager.health_check(),
                'metrics': self.db_manager.get_performance_metrics()
            }
            
            self.health_history.append(health_status)
            
            # åªä¿ç•™æœ€è¿‘24å°æ—¶çš„è®°å½•
            cutoff_time = current_time - 86400
            self.health_history = [h for h in self.health_history if h['timestamp'] > cutoff_time]
            
            self.last_check_time = current_time
            
            if not health_status['healthy']:
                logger.warning("âš ï¸ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•é‡å»ºè¿æ¥æ± ...")
                self.db_manager.rebuild_pool_if_needed()

class VideoRecordManager:
    """è§†é¢‘è®°å½•ç®¡ç†å™¨ - å¤šç”µè„‘åä½œé˜²é‡å¤å¤„ç†"""
    
    def __init__(self, computer_name: str, db_manager: DatabaseManager, log_file_path: str = None):
        self.computer_name = computer_name
        self.db_manager = db_manager
        self.computer_ip = self._get_local_ip()
        self.log_file_path = log_file_path  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
        # æ–°å¢æŸ¥é‡ç›¸å…³é…ç½®
        self.video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.m4v', '.3gp')
        self.duplicate_output_path = None  # é‡å¤è§†é¢‘ç§»åŠ¨ç›®æ ‡è·¯å¾„ï¼Œå¯é…ç½®
        self.opencv_timeout = 30000  # OpenCVè¶…æ—¶è®¾ç½®(æ¯«ç§’)
        
        # æ•°æ®åº“è¡¨åé…ç½®ï¼ˆå›ºå®šä¸ºVideo_Editingï¼‰
        self.videos_table = 'Video_Editing'
        # ç§»é™¤é¢å¤–è¡¨ï¼Œç»Ÿä¸€ä½¿ç”¨Video_Editingè¡¨
        
        # æˆåŠŸæ’å…¥æ•°æ®åº“çš„è§†é¢‘è®¡æ•°å™¨
        self.db_insert_success_count = 0
        self.db_insert_total_count = 0
        
        # åˆå§‹åŒ–å¢å¼ºæ•°æ®åº“è¡¨
        self._create_enhanced_tables()
        
    def _get_local_ip(self) -> str:
        """è·å–æœ¬æœºIPåœ°å€"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
                s.connect(("8.8.8.8", 80))
                return s.getsockname()[0]
        except Exception:
            return "127.0.0.1"
    
    def _get_config_hash(self) -> str:
        """è·å–å½“å‰é…ç½®çš„å“ˆå¸Œå€¼"""
        config_data = {
            'enable_head_tail_cut': ENABLE_HEAD_TAIL_CUT,
            'enable_cropping': ENABLE_CROPPING,
            'head_cut_time': HEAD_CUT_TIME if ENABLE_HEAD_TAIL_CUT else 0,
            'tail_cut_time': TAIL_CUT_TIME if ENABLE_HEAD_TAIL_CUT else 0,
            'target_resolution': TARGET_RESOLUTION if ENABLE_CROPPING else None,
            'quality_mode': QUALITY_MODE
        }
        config_str = json.dumps(config_data, sort_keys=True)
        return hashlib.sha256(config_str.encode()).hexdigest()
    
    def _create_enhanced_tables(self):
        """åˆ›å»ºå¢å¼ºçš„æŸ¥é‡æ•°æ®åº“è¡¨ç»“æ„"""
        if not self.db_manager.is_available():
            return
        
        try:
            # åˆ›å»ºå›ºå®šçš„Video_Editingè¡¨ï¼ˆä¸ç”¨æˆ·è§„èŒƒå®Œå…¨ä¸€è‡´ + å…¼å®¹ä»£ç éœ€æ±‚ï¼‰
            TABLE_NAME = "Video_Editing"
            videos_table_sql = f"""
            CREATE TABLE IF NOT EXISTS `{TABLE_NAME}` (
                `id` INT AUTO_INCREMENT PRIMARY KEY,
                `input_path` VARCHAR(1024) NOT NULL COMMENT 'è¾“å…¥è·¯å¾„',
                `output_path` VARCHAR(1024) COMMENT 'è¾“å‡ºè·¯å¾„',
                `video_name` VARCHAR(255) NOT NULL COMMENT 'è§†é¢‘å',
                `pre_processing_size` BIGINT COMMENT 'å¤„ç†å‰è§†é¢‘å¤§å° (Bytes)',
                `resolution` VARCHAR(50) COMMENT 'åˆ†è¾¨ç‡',
                `hash_value` VARCHAR(64) NOT NULL COMMENT 'å“ˆå¸Œå€¼',
                `operator` VARCHAR(100) COMMENT 'å¤„ç†äºº',
                `computer_name` VARCHAR(100) COMMENT 'ç”µè„‘å·',
                `computer_ip` VARCHAR(50) COMMENT 'ç”µè„‘IP',
                `status` TINYINT NOT NULL COMMENT 'å¤„ç†çŠ¶æ€ 0: å¤±è´¥, 1: æˆåŠŸ',
                `log_path` VARCHAR(1024) COMMENT 'æ—¥å¿—è·¯å¾„',
                `created_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
                `updated_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                UNIQUE KEY `hash_value_unique` (`hash_value`)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
            """
            
            # åªåˆ›å»º Video_Editing è¡¨ï¼Œç§»é™¤æ‰€æœ‰é¢å¤–è¡¨
            
            # æ‰§è¡Œè¡¨åˆ›å»º
            success = self.db_manager.execute_with_retry(videos_table_sql)
            if success:
                logger.info(f"âœ… æ•°æ®è¡¨ {TABLE_NAME} åˆ›å»º/éªŒè¯æˆåŠŸ")
            else:
                logger.warning(f"âš ï¸ æ•°æ®è¡¨ {TABLE_NAME} åˆ›å»ºå¤±è´¥")
            
            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
            self._add_missing_columns_if_needed(TABLE_NAME)
            
            # ç¡®è®¤è¡¨åé…ç½®
            self.videos_table = TABLE_NAME
                    
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¢å¼ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
    
    def _add_missing_columns_if_needed(self, table_name: str):
        """æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰- å·²ç¦ç”¨å…¼å®¹å­—æ®µæ·»åŠ """
        # ä¸å†æ·»åŠ ä»»ä½•å…¼å®¹å­—æ®µï¼Œä¿æŒè¡¨ç»“æ„ç®€æ´
        logger.info("âœ… è·³è¿‡å…¼å®¹åˆ—æ£€æŸ¥ - ä½¿ç”¨å›ºå®šè¡¨ç»“æ„")
        pass
    
    def calculate_video_sha256(self, video_path: str, quick_mode: bool = False) -> str:
        """è®¡ç®—è§†é¢‘æ–‡ä»¶SHA256ï¼ˆæ”¯æŒå¿«é€Ÿæ¨¡å¼å’Œå®Œæ•´æ¨¡å¼ï¼‰"""
        try:
            # éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§å’Œå¯è¯»æ€§
            if not os.path.exists(video_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                return ""
                
            if not os.access(video_path, os.R_OK):
                logger.error(f"æ–‡ä»¶æ— æ³•è¯»å–: {video_path}")
                return ""
            
            # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨æ–°çš„å¿«é€Ÿå“ˆå¸Œè®¡ç®—æ–¹æ³•
            if quick_mode:
                return self.calculate_file_hash_fast(video_path)
            
            # å®Œæ•´æ¨¡å¼ï¼šè®¡ç®—æ•´ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼
            hash_sha256 = hashlib.sha256()
            pre_processing_size = os.path.getsize(video_path)
            
            with open(video_path, "rb") as f:
                # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜å ç”¨
                bytes_read = 0
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_sha256.update(chunk)
                    bytes_read += len(chunk)
                    
                    # éªŒè¯è¯»å–è¿›åº¦ï¼Œç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§
                    if bytes_read > pre_processing_size:
                        logger.warning(f"æ–‡ä»¶å¤§å°å¼‚å¸¸: {video_path}, é¢„æœŸ{pre_processing_size}, å®é™…è¯»å–{bytes_read}")
                        break
                        
            # éªŒè¯å®é™…è¯»å–çš„å­—èŠ‚æ•°
            if bytes_read != pre_processing_size:
                logger.warning(f"æ–‡ä»¶è¯»å–ä¸å®Œæ•´: {video_path}, é¢„æœŸ{pre_processing_size}, å®é™…{bytes_read}")
                
            return hash_sha256.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—è§†é¢‘SHA256å¤±è´¥ {video_path}: {e}")
            return ""
    
    def _calculate_quick_hash(self, video_path: str) -> str:
        """å¿«é€Ÿå“ˆå¸Œè®¡ç®—ï¼ˆä»…è®¡ç®—æ–‡ä»¶çš„å…³é”®éƒ¨åˆ†ï¼Œå¢å¼ºä¸¥è°¨æ€§ï¼‰"""
        try:
            pre_processing_size = os.path.getsize(video_path)
            file_mtime = os.path.getmtime(video_path)
            video_name = os.path.basename(video_path)
            
            hash_sha256 = hashlib.sha256()
            
            # æ·»åŠ æ–‡ä»¶åŸºæœ¬ä¿¡æ¯åˆ°å“ˆå¸Œè®¡ç®—
            file_info = f"{video_name}|{pre_processing_size}|{file_mtime:.6f}"
            hash_sha256.update(file_info.encode('utf-8'))
            
            with open(video_path, "rb") as f:
                # è¯»å–æ–‡ä»¶å¤´éƒ¨ 1MBï¼ˆåŒ…å«è§†é¢‘å…ƒæ•°æ®ï¼‰
                header = f.read(1024 * 1024)
                if header:
                    hash_sha256.update(b"HEADER:")
                    hash_sha256.update(header)
                
                # å¦‚æœæ–‡ä»¶å¤§äº 10MBï¼Œè¯»å–å¤šä¸ªé‡‡æ ·ç‚¹
                if pre_processing_size > 10 * 1024 * 1024:
                    # è¯»å–1/4ä½ç½®çš„1MB
                    quarter_pos = pre_processing_size // 4
                    f.seek(quarter_pos)
                    quarter = f.read(1024 * 1024)
                    if quarter:
                        hash_sha256.update(b"QUARTER:")
                        hash_sha256.update(quarter)
                    
                    # è¯»å–ä¸­éƒ¨ 1MB
                    middle_pos = pre_processing_size // 2
                    f.seek(middle_pos)
                    middle = f.read(1024 * 1024)
                    if middle:
                        hash_sha256.update(b"MIDDLE:")
                        hash_sha256.update(middle)
                    
                    # è¯»å–3/4ä½ç½®çš„1MB
                    three_quarter_pos = pre_processing_size * 3 // 4
                    f.seek(three_quarter_pos)
                    three_quarter = f.read(1024 * 1024)
                    if three_quarter:
                        hash_sha256.update(b"THREE_QUARTER:")
                        hash_sha256.update(three_quarter)
                    
                    # è¯»å–å°¾éƒ¨ 1MB
                    tail_pos = max(0, pre_processing_size - 1024 * 1024)
                    f.seek(tail_pos)
                    tail = f.read(1024 * 1024)
                    if tail:
                        hash_sha256.update(b"TAIL:")
                        hash_sha256.update(tail)
                
                # æ·»åŠ é…ç½®ä¿¡æ¯åˆ°å“ˆå¸Œï¼ˆç¡®ä¿ç›¸åŒæ–‡ä»¶ä¸åŒé…ç½®æœ‰ä¸åŒå“ˆå¸Œï¼‰
                config_hash = self._get_config_hash()
                hash_sha256.update(f"CONFIG:{config_hash}".encode())
                
            result_hash = hash_sha256.hexdigest()
            logger.debug(f"å¿«é€Ÿå“ˆå¸Œè®¡ç®—å®Œæˆ: {video_name} -> {result_hash[:16]}...")
            return result_hash
            
        except Exception as e:
            logger.error(f"å¿«é€Ÿå“ˆå¸Œè®¡ç®—å¤±è´¥ {video_path}: {e}")
            return ""
    
    def verify_video_integrity(self, video_path: str) -> bool:
        """éªŒè¯è§†é¢‘æ–‡ä»¶å®Œæ•´æ€§å’Œå¯æ’­æ”¾æ€§"""
        try:
            # åŸºæœ¬æ–‡ä»¶æ£€æŸ¥
            if not os.path.exists(video_path):
                logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                return False
                
            if not os.access(video_path, os.R_OK):
                logger.error(f"è§†é¢‘æ–‡ä»¶æ— æ³•è¯»å–: {video_path}")
                return False
            
            pre_processing_size = os.path.getsize(video_path)
            if pre_processing_size == 0:
                logger.error(f"è§†é¢‘æ–‡ä»¶ä¸ºç©º: {video_path}")
                return False
            
            # ä½¿ç”¨ffprobeéªŒè¯è§†é¢‘æ ¼å¼å’Œå®Œæ•´æ€§
            try:
                video_info = get_video_info(video_path)
                if not video_info:
                    logger.error(f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {video_path}")
                    return False
                
                # æ£€æŸ¥å…³é”®ä¿¡æ¯
                duration = video_info.get('duration', 0)
                width = video_info.get('width', 0)
                height = video_info.get('height', 0)
                
                if duration <= 0:
                    logger.error(f"è§†é¢‘æ—¶é•¿æ— æ•ˆ: {video_path}, æ—¶é•¿: {duration}")
                    return False
                    
                if width <= 0 or height <= 0:
                    logger.error(f"è§†é¢‘åˆ†è¾¨ç‡æ— æ•ˆ: {video_path}, åˆ†è¾¨ç‡: {width}x{height}")
                    return False
                
                # æ£€æŸ¥è§†é¢‘ç¼–ç å™¨ä¿¡æ¯
                codec = video_info.get('codec_name', '')
                if not codec:
                    logger.warning(f"æ— æ³•è·å–è§†é¢‘ç¼–ç å™¨ä¿¡æ¯: {video_path}")
                
                logger.debug(f"è§†é¢‘å®Œæ•´æ€§éªŒè¯é€šè¿‡: {os.path.basename(video_path)} "
                           f"({width}x{height}, {duration:.2f}s, {codec})")
                return True
                
            except Exception as ffprobe_error:
                logger.error(f"ffprobeéªŒè¯å¤±è´¥ {video_path}: {ffprobe_error}")
                return False
                
        except Exception as e:
            logger.error(f"è§†é¢‘å®Œæ•´æ€§éªŒè¯å¼‚å¸¸ {video_path}: {e}")
            return False
    
    def resolve_hash_conflict(self, video_path: str, hash_value: str, existing_record: Dict) -> str:
        """è§£å†³å“ˆå¸Œå†²çªï¼šç›¸åŒå“ˆå¸Œä½†ä¸åŒæ–‡ä»¶çš„æƒ…å†µ"""
        try:
            video_name = os.path.basename(video_path)
            existing_path = existing_record.get('input_path', '')
            existing_name = existing_record.get('video_name', '')
            
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å“ˆå¸Œå†²çª:")
            logger.warning(f"   å½“å‰æ–‡ä»¶: {video_path}")
            logger.warning(f"   å·²å­˜åœ¨æ–‡ä»¶: {existing_path}")
            logger.warning(f"   å“ˆå¸Œå€¼: {hash_value[:16]}...")
            
            # ç­–ç•¥1ï¼šå¦‚æœæ˜¯ç›¸åŒè·¯å¾„ï¼Œè®¤ä¸ºæ˜¯åŒä¸€æ–‡ä»¶
            if video_path == existing_path:
                logger.info(f"âœ… ç¡®è®¤ä¸ºåŒä¸€æ–‡ä»¶ (è·¯å¾„å®Œå…¨åŒ¹é…)")
                return "SAME_FILE"
            
            # ç­–ç•¥2ï¼šå¦‚æœæ–‡ä»¶åç›¸åŒä¸”å¤§å°ç›¸åŒï¼Œå¯èƒ½æ˜¯ç§»åŠ¨äº†ä½ç½®
            current_size = os.path.getsize(video_path)
            existing_size = existing_record.get('pre_processing_size', 0)
            
            if video_name == existing_name and current_size == existing_size:
                logger.info(f"âœ… å¯èƒ½æ˜¯ç›¸åŒæ–‡ä»¶çš„ä¸åŒä½ç½® (åç§°+å¤§å°åŒ¹é…)")
                return "MOVED_FILE"
            
            # ç­–ç•¥3ï¼šé‡æ–°è®¡ç®—å®Œæ•´å“ˆå¸Œè¿›è¡Œå¯¹æ¯”
            logger.info(f"ğŸ”„ é‡æ–°è®¡ç®—å®Œæ•´å“ˆå¸Œè¿›è¡Œç²¾ç¡®å¯¹æ¯”...")
            full_hash = self.calculate_video_sha256(video_path, quick_mode=False)
            
            if full_hash == hash_value:
                logger.warning(f"âš ï¸ çœŸå®å“ˆå¸Œå†²çª - éœ€è¦äººå·¥å¹²é¢„")
                return "REAL_CONFLICT"
            else:
                logger.info(f"âœ… å¿«é€Ÿå“ˆå¸Œå†²çªï¼Œå®Œæ•´å“ˆå¸Œä¸åŒ - ä¸ºä¸åŒæ–‡ä»¶")
                return "DIFFERENT_FILE"
                
        except Exception as e:
            logger.error(f"è§£å†³å“ˆå¸Œå†²çªå¤±è´¥: {e}")
            return "RESOLUTION_ERROR"
    
    def quick_database_check_before_cache(self, video_path: str) -> Tuple[bool, Optional[Dict]]:
        """ç¼“å­˜å‰å¿«é€Ÿæ•°æ®åº“æ£€æŸ¥ - æ ¹æ®ç”µè„‘å·å’Œè§†é¢‘åå¿«é€Ÿåˆ¤æ–­æ˜¯å¦å·²å¤„ç†"""
        if not self.db_manager.is_available():
            return False, None
        
        video_name = os.path.basename(video_path)
        
        try:
            # å¿«é€ŸæŸ¥è¯¢ï¼šæŒ‰ç…§ç”µè„‘å·å’Œè§†é¢‘åæŸ¥æ‰¾ status=1 çš„è®°å½•
            query = """
                SELECT id, computer_name, computer_ip, video_name, output_path, updated_time
                FROM Video_Editing 
                WHERE video_name = %s AND status = 1
                ORDER BY updated_time DESC
                LIMIT 1
            """
            params = (video_name,)
            results = self.db_manager.fetch_all_with_retry(query, params)
            
            if results and len(results) > 0:
                record = results[0]
                db_record = {
                    'id': record[0],
                    'computer_name': record[1], 
                    'computer_ip': record[2],
                    'video_name': record[3],
                    'output_path': record[4],
                    'updated_time': record[5]
                }
                
                # å¦‚æœæ˜¯æœ¬ç”µè„‘å¤„ç†çš„ï¼Œç›´æ¥è·³è¿‡
                if record[1] == self.computer_name:
                    logging.info(f"ğŸ” å¿«é€Ÿæ£€æŸ¥: æœ¬ç”µè„‘å·²å¤„ç†è¿‡ {video_name}")
                    return True, db_record
                
                # å¦‚æœæ˜¯å…¶ä»–ç”µè„‘å¤„ç†çš„ï¼Œä¹Ÿè·³è¿‡ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
                logging.info(f"ğŸ” å¿«é€Ÿæ£€æŸ¥: å…¶ä»–ç”µè„‘({record[1]})å·²å¤„ç†è¿‡ {video_name}")
                return True, db_record
            
            # æœªæ‰¾åˆ°å·²å¤„ç†è®°å½•
            logging.debug(f"ğŸ” å¿«é€Ÿæ£€æŸ¥: {video_name} æœªæ‰¾åˆ°å¤„ç†è®°å½•ï¼Œéœ€è¦ç¼“å­˜")
            return False, None
            
        except Exception as e:
            logging.error(f"å¿«é€Ÿæ•°æ®åº“æ£€æŸ¥å¤±è´¥ {video_name}: {e}")
            return False, None

    def is_video_processed(self, video_path: str, hash_value: str = None) -> Tuple[bool, Optional[Dict]]:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦å·²è¢«å¤„ç†ï¼ˆä½¿ç”¨æ–°çš„ç»„åˆæŸ¥é‡æ–¹æ³•ï¼‰"""
        if not self.db_manager.is_available():
            return False, None
        
        # ç¬¬ä¸€æ­¥ï¼šéªŒè¯è§†é¢‘æ–‡ä»¶å®Œæ•´æ€§
        if not self.verify_video_integrity(video_path):
            logger.error(f"è§†é¢‘æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œè·³è¿‡å¤„ç†: {video_path}")
            return False, None
            
        # è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        try:
            pre_processing_size = os.path.getsize(video_path)
            video_name = os.path.basename(video_path)
            video_info = self.get_video_info_fast(video_path)
            resolution = f"{video_info.get('width', 0)}x{video_info.get('height', 0)}" if video_info else "unknown"
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {video_path}: {e}")
            return False, None
            
        if not hash_value:
            # ä½¿ç”¨å¿«é€Ÿå“ˆå¸Œè¿›è¡Œæ•°æ®åº“æŸ¥è¯¢
            hash_value = self.calculate_file_hash_fast(video_path)
            if not hash_value:
                logger.warning(f"æ— æ³•è®¡ç®—å“ˆå¸Œå€¼: {video_path}")
                return False, None
        
        # ä½¿ç”¨æ–°çš„æŸ¥é‡æ–¹æ³•ï¼šcheck_if_processed_successfullyï¼ˆå››é‡éªŒè¯ï¼‰
        logger.info(f"ğŸ” å¼€å§‹æ–°æŸ¥é‡æ£€æŸ¥: {video_name} | å¤§å°:{pre_processing_size} | åˆ†è¾¨ç‡:{resolution}")
        try:
            is_processed = self.check_if_processed_successfully(
                video_name=video_name,
                pre_processing_size=pre_processing_size,
                resolution=resolution,
                hash_value=hash_value
            )
            
            if is_processed:
                # è·å–è¯¦ç»†è®°å½•ä¿¡æ¯
                detail_query = """
                    SELECT id, input_path, output_path, video_name, pre_processing_size, 
                           resolution, hash_value, operator, computer_name, computer_ip, 
                           status, created_time, updated_time
                    FROM Video_Editing 
                    WHERE video_name = %s 
                    AND pre_processing_size = %s 
                    AND resolution = %s
                    AND hash_value = %s
                    AND status = 1
                    ORDER BY updated_time DESC LIMIT 1
                """
                params = (video_name, pre_processing_size, resolution, hash_value)
                results = self.db_manager.fetch_all_with_retry(detail_query, params)
                record = results[0] if results else None
                
                if record:
                    logger.info(f"ğŸ” å‘ç°å·²å¤„ç†è§†é¢‘ (å››é‡åŒ¹é…): {video_name} "
                              f"(å¤„ç†è€…: {record.get('computer_name', 'unknown')}, "
                              f"å¤„ç†æ—¶é—´: {record.get('updated_time', 'unknown')})")
                    return True, record
                else:
                    logger.info(f"ğŸ” å‘ç°å·²å¤„ç†è§†é¢‘ (å››é‡åŒ¹é…): {video_name}")
                    return True, None
            
            return False, None
            
        except Exception as e:
            logger.error(f"æŸ¥é‡æ£€æŸ¥å¤±è´¥ {video_path}: {e}")
            return False, None
    
    def is_video_processing(self, video_path: str, hash_value: str = None) -> Tuple[bool, Optional[Dict]]:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦æ­£åœ¨è¢«å…¶ä»–ç”µè„‘å¤„ç†ï¼ˆä½¿ç”¨å¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼‰"""
        if not self.db_manager.is_available():
            return False, None
            
        if not hash_value:
            hash_value = self.calculate_video_sha256(video_path)
            if not hash_value:
                return False, None
        
        # åœ¨æ–°è¡¨ç»“æ„ä¸­ï¼Œæˆ‘ä»¬åªæ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒhash_valueä¸”çŠ¶æ€ä¸ºæˆåŠŸçš„è®°å½•
        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è®¤ä¸ºå¯ä»¥å¤„ç†ï¼ˆç®€åŒ–é€»è¾‘ï¼Œé¿å…å¤æ‚çš„æ­£åœ¨å¤„ç†çŠ¶æ€ç®¡ç†ï¼‰
        query = """
            SELECT * FROM Video_Editing 
            WHERE hash_value = %s AND status = 1
            ORDER BY updated_time DESC LIMIT 1
        """
        
        results = self.db_manager.fetch_all_with_retry(query, (hash_value,))
        result = results[0] if results else None
        
        if result:
            logger.info(f"ğŸ”„ è§†é¢‘å·²è¢«å¤„ç†: {os.path.basename(video_path)} "
                      f"(å¤„ç†è€…: {result['computer_name']}, "
                      f"å®Œæˆæ—¶é—´: {result['updated_time']})")
            return True, result
            
        return False, None
    
    def _cleanup_zombie_record(self, record_id: int):
        """æ¸…ç†åƒµå°¸å¤„ç†è®°å½•ï¼ˆåœ¨æ–°è¡¨ç»“æ„ä¸­ï¼Œç›´æ¥åˆ é™¤è®°å½•ï¼‰"""
        try:
            with self.db_manager.get_connection() as connection:
                if not connection or not connection.is_connected():
                    return
                
                cursor = connection.cursor()
                cursor.execute(
                    "DELETE FROM Video_Editing WHERE id = %s",
                    (record_id,)
                )
                connection.commit()
                
        except Error as e:
            error_code = getattr(e, 'errno', 0)
            logger.error(f"âŒ æ¸…ç†åƒµå°¸è®°å½•å¤±è´¥ (é”™è¯¯ä»£ç : {error_code}): {e}")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†åƒµå°¸è®°å½•æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
    
    def start_processing(self, video_path: str, hash_value: str = None, 
                        video_info: Dict = None) -> bool:
        """æ ‡è®°è§†é¢‘å¼€å§‹å¤„ç†ï¼ˆåœ¨æ–°è¡¨ç»“æ„ä¸­æš‚æ—¶è·³è¿‡ï¼Œå¤„ç†å®Œæˆåç›´æ¥æ’å…¥æˆåŠŸè®°å½•ï¼‰"""
        # åœ¨æ–°çš„è¡¨ç»“æ„ä¸­ï¼Œæˆ‘ä»¬ç®€åŒ–æµç¨‹ï¼šä¸æ’å…¥å¤„ç†ä¸­çŠ¶æ€ï¼Œç›´æ¥åœ¨å®Œæˆæ—¶æ’å…¥æˆåŠŸè®°å½•
        logger.info(f"ğŸ“ å‡†å¤‡å¤„ç†: {os.path.basename(video_path)}")
        return True
    
    def complete_processing(self, video_path: str, output_path: str, 
                          processing_time: float, hash_value: str = None) -> bool:
        """æ ‡è®°è§†é¢‘å¤„ç†å®Œæˆï¼ˆä½¿ç”¨å¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼‰"""
        if not self.db_manager.is_available():
            return True
            
        if not hash_value:
            hash_value = self.calculate_video_sha256(video_path)
            if not hash_value:
                return True
        
        video_name = os.path.basename(video_path)
        
        # è·å–è§†é¢‘ä¿¡æ¯
        pre_processing_size = os.path.getsize(video_path) if os.path.exists(video_path) else 0
        resolution = None
        try:
            video_info = get_video_info(video_path)
            if video_info:
                width = video_info.get('width', 0)
                height = video_info.get('height', 0)
                if width and height:
                    resolution = f"{width}x{height}"
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        
        # ä½¿ç”¨å¢å¼ºçš„æ•°æ®åº“æ‰§è¡Œæ–¹æ³•
        insert_query = """
            INSERT INTO Video_Editing 
            (input_path, output_path, video_name, pre_processing_size, resolution, 
             hash_value, operator, computer_name, computer_ip, status, log_path)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, 1, %s)
            ON DUPLICATE KEY UPDATE
            output_path = VALUES(output_path),
            status = 1,
            updated_time = CURRENT_TIMESTAMP
        """
        
        params = (video_path, output_path, video_name, pre_processing_size, resolution, hash_value,
                 PROCESSOR_NAME, self.computer_name, self.computer_ip, self.log_file_path)
        
        if self.db_manager.execute_with_retry(insert_query, params):
            logger.info(f"âœ… æ ‡è®°å¤„ç†å®Œæˆ: {video_name} (å¤„ç†æ—¶é—´: {processing_time:.2f}s)")
            return True
        else:
            logger.error(f"âŒ æ ‡è®°å¤„ç†å®Œæˆå¤±è´¥: {video_name}")
            return True  # ä»ç„¶è¿”å›Trueï¼Œä¸å½±å“è§†é¢‘å¤„ç†æµç¨‹
    
    def fail_processing(self, video_path: str, error_message: str, 
                       hash_value: str = None) -> bool:
        """æ ‡è®°è§†é¢‘å¤„ç†å¤±è´¥ï¼ˆä½¿ç”¨å¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼‰"""
        if not self.db_manager.is_available():
            return True
            
        if not hash_value:
            hash_value = self.calculate_video_sha256(video_path)
            if not hash_value:
                return True
        
        video_name = os.path.basename(video_path)
        
        # è·å–è§†é¢‘ä¿¡æ¯
        pre_processing_size = os.path.getsize(video_path) if os.path.exists(video_path) else 0
        resolution = None
        try:
            video_info = get_video_info(video_path)
            if video_info:
                width = video_info.get('width', 0)
                height = video_info.get('height', 0)
                if width and height:
                    resolution = f"{width}x{height}"
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        
        # ä½¿ç”¨å¢å¼ºçš„æ•°æ®åº“æ‰§è¡Œæ–¹æ³•
        insert_query = """
            INSERT INTO Video_Editing 
            (input_path, output_path, video_name, pre_processing_size, resolution, 
             hash_value, operator, computer_name, computer_ip, status, log_path)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, 0, %s)
            ON DUPLICATE KEY UPDATE
            status = 0,
            updated_time = CURRENT_TIMESTAMP
        """
        
        # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦ï¼Œé¿å…æ•°æ®åº“å­—æ®µæº¢å‡º
        truncated_error = error_message[:1000] if error_message else None
        
        params = (video_path, None, video_name, pre_processing_size, resolution, hash_value,
                 PROCESSOR_NAME, self.computer_name, self.computer_ip, self.log_file_path)
        
        if self.db_manager.execute_with_retry(insert_query, params):
            logger.info(f"âŒ æ ‡è®°å¤„ç†å¤±è´¥: {video_name} (é”™è¯¯: {error_message[:100]}...)")
            return True
        else:
            logger.error(f"âŒ æ ‡è®°å¤„ç†å¤±è´¥è®°å½•å¤±è´¥: {video_name}")
            return True  # ä»ç„¶è¿”å›Trueï¼Œä¸å½±å“è§†é¢‘å¤„ç†æµç¨‹
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨å¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼‰"""
        if not self.db_manager.is_available():
            return {}
        
        # æ€»ä½“ç»Ÿè®¡æŸ¥è¯¢
        status_query = """
            SELECT 
                status,
                COUNT(*) as count
            FROM Video_Editing 
            WHERE created_time > DATE_SUB(NOW(), INTERVAL 7 DAY)
            GROUP BY status
        """
        
        # å„ç”µè„‘ç»Ÿè®¡æŸ¥è¯¢
        computer_query = """
            SELECT 
                computer_name,
                COUNT(*) as total,
                SUM(CASE WHEN status = 1 THEN 1 ELSE 0 END) as completed
            FROM Video_Editing 
            WHERE created_time > DATE_SUB(NOW(), INTERVAL 7 DAY)
            GROUP BY computer_name
            ORDER BY completed DESC
        """
        
        # ä½¿ç”¨å¢å¼ºçš„æ•°æ®åº“æŸ¥è¯¢æ–¹æ³•
        status_stats = self.db_manager.fetch_all_with_retry(status_query)
        computer_stats = self.db_manager.fetch_all_with_retry(computer_query)
        
        return {
            'status_stats': status_stats if status_stats else [],
            'computer_stats': computer_stats if computer_stats else []
        }
    
    # ================== æ–°å¢æ™ºèƒ½æŸ¥é‡ç³»ç»Ÿæ–¹æ³• ==================
    
    def check_if_processed_successfully(self, video_name, pre_processing_size, resolution, hash_value):
        """
        æ ¸å¿ƒæ£€æŸ¥1é˜¶æ®µï¼šè§†é¢‘åã€å¤§å°ã€åˆ†è¾¨ç‡å’Œå“ˆå¸Œå€¼è”åˆæŸ¥é‡åˆ¤æ–­
        åªæœ‰æ‰€æœ‰æ¡ä»¶éƒ½åŒ¹é…æ‰è®¤ä¸ºè¯¥è§†é¢‘å·²è¢«æˆåŠŸå¤„ç†è¿‡ã€‚
        """
        if not all([video_name, pre_processing_size, resolution, hash_value]):
            logger.warning("å»é‡æ£€æŸ¥ç¼ºå°‘å¿…è¦å‚æ•°ï¼Œæ— æ³•æ‰§è¡Œã€‚")
            return False
        
        if not self.db_manager.is_available():
            logger.error("æ— æ³•è¿æ¥æ•°æ®åº“ï¼Œè·³è¿‡å»é‡æ£€æŸ¥ã€‚")
            return False  # æ— æ³•è¿æ¥æ•°æ®åº“ï¼Œé»˜è®¤æœªå¤„ç†ï¼Œä»¥å…æ¼å¤„ç†
        
        try:
            # ã€ä¿é™©æ£€æŸ¥ã€‘ï¼šSQLæŸ¥è¯¢ç¡®ä¿æ¯ä¸ªWHEREæ¡ä»¶
            query = """
                SELECT id FROM Video_Editing
                WHERE video_name = %s
                AND pre_processing_size = %s  
                AND resolution = %s
                AND hash_value = %s
                AND status = 1
            """
            params = (video_name, pre_processing_size, resolution, hash_value)
            results = self.db_manager.fetch_all_with_retry(query, params)
            
            if len(results) > 0:
                logger.info(f"âœ… å››é‡éªŒè¯åŒ¹é…æˆåŠŸ: {video_name}")
                return True
            else:
                logger.debug(f"âŒ å››é‡éªŒè¯æœªåŒ¹é…: {video_name}")
                return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥è§†é¢‘å¤„ç†çŠ¶æ€å‡ºé”™: {e}")
            return False

    def calculate_file_hash_fast(self, input_path: str, chunk_size: int = 1048576, sample_size: int = 3) -> str:
        """
        å¿«é€Ÿè®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œå¸¦æœ‰5æ¬¡é‡è¯•é€»è¾‘
        å¯¹äºå¤§æ–‡ä»¶ï¼Œç›´æ¥é‡‡æ ·å¤´æ–‡ä»¶ã€ä¸­ã€å°¾è®¡ç®—å“ˆå¸Œï¼Œä»¥æé«˜æ•ˆç‡ã€‚
        """
        max_retries = 5  # ã€æ–°å¢ã€‘å®šä¹‰æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay = 5   # ã€æ–°å¢ã€‘å®šä¹‰é‡è¯•é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                pre_processing_size = os.path.getsize(input_path)
                hash_sha256 = hashlib.sha256()

                # å¯¹äºå°æ–‡ä»¶ï¼Œç›´æ¥è®¡ç®—å®Œæ•´å“ˆå¸Œ
                if pre_processing_size <= chunk_size * sample_size:
                    with open(input_path, "rb") as f:
                        for chunk in iter(lambda: f.read(chunk_size), b""):
                            hash_sha256.update(chunk)
                    return hash_sha256.hexdigest()  # æˆåŠŸåˆ™ç›´æ¥è¿”å›

                # å¯¹äºå¤§æ–‡ä»¶ï¼Œé‡‡æ ·è®¡ç®—
                with open(input_path, "rb") as f:
                    # è¯»å–å¼€å¤´
                    f.seek(0)
                    hash_sha256.update(f.read(chunk_size))

                    # è¯»å–ä¸­é—´éƒ¨åˆ†ï¼ˆå¤šä¸ªé‡‡æ ·ç‚¹ï¼‰
                    for i in range(1, sample_size):
                        pos = int(pre_processing_size * i / sample_size) 
                        f.seek(max(0, pos - chunk_size // 2))
                        hash_sha256.update(f.read(chunk_size))

                    # è¯»å–ç»“å°¾
                    f.seek(max(0, pre_processing_size - chunk_size))
                    hash_sha256.update(f.read(chunk_size))

                # æ·»åŠ æ–‡ä»¶å¤§å°ä¿¡æ¯ï¼Œæé«˜å“ˆå¸Œå”¯ä¸€æ€§
                hash_sha256.update(str(pre_processing_size).encode())
                return hash_sha256.hexdigest()  # æˆåŠŸåˆ™ç›´æ¥è¿”å›
            except Exception as e:
                # ã€æ–°å¢ã€‘è¿›è¡Œé”™è¯¯åˆ†æå’Œé‡è¯•ç­–ç•¥
                logger.warning(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {input_path} - {e}")
                if attempt < max_retries - 1:
                    logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                else:
                    # ã€æ–°å¢ã€‘å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None
                    logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå½»åº•å¤±è´¥: {input_path}")
                    return None
        
        # ã€æ–°å¢ã€‘å…œåº•ï¼Œå¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œç¡®ä¿è¿”å› None
        return None

    def calculate_file_hash_full(self, input_path: str, chunk_size: int = 1048576) -> str:
        """è®¡ç®—æ–‡ä»¶çš„å®Œæ•´SHA256å“ˆå¸Œå€¼"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(input_path, "rb") as f:
                for chunk in iter(lambda: f.read(chunk_size), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {input_path}: {e}")
            return None

    def get_video_info_with_ffmpeg(self, input_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨FFmpegè·å–è§†é¢‘ä¿¡æ¯ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        try:
            # æ£€æŸ¥FFmpegæ˜¯å¦å¯ç”¨
            result = subprocess.run(
                ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', '-show_streams', input_path],
                capture_output=True, text=True, timeout=30)

            if result.returncode != 0:
                return None

            info = json.loads(result.stdout)

            # æå–è§†é¢‘æµä¿¡æ¯
            video_stream = None
            for stream in info.get('streams', []):
                if stream.get('codec_type') == 'video':
                    video_stream = stream
                    break

            if not video_stream:
                return None

            # è·å–è§†é¢‘ä¿¡æ¯
            pre_processing_size = os.path.getsize(input_path)
            duration = float(info['format'].get('duration', 0))
            frame_rate = self._parse_frame_rate(video_stream.get('r_frame_rate', '0/0'))
            width = video_stream.get('width', 0)
            height = video_stream.get('height', 0)
            resolution = f"{width}x{height}"

            return {
                'pre_processing_size': pre_processing_size,
                'duration': duration,
                'frame_rate': frame_rate,
                'resolution': resolution
            }
        except Exception as e:
            logger.warning(f"ä½¿ç”¨FFmpegè·å–è§†é¢‘ä¿¡æ¯å¤±è´¥ {input_path}: {e}")
            return None

    def _parse_frame_rate(self, rate_str: str) -> float:
        """è§£æå¸§ç‡å­—ç¬¦ä¸²"""
        try:
            if '/' in rate_str:
                num, den = rate_str.split('/')
                if float(den) != 0:
                    return float(num) / float(den)
            return float(rate_str) if rate_str else 0
        except:
            return 0

    def get_video_info_fast(self, input_path: str) -> Dict[str, Any]:
        """å¿«é€Ÿè·å–è§†é¢‘ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # é¦–å…ˆå°è¯•ä½¿ç”¨ç°æœ‰çš„get_video_infoå‡½æ•°
        try:
            video_info = get_video_info(input_path)
            if video_info:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                pre_processing_size = os.path.getsize(input_path)
                duration = video_info.get('duration', 0)
                frame_rate = video_info.get('fps', 0)
                width = video_info.get('width', 0)
                height = video_info.get('height', 0)
                resolution = f"{width}x{height}"

                return {
                    'pre_processing_size': pre_processing_size,
                    'duration': duration,
                    'frame_rate': frame_rate,
                    'resolution': resolution
                }
        except Exception as e:
            logger.warning(f"ä½¿ç”¨get_video_infoè·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")

        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨FFmpeg
        return self.get_video_info_with_ffmpeg(input_path)

    def check_file_already_processed(self, input_path: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆé€šè¿‡æ–‡ä»¶è·¯å¾„ï¼‰"""
        if not self.db_manager.is_available():
            return None
            
        try:
            query = f"""
                SELECT id, hash_value, pre_processing_size, created_time 
                FROM {self.videos_table} 
                WHERE input_path = %s
            """
            
            results = self.db_manager.fetch_all_with_retry(query, (input_path,))
            result = results[0] if results else None
            return result
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†å¤±è´¥: {e}")
            return None

    def check_duplicate_by_attributes(self, video_info: Dict[str, Any], exclude_path: str = None) -> List[Dict[str, Any]]:
        """æ ¹æ®å±æ€§æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤è§†é¢‘ï¼ˆæ’é™¤æŒ‡å®šè·¯å¾„ï¼‰"""
        if not self.db_manager.is_available():
            return []
            
        try:
            if exclude_path:
                query = f"""
                    SELECT id, hash_value, input_path, video_name 
                    FROM {self.videos_table} 
                    WHERE pre_processing_size = %s AND resolution = %s
                    AND input_path != %s
                    AND status = 1
                """
                params = (
                    video_info['pre_processing_size'],
                    video_info['resolution'],
                    exclude_path
                )
            else:
                query = f"""
                    SELECT id, hash_value, input_path, video_name 
                    FROM {self.videos_table} 
                    WHERE pre_processing_size = %s AND resolution = %s
                    AND status = 1
                """
                params = (
                    video_info['pre_processing_size'],
                    video_info['resolution']
                )

            results = self.db_manager.fetch_all_with_retry(query, params)
            return results if results else []
        except Exception as e:
            logger.error(f"æ£€æŸ¥é‡å¤è§†é¢‘å¤±è´¥: {e}")
            return []

    # ç§»é™¤record_to_tobe_deletedæ–¹æ³• - ä¸å†ä½¿ç”¨é¢å¤–è¡¨

    def move_duplicate_video(self, input_path: str, moved_path: str) -> Tuple[bool, str]:
        """ç§»åŠ¨é‡å¤è§†é¢‘åˆ°æŒ‡å®šç›®å½•"""
        if not moved_path or not self.duplicate_output_path:
            logger.warning(f"æœªé…ç½®é‡å¤è§†é¢‘è¾“å‡ºè·¯å¾„ï¼Œè·³è¿‡ç§»åŠ¨: {input_path}")
            return False, moved_path
            
        try:
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(moved_path), exist_ok=True)

            # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ åç¼€
            counter = 1
            original_moved_path = moved_path
            while os.path.exists(moved_path):
                name, ext = os.path.splitext(original_moved_path)
                moved_path = f"{name}_{counter}{ext}"
                counter += 1

            # ç§»åŠ¨æ–‡ä»¶
            shutil.move(input_path, moved_path)
            logger.info(f"æˆåŠŸç§»åŠ¨é‡å¤è§†é¢‘: {os.path.basename(input_path)} -> {moved_path}")
            return True, moved_path
        except Exception as e:
            logger.error(f"ç§»åŠ¨é‡å¤è§†é¢‘å¤±è´¥ {input_path}: {e}")
            return False, moved_path

    # ç§»é™¤record_delete_failæ–¹æ³• - ä¸å†ä½¿ç”¨é¢å¤–è¡¨

    # ç§»é™¤record_problem_videoæ–¹æ³• - ä¸å†ä½¿ç”¨é¢å¤–è¡¨

    def insert_video_record(self, input_path: str, video_name: str, hash_value: str, video_info: Dict[str, Any]) -> bool:
        """æ’å…¥è§†é¢‘è®°å½•åˆ°æ•°æ®åº“ - å¸¦é‡è¯•æœºåˆ¶"""
        import time
        
        self.db_insert_total_count += 1  # ç»Ÿè®¡æ€»å°è¯•æ¬¡æ•°
        
        if not self.db_manager.is_available():
            return True
            
        max_retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delays = [1, 2, 5, 10, 30]  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                query = f"""
                    INSERT INTO {self.videos_table} (input_path, output_path, video_name, pre_processing_size, resolution, hash_value, operator, computer_name, computer_ip, status, log_path)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE
                    video_name = VALUES(video_name),
                    pre_processing_size = VALUES(pre_processing_size),
                    resolution = VALUES(resolution),
                    updated_time = CURRENT_TIMESTAMP
                """
                
                params = (
                    input_path,  # input_path (required field)
                    None,       # output_path (null for video records)
                    video_name,  # video_name (required field)
                    video_info.get('pre_processing_size', 0),  # pre_processing_size
                    video_info.get('resolution', ''),  # resolution
                    hash_value, # hash_value (required field)
                    PROCESSOR_NAME,  # operator
                    self.computer_name,  # computer_name
                    getattr(self, 'computer_ip', ''),  # computer_ip
                    1,  # status (required field, 1 = success)
                    getattr(self, 'log_file_path', '')  # log_path
                )
                
                if self.db_manager.execute_with_retry(query, params):
                    self.db_insert_success_count += 1  # ç»Ÿè®¡æˆåŠŸæ¬¡æ•°
                    if attempt > 0:
                        logger.info(f"âœ… é‡è¯•æˆåŠŸæ’å…¥è§†é¢‘è®°å½•: {video_name} (ç¬¬{attempt+1}æ¬¡å°è¯•) (æˆåŠŸ: {self.db_insert_success_count}/{self.db_insert_total_count})")
                    else:
                        logger.info(f"âœ… æˆåŠŸæ’å…¥/æ›´æ–°è§†é¢‘è®°å½•: {video_name} (æˆåŠŸ: {self.db_insert_success_count}/{self.db_insert_total_count})")
                    return True
                else:
                    if attempt < max_retries - 1:
                        delay = retry_delays[attempt]
                        logger.warning(f"âš ï¸ æ’å…¥è§†é¢‘è®°å½•å¤±è´¥: {video_name} (ç¬¬{attempt+1}æ¬¡å°è¯•)ï¼Œ{delay}ç§’åé‡è¯•...")
                        time.sleep(delay)
                        continue
                    else:
                        logger.error(f"âŒ æ’å…¥è§†é¢‘è®°å½•æœ€ç»ˆå¤±è´¥: {video_name} (å·²å°è¯•{max_retries}æ¬¡)")
                        return False
                        
            except Exception as e:
                if attempt < max_retries - 1:
                    delay = retry_delays[attempt]
                    logger.warning(f"âš ï¸ æ’å…¥è§†é¢‘è®°å½•å¼‚å¸¸: {video_name} (ç¬¬{attempt+1}æ¬¡å°è¯•) - {e}ï¼Œ{delay}ç§’åé‡è¯•...")
                    time.sleep(delay)
                    continue
                else:
                    logger.error(f"âŒ æ’å…¥è§†é¢‘è®°å½•æœ€ç»ˆå¼‚å¸¸: {video_name} (å·²å°è¯•{max_retries}æ¬¡) - {e}")
                    return False
        
        return False
    
    def get_db_insert_statistics(self) -> Dict[str, int]:
        """è·å–æ•°æ®åº“æ’å…¥ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'success_count': self.db_insert_success_count,
            'total_count': self.db_insert_total_count,
            'failed_count': self.db_insert_total_count - self.db_insert_success_count,
            'success_rate': (self.db_insert_success_count / self.db_insert_total_count * 100) if self.db_insert_total_count > 0 else 0
        }
    
    def reset_db_insert_statistics(self):
        """é‡ç½®æ•°æ®åº“æ’å…¥ç»Ÿè®¡ä¿¡æ¯"""
        self.db_insert_success_count = 0
        self.db_insert_total_count = 0

    def record_processing_result(self, video_path: str, input_sha256: str = None, 
                               output_path: str = None, output_sha256: str = None,
                               processing_status: str = 'completed', metadata: Dict = None):
        """è®°å½•è§†é¢‘å¤„ç†ç»“æœåˆ° Video_Editing è¡¨
        
        Args:
            video_path: åŸå§‹è§†é¢‘è·¯å¾„
            input_sha256: è¾“å…¥æ–‡ä»¶SHA256
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_sha256: è¾“å‡ºæ–‡ä»¶SHA256
            processing_status: å¤„ç†çŠ¶æ€ ('completed', 'failed', 'skipped')
            metadata: é¢å¤–å…ƒæ•°æ® (å¦‚ROIä¿¡æ¯)
        """
        if not self.db_manager.is_available():
            return
            
        try:
            import socket
            import getpass
            
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            video_name = os.path.basename(video_path)
            pre_processing_size = os.path.getsize(video_path) if os.path.exists(video_path) else 0
            
            # å¦‚æœæ²¡æœ‰æä¾›è¾“å…¥å“ˆå¸Œï¼Œå°è¯•è®¡ç®—
            if not input_sha256 and os.path.exists(video_path):
                try:
                    input_sha256 = self.calculate_video_sha256(video_path, quick_mode=True)
                except Exception as e:
                    logger.debug(f"æ— æ³•è®¡ç®—è¾“å…¥æ–‡ä»¶å“ˆå¸Œ: {e}")
                    input_sha256 = ""
                    
            # è·å–è§†é¢‘åˆ†è¾¨ç‡ä¿¡æ¯
            resolution = "0x0"
            try:
                video_info = self.get_video_info_fast(video_path)
                if video_info:
                    resolution = video_info.get('resolution', '0x0')
            except Exception as e:
                logger.debug(f"æ— æ³•è·å–è§†é¢‘åˆ†è¾¨ç‡: {e}")
            
            # è·å–ç³»ç»Ÿä¿¡æ¯
            try:
                computer_name = COMPUTER_NAME if COMPUTER_NAME else socket.gethostname()
                computer_ip = COMPUTER_IP if COMPUTER_IP else socket.gethostbyname(socket.gethostname())
                operator = PROCESSOR_NAME
            except Exception as e:
                logger.debug(f"æ— æ³•è·å–ç³»ç»Ÿä¿¡æ¯: {e}")
                computer_name = "unknown"
                computer_ip = "unknown"
                operator = "unknown"
            
            # å¤„ç†çŠ¶æ€æ˜ å°„
            status_mapping = {
                'completed': 1,  # æˆåŠŸ
                'failed': 0,     # å¤±è´¥
                'skipped': 0     # è·³è¿‡è§†ä¸ºå¤±è´¥
            }
            status = status_mapping.get(processing_status, 0)
            
            # æ’å…¥åˆ° Video_Editing è¡¨
            query = """
                INSERT INTO Video_Editing (
                    input_path, output_path, video_name, pre_processing_size, 
                    resolution, hash_value, operator, computer_name, computer_ip, status, log_path
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                output_path = VALUES(output_path),
                video_name = VALUES(video_name),
                pre_processing_size = VALUES(pre_processing_size),
                resolution = VALUES(resolution),
                operator = VALUES(operator),
                computer_name = VALUES(computer_name),
                computer_ip = VALUES(computer_ip),
                status = VALUES(status),
                log_path = VALUES(log_path),
                updated_time = CURRENT_TIMESTAMP
            """
            
            params = (
                video_path,                    # input_path
                output_path or '',             # output_path
                video_name,                    # video_name
                pre_processing_size,           # pre_processing_size
                resolution,                    # resolution
                input_sha256 or '',           # hash_value
                operator,                     # operator
                computer_name,                # computer_name
                computer_ip,                  # computer_ip
                status,                       # status
                self.log_file_path or ''      # log_path
            )
            
            # å¸¦é‡è¯•æœºåˆ¶çš„æ‰§è¡Œ
            max_retries = 5
            retry_delays = [1, 2, 5, 10, 30]
            
            for attempt in range(max_retries):
                try:
                    success = self.db_manager.execute_with_retry(query, params)
                    
                    if success:
                        if attempt > 0:
                            logger.info(f"âœ… é‡è¯•æˆåŠŸè®°å½•å¤„ç†ç»“æœåˆ°Video_Editingè¡¨: {video_name} -> {processing_status} (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                        else:
                            logger.info(f"âœ… æˆåŠŸè®°å½•å¤„ç†ç»“æœåˆ°Video_Editingè¡¨: {video_name} -> {processing_status}")
                        break
                    else:
                        if attempt < max_retries - 1:
                            delay = retry_delays[attempt]
                            logger.warning(f"âš ï¸ è®°å½•å¤„ç†ç»“æœåˆ°Video_Editingè¡¨å¤±è´¥: {video_name} (ç¬¬{attempt+1}æ¬¡å°è¯•)ï¼Œ{delay}ç§’åé‡è¯•...")
                            import time
                            time.sleep(delay)
                            continue
                        else:
                            logger.error(f"âŒ è®°å½•å¤„ç†ç»“æœåˆ°Video_Editingè¡¨æœ€ç»ˆå¤±è´¥: {video_name} (å·²å°è¯•{max_retries}æ¬¡)")
                            
                except Exception as e:
                    if attempt < max_retries - 1:
                        delay = retry_delays[attempt]
                        logger.warning(f"âš ï¸ è®°å½•å¤„ç†ç»“æœå¼‚å¸¸: {video_name} (ç¬¬{attempt+1}æ¬¡å°è¯•) - {e}ï¼Œ{delay}ç§’åé‡è¯•...")
                        import time
                        time.sleep(delay)
                        continue
                    else:
                        logger.error(f"âŒ è®°å½•å¤„ç†ç»“æœæœ€ç»ˆå¼‚å¸¸: {video_name} (å·²å°è¯•{max_retries}æ¬¡) - {e}")
                        break
                
        except Exception as e:
            logger.error(f"è®°å½•å¤„ç†ç»“æœå¼‚å¸¸ {os.path.basename(video_path)}: {e}")

    def set_duplicate_output_path(self, output_path: str):
        """è®¾ç½®é‡å¤è§†é¢‘ç§»åŠ¨ç›®æ ‡è·¯å¾„"""
        self.duplicate_output_path = output_path
        if output_path and not os.path.exists(output_path):
            try:
                os.makedirs(output_path, exist_ok=True)
                logger.info(f"åˆ›å»ºé‡å¤è§†é¢‘è¾“å‡ºç›®å½•: {output_path}")
            except Exception as e:
                logger.error(f"åˆ›å»ºé‡å¤è§†é¢‘è¾“å‡ºç›®å½•å¤±è´¥: {e}")

    def enhanced_duplicate_check(self, video_path: str) -> Tuple[bool, str]:
        """
        å¢å¼ºçš„é‡å¤æ£€æŸ¥æµç¨‹
        è¿”å›: (æ˜¯å¦ä¸ºé‡å¤, å¤„ç†ç»“æœæ¶ˆæ¯)
        """
        video_name = os.path.basename(video_path)
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡
            existing_record = self.check_file_already_processed(video_path)
            if existing_record:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹ï¼ˆé€šè¿‡æ–‡ä»¶å¤§å°ï¼‰
                current_pre_processing_size = os.path.getsize(video_path)
                if existing_record['pre_processing_size'] == current_pre_processing_size:
                    logger.info(f"æ–‡ä»¶æœªä¿®æ”¹ï¼Œè·³è¿‡: {video_name}")
                    return True, "æ–‡ä»¶å·²å­˜åœ¨ä¸”æœªä¿®æ”¹"
                else:
                    logger.info(f"æ–‡ä»¶å·²ä¿®æ”¹ï¼Œé‡æ–°å¤„ç†: {video_name}")

            # ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ (ä½¿ç”¨å¿«é€ŸSHA256)
            hash_value = self.calculate_file_hash_fast(video_path)
            if not hash_value:
                error_msg = "æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"
                logger.error(f"{error_msg}: {video_name}")
                # ç›´æ¥è®°å½•åˆ°æ—¥å¿—ï¼Œä¸å†ä½¿ç”¨é¢å¤–è¡¨
                return False, error_msg

            # ç¬¬ä¸‰æ­¥ï¼šè·å–è§†é¢‘ä¿¡æ¯
            video_info = self.get_video_info_fast(video_path)
            if not video_info:
                error_msg = "æ— æ³•è·å–è§†é¢‘ä¿¡æ¯"
                logger.error(f"{error_msg}: {video_name}")
                # ç›´æ¥è®°å½•åˆ°æ—¥å¿—ï¼Œä¸å†ä½¿ç”¨é¢å¤–è¡¨
                return False, error_msg

            # ç¬¬å››æ­¥ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å±æ€§ç›¸åŒçš„è§†é¢‘ï¼ˆæ’é™¤å½“å‰æ–‡ä»¶ï¼‰
            duplicates = self.check_duplicate_by_attributes(video_info, exclude_path=video_path)

            if duplicates:
                logger.info(f"æ‰¾åˆ° {len(duplicates)} ä¸ªå±æ€§ç›¸åŒçš„è§†é¢‘: {video_name}")

                for duplicate in duplicates:
                    # æ£€æŸ¥å“ˆå¸Œå€¼æ˜¯å¦ä¹Ÿç›¸åŒ
                    if duplicate['hash_value'] == hash_value:
                        logger.info(f"å‘ç°å®Œå…¨é‡å¤çš„è§†é¢‘: {duplicate['video_name']}")

                        # ç”Ÿæˆç§»åŠ¨åçš„è·¯å¾„ï¼ˆå¦‚æœé…ç½®äº†é‡å¤è§†é¢‘è¾“å‡ºè·¯å¾„ï¼‰
                        moved_path = None
                        if self.duplicate_output_path:
                            file_ext = os.path.splitext(video_name)[1]
                            new_filename = f"{hash_value}{file_ext}"
                            moved_path = os.path.join(self.duplicate_output_path, new_filename)
                        
                        # å¦‚æœé…ç½®äº†è¾“å‡ºè·¯å¾„ï¼Œåˆ™ç§»åŠ¨æ–‡ä»¶
                        if moved_path:
                            success, final_path = self.move_duplicate_video(video_path, moved_path)
                            if not success:
                                logger.error(f"ç§»åŠ¨é‡å¤è§†é¢‘å¤±è´¥: {video_name}")
                                return True, "å‘ç°é‡å¤ï¼Œä½†ç§»åŠ¨å¤±è´¥"
                            logger.info(f"é‡å¤è§†é¢‘å·²ç§»åŠ¨: {video_name} -> {final_path}")
                        else:
                            logger.info(f"å‘ç°é‡å¤è§†é¢‘ä½†æœªé…ç½®ç§»åŠ¨è·¯å¾„: {video_name}")

                        return True, "å‘ç°é‡å¤è§†é¢‘ï¼Œå·²å¤„ç†"

            # ç¬¬äº”æ­¥ï¼šæ’å…¥å½“å‰è§†é¢‘è®°å½• - å¿…é¡»æˆåŠŸæ‰èƒ½ç»§ç»­
            if existing_record:
                # æ›´æ–°ç°æœ‰è®°å½•
                success = self.insert_video_record(video_path, video_name, hash_value, video_info)
            else:
                # æ’å…¥æ–°è®°å½•
                success = self.insert_video_record(video_path, video_name, hash_value, video_info)
                
            if success:
                return False, "æ–°è§†é¢‘ï¼Œå·²è®°å½•åˆ°æ•°æ®åº“"
            else:
                # æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œç­‰å¾…å¹¶é‡æ–°å°è¯•
                logger.error(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œè§†é¢‘å°†ç­‰å¾…å¤„ç†: {video_name}")
                return True, "æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œè·³è¿‡å¤„ç†ç­‰å¾…é‡è¯•"
                
        except Exception as e:
            error_msg = f"æŸ¥é‡æ£€æŸ¥å¼‚å¸¸: {str(e)}"
            logger.error(f"å¤„ç†è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ {video_path}: {e}")
            # ç›´æ¥è®°å½•åˆ°æ—¥å¿—ï¼Œä¸å†ä½¿ç”¨é¢å¤–è¡¨
            return False, error_msg

class DatabaseSystemManager:
    """æ•°æ®åº“ç³»ç»Ÿç®¡ç†å™¨ - æ•´åˆæ‰€æœ‰æ•°æ®åº“åŠŸèƒ½"""
    
    def __init__(self, computer_name: str, log_file_path: str = None):
        self.computer_name = computer_name
        self.db_manager = DatabaseManager()
        self.db_monitor = DatabaseMonitor(self.db_manager)
        self.record_manager = VideoRecordManager(computer_name, self.db_manager, log_file_path)
        self.last_health_check = 0
        self.health_check_interval = 300  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        
    def initialize_system(self) -> bool:
        """åˆå§‹åŒ–æ•°æ®åº“ç³»ç»Ÿ"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“ç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
            if not self.db_manager.initialize():
                logger.error("âŒ æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
                return False
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_status = self.db_manager.health_check()
            if health_status:
                logger.info("âœ… æ•°æ®åº“å¥åº·æ£€æŸ¥é€šè¿‡")
            else:
                logger.warning("âš ï¸ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œä½†ç³»ç»Ÿå¯ç»§ç»­è¿è¡Œ")
            
            # è·å–æ€§èƒ½æŒ‡æ ‡
            metrics = self.db_manager.get_performance_metrics()
            if metrics:
                logger.info(f"ğŸ“Š æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡: è¿æ¥æ•°={metrics.get('active_connections', 'N/A')}, "
                           f"ç¼“å†²æ± ä½¿ç”¨ç‡={metrics.get('buffer_pool_usage_percent', 'N/A')}%")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.record_manager.get_processing_statistics()
            if stats:
                status_stats = stats.get('status_stats', [])
                total_processed = sum(item.get('count', 0) for item in status_stats)
                logger.info(f"ğŸ“ˆ è¿‘7å¤©å¤„ç†ç»Ÿè®¡: æ€»è®¡{total_processed}ä¸ªè§†é¢‘")
            
            logger.info("âœ… æ•°æ®åº“ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def periodic_health_check(self):
        """å®šæœŸå¥åº·æ£€æŸ¥"""
        current_time = time.time()
        if current_time - self.last_health_check > self.health_check_interval:
            try:
                # è®°å½•å¥åº·çŠ¶æ€
                self.db_monitor.log_health_status()
                
                # æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
                if not self.db_manager.is_available():
                    logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œå°è¯•é‡å»º...")
                    self.db_manager.rebuild_pool_if_needed()
                
                self.last_health_check = current_time
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
    
    def get_system_status_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š"""
        try:
            return self.db_monitor.get_comprehensive_status()
        except Exception as e:
            logger.error(f"âŒ è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Šå¤±è´¥: {e}")
            return {
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def is_video_already_processed(self, video_path: str, hash_value: str = None) -> Tuple[bool, Optional[Dict]]:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦å·²è¢«å¤„ç†"""
        self.periodic_health_check()  # æ‰§è¡Œå®šæœŸæ£€æŸ¥
        return self.record_manager.is_already_processed(video_path, hash_value)
    
    def mark_video_complete(self, video_path: str, output_path: str, 
                          processing_time: float, hash_value: str = None) -> bool:
        """æ ‡è®°è§†é¢‘å¤„ç†å®Œæˆ"""
        return self.record_manager.complete_processing(video_path, output_path, processing_time, hash_value)
    
    def mark_video_failed(self, video_path: str, error_message: str, hash_value: str = None) -> bool:
        """æ ‡è®°è§†é¢‘å¤„ç†å¤±è´¥"""
        return self.record_manager.fail_processing(video_path, error_message, hash_value)
    
    def cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'db_manager') and self.db_manager:
                self.db_manager.cleanup()
            logger.info("âœ… æ•°æ®åº“ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ç³»ç»Ÿèµ„æºæ¸…ç†å¤±è´¥: {e}")

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®æ•°æ®ç»“æ„"""
    video_path: str
    local_path: str
    pre_processing_size: int
    download_time: float
    last_access: float
    is_complete: bool
    priority: int
    access_count: int = 0

class NetworkPerformanceMonitor:
    """ç½‘ç»œæ€§èƒ½ç›‘æ§å™¨ - 2025å¹´æ™ºèƒ½ç›‘æ§"""
    
    def __init__(self):
        self.transfer_history = deque(maxlen=100)
        self.current_speed = 0.0
        self.avg_speed = 0.0
        self.peak_speed = 0.0
        self.error_count = 0
        self.last_test_time = 0
        
    def record_transfer(self, bytes_transferred: int, duration: float):
        """è®°å½•ä¼ è¾“æ€§èƒ½"""
        if duration <= 0:
            return
            
        # è®¡ç®—ä¼ è¾“é€Ÿåº¦ï¼ˆå­—èŠ‚/ç§’ï¼‰
        speed = bytes_transferred / duration
        current_time = time.time()
        
        # è®°å½•ä¼ è¾“å†å²
        self.transfer_history.append((current_time, speed))
        
        # æ›´æ–°å³°å€¼é€Ÿåº¦
        if speed > self.peak_speed:
            self.peak_speed = speed
            
        # è®¡ç®—æ»‘åŠ¨å¹³å‡é€Ÿåº¦ï¼ˆæœ€è¿‘60ç§’å†…çš„æ•°æ®ï¼‰
        recent_transfers = [(t, s) for t, s in self.transfer_history if current_time - t < 60]
        if recent_transfers:
            total_bytes = sum(s * 1 for _, s in recent_transfers)  # å‡è®¾æ¯æ¬¡è®°å½•ä»£è¡¨1ç§’
            self.avg_speed = total_bytes / len(recent_transfers)
        else:
            self.avg_speed = 0
            
        self.current_speed = speed
        
        # æ›´æ–°æœ€åæµ‹è¯•æ—¶é—´
        self.last_test_time = current_time
        
    def get_optimal_chunk_size(self) -> int:
        """æ ¹æ®ç½‘ç»œæ€§èƒ½åŠ¨æ€è°ƒæ•´å—å¤§å°"""
        if self.avg_speed > 100 * 1024 * 1024:  # > 100MB/s
            return 100 * 1024 * 1024  # 100MB chunks
        elif self.avg_speed > 50 * 1024 * 1024:  # > 50MB/s
            return 50 * 1024 * 1024   # 50MB chunks
        elif self.avg_speed > 10 * 1024 * 1024:  # > 10MB/s
            return 25 * 1024 * 1024   # 25MB chunks
        else:
            return 10 * 1024 * 1024   # 10MB chunks
            
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'current_speed_mbps': self.current_speed / (1024 * 1024),
            'avg_speed_mbps': self.avg_speed / (1024 * 1024),
            'peak_speed_mbps': self.peak_speed / (1024 * 1024),
            'error_count': self.error_count,
            'last_test_time': self.last_test_time,
            'transfer_count': len(self.transfer_history)
        }
        
    def record_error(self):
        """è®°å½•ä¼ è¾“é”™è¯¯"""
        self.error_count += 1
        
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.transfer_history.clear()
        self.current_speed = 0.0
        self.avg_speed = 0.0
        self.peak_speed = 0.0
        self.error_count = 0
            

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - 2025å¹´æé™ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        self.cache_dir = Path(LOCAL_CACHE_DIR)
        self.temp_dir = Path(TEMP_PROCESSING_DIR)
        self.cache_entries: Dict[str, CacheEntry] = {}
        self.cache_sha256_map: Dict[str, str] = {}  # SHA256 -> ç¼“å­˜è·¯å¾„æ˜ å°„
        self.download_queue = queue.PriorityQueue()
        self.active_downloads: Dict[str, threading.Thread] = {}
        self.cache_lock = threading.RLock()
        self.total_cache_size = 0
        self._network_monitor = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        # ç¼“å­˜ç»Ÿè®¡
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0
        
        # åˆ›å»ºç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # å»¶è¿ŸåŠ è½½ï¼Œæå‡å¯åŠ¨é€Ÿåº¦
        self._cache_loaded = False
        self._monitors_started = False
        
        logging.info(f"ğŸš€ æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ (å»¶è¿ŸåŠ è½½æ¨¡å¼)")
        logging.info(f"   ç¼“å­˜ç›®å½•: {self.cache_dir}")
        logging.info(f"   ä¸´æ—¶ç›®å½•: {self.temp_dir}")
        logging.info(f"   æœ€å¤§ç¼“å­˜: {MAX_CACHE_SIZE_GB}GB")
    
    @property
    def network_monitor(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ç½‘ç»œç›‘æ§å™¨"""
        if self._network_monitor is None:
            self._network_monitor = NetworkPerformanceMonitor()
        return self._network_monitor
    
    def _ensure_cache_loaded(self):
        """ç¡®ä¿ç¼“å­˜å·²åŠ è½½"""
        if not self._cache_loaded:
            self._scan_existing_cache()
            self._cache_loaded = True
    
    def _ensure_monitors_started(self):
        """ç¡®ä¿ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨"""
        if not self._monitors_started:
            self._start_monitor_threads()
            self._monitors_started = True
        logging.info(f"   ç°æœ‰ç¼“å­˜: {len(self.cache_entries)}ä¸ªæ–‡ä»¶")
        
    def _scan_existing_cache(self):
        """æ‰«æç°æœ‰ç¼“å­˜æ–‡ä»¶å¹¶è®¡ç®—SHA256"""
        try:
            for cache_file in self.cache_dir.glob("*"):
                if cache_file.is_file():
                    pre_processing_size = cache_file.stat().st_size
                    access_time = cache_file.stat().st_atime
                    self.total_cache_size += pre_processing_size
                    
                    # è®¡ç®—SHA256å“ˆå¸Œå€¼
                    try:
                        hash_value = self._calculate_file_sha256(cache_file)
                    except Exception as e:
                        logging.warning(f"è®¡ç®—SHA256å¤±è´¥ {cache_file.name}: {e}")
                        hash_value = ""
                    
                    # ğŸ”§ åˆ›å»ºç¼“å­˜æ¡ç›® - ä½¿ç”¨æ–‡ä»¶åä½œä¸ºé”®ä»¥æ”¯æŒå¿«é€ŸæŸ¥æ‰¾
                    entry = CacheEntry(
                        video_path=f"cached://{cache_file.name}",
                        local_path=str(cache_file),
                        pre_processing_size=pre_processing_size,
                        download_time=cache_file.stat().st_mtime,
                        last_access=access_time,
                        is_complete=True,
                        priority=1,
                        access_count=1
                    )
                    
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé”®æ·»åŠ åˆ°cache_entries
                    # è¿™æ ·å¯ä»¥é€šè¿‡æ–‡ä»¶ååŒ¹é…åˆ°ç¼“å­˜ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡
                    file_basename = cache_file.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
                    self.cache_entries[file_basename] = entry
                    
                    # å»ºç«‹SHA256æ˜ å°„ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ï¼‰
                    if hash_value:
                        self.cache_sha256_map[hash_value] = str(cache_file)
                    
            logging.info(f"æ‰«æåˆ° {len(self.cache_sha256_map)} ä¸ªç¼“å­˜æ–‡ä»¶ï¼Œæ€»å¤§å°: {self.total_cache_size/1024/1024/1024:.2f}GB")
            logging.info(f"å»ºç«‹SHA256æ˜ å°„ {len(self.cache_sha256_map)} æ¡è®°å½•")
        except Exception as e:
            logging.error(f"æ‰«æç¼“å­˜å¤±è´¥: {e}")
            
    def _calculate_file_sha256(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶SHA256å“ˆå¸Œå€¼"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            # åˆ†å—è¯»å–å¤§æ–‡ä»¶
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
        
    def _start_monitor_threads(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        # ç£ç›˜ç©ºé—´ç›‘æ§çº¿ç¨‹
        if MONITOR_DISK_SPACE:
            threading.Thread(target=self._disk_space_monitor, daemon=True).start()
            
        # ç¼“å­˜æ¸…ç†çº¿ç¨‹
        if AUTO_CLEANUP_CACHE:
            threading.Thread(target=self._cache_cleanup_monitor, daemon=True).start()
            
    def _disk_space_monitor(self):
        """ç£ç›˜ç©ºé—´ç›‘æ§çº¿ç¨‹"""
        while True:
            try:
                cache_disk_usage = psutil.disk_usage(str(self.cache_dir))
                free_gb = cache_disk_usage.free / (1024**3)
                
                if free_gb < MIN_FREE_SPACE_GB:
                    logging.warning(f"âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³: {free_gb:.1f}GB < {MIN_FREE_SPACE_GB}GB")
                    self._emergency_cleanup()
                    
                time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logging.error(f"ç£ç›˜ç©ºé—´ç›‘æ§é”™è¯¯: {e}")
                time.sleep(60)
                
    def _cache_cleanup_monitor(self):
        """ç¼“å­˜æ¸…ç†ç›‘æ§çº¿ç¨‹"""
        while True:
            try:
                self._cleanup_old_cache()
                time.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
            except Exception as e:
                logging.error(f"ç¼“å­˜æ¸…ç†é”™è¯¯: {e}")
                time.sleep(60)
                
    def _emergency_cleanup(self):
        """ç´§æ€¥æ¸…ç†ç¼“å­˜"""
        with self.cache_lock:
            # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€è€çš„ç¼“å­˜
            sorted_entries = sorted(
                self.cache_entries.values(),
                key=lambda x: x.last_access
            )
            
            cleaned_size = 0
            target_size = 5 * 1024**3  # æ¸…ç†5GBç©ºé—´
            
            for entry in sorted_entries:
                if cleaned_size >= target_size:
                    break
                    
                try:
                    if os.path.exists(entry.local_path):
                        pre_processing_size = os.path.getsize(entry.local_path)
                        os.remove(entry.local_path)
                        cleaned_size += pre_processing_size
                        self.total_cache_size -= pre_processing_size
                        
                    del self.cache_entries[entry.video_path]
                    logging.info(f"ğŸ§¹ ç´§æ€¥æ¸…ç†ç¼“å­˜: {os.path.basename(entry.local_path)}")
                    
                except Exception as e:
                    logging.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥ {entry.local_path}: {e}")
                    
            logging.info(f"ğŸ§¹ ç´§æ€¥æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾ {cleaned_size/(1024**3):.2f}GB ç©ºé—´")
    
    def _cleanup_old_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        max_cache_bytes = MAX_CACHE_SIZE_GB * 1024**3
        
        with self.cache_lock:
            # å¦‚æœç¼“å­˜è¶…å‡ºé™åˆ¶ï¼Œæ¸…ç†æœ€æ—§çš„æ–‡ä»¶
            if self.total_cache_size > max_cache_bytes:
                excess = self.total_cache_size - max_cache_bytes
                
                sorted_entries = sorted(
                    self.cache_entries.values(),
                    key=lambda x: x.last_access
                )
                
                cleaned_size = 0
                for entry in sorted_entries:
                    if cleaned_size >= excess:
                        break
                        
                    # æ ¹æ®è®¿é—®é¢‘ç‡å’Œæ—¶é—´å†³å®šæ¸…ç†ç­–ç•¥
                    access_age = current_time - entry.last_access
                    if (access_age > 7200 or  # 2å°æ—¶æœªè®¿é—®
                        (access_age > 1800 and entry.access_count < 2)):  # 30åˆ†é’Ÿæœªè®¿é—®ä¸”è®¿é—®æ¬¡æ•°å°‘
                        try:
                            if os.path.exists(entry.local_path):
                                pre_processing_size = os.path.getsize(entry.local_path)
                                os.remove(entry.local_path)
                                cleaned_size += pre_processing_size
                                self.total_cache_size -= pre_processing_size
                                
                            del self.cache_entries[entry.video_path]
                            
                        except Exception as e:
                            logging.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥ {entry.local_path}: {e}")
                            
    def get_cached_path(self, video_path: str) -> Optional[str]:
        """è·å–ç¼“å­˜è·¯å¾„ - ğŸ”§ å¢å¼ºç‰ˆï¼šæ”¯æŒSHA256å®Œæ•´æ€§éªŒè¯å’Œæ–­ç‚¹ç»­ä¼ æ£€æµ‹"""
        self.total_requests += 1
        
        with self.cache_lock:
            # 1ï¸âƒ£ æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„ç¼“å­˜
            if video_path in self.cache_entries:
                entry = self.cache_entries[video_path]
                if entry.is_complete and os.path.exists(entry.local_path):
                    # ğŸ”’ éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆSHA256æ£€æŸ¥ï¼‰
                    if self._verify_cache_integrity(entry.local_path, video_path):
                        entry.last_access = time.time()
                        entry.access_count += 1
                        self.cache_hits += 1
                        logging.info(f"âœ… ç¼“å­˜å‘½ä¸­ä¸”å®Œæ•´: {os.path.basename(video_path)}")
                        return entry.local_path
                    else:
                        logging.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œæ ‡è®°ä¸ºä¸å®Œæ•´: {os.path.basename(video_path)}")
                        entry.is_complete = False
                else:
                    # ç¼“å­˜æ–‡ä»¶æŸåï¼Œç§»é™¤è®°å½•
                    if video_path in self.cache_entries:
                        del self.cache_entries[video_path]
            
            # 2ï¸âƒ£ ğŸ”§ æ”¹è¿›ï¼šé€šè¿‡æ–‡ä»¶åç´¢å¼•æŸ¥æ‰¾ç°æœ‰ç¼“å­˜
            video_name = os.path.basename(video_path)
            video_basename = os.path.splitext(video_name)[0]  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            
            # é¦–å…ˆå°è¯•é€šè¿‡æ–‡ä»¶åç´¢å¼•å¿«é€ŸæŸ¥æ‰¾
            if video_basename in self.cache_entries:
                entry = self.cache_entries[video_basename]
                if entry.is_complete and os.path.exists(entry.local_path):
                    logging.info(f"ğŸ¯ é€šè¿‡æ–‡ä»¶åç´¢å¼•æ‰¾åˆ°ç°æœ‰ç¼“å­˜: {video_name}")
                    entry.last_access = time.time()
                    entry.access_count += 1
                    self.cache_hits += 1
                    return entry.local_path
                else:
                    # æ¸…ç†æ— æ•ˆç¼“å­˜æ¡ç›®
                    del self.cache_entries[video_basename]
            
            # 3ï¸âƒ£ ğŸ”§ å¤‡ç”¨ï¼šé€šè¿‡æ–‡ä»¶åæ¨¡å¼åŒ¹é…æŸ¥æ‰¾ç°æœ‰ç¼“å­˜
            for cache_file in self.cache_dir.glob(f"*_{video_name}"):
                if cache_file.is_file():
                    logging.info(f"ğŸ¯ é€šè¿‡æ–‡ä»¶åæ¨¡å¼æ‰¾åˆ°ç°æœ‰ç¼“å­˜: {video_name}")
                    # åˆ›å»ºç¼“å­˜æ¡ç›®å¹¶æ·»åŠ åˆ°ç´¢å¼•
                    cache_size = cache_file.stat().st_size
                    entry = CacheEntry(
                        video_path=video_path,
                        local_path=str(cache_file),
                        pre_processing_size=cache_size,
                        download_time=cache_file.stat().st_mtime,
                        last_access=time.time(),
                        is_complete=True,
                        priority=1,
                        access_count=1
                    )
                    self.cache_entries[video_path] = entry
                    self.cache_entries[video_basename] = entry  # åŒæ—¶æ·»åŠ åˆ°æ–‡ä»¶åç´¢å¼•
                    self.cache_hits += 1
                    return str(cache_file)
            
            # 3ï¸âƒ£ ğŸ”§ é€šè¿‡SHA256æ˜ å°„æŸ¥æ‰¾ç°æœ‰ç¼“å­˜ï¼ˆå¦‚æœåŸå§‹æ–‡ä»¶å¯è®¿é—®ï¼‰
            try:
                if os.path.exists(video_path):
                    video_sha256 = self._calculate_file_sha256(Path(video_path))
                    if video_sha256 in self.cache_sha256_map:
                        cached_path = self.cache_sha256_map[video_sha256]
                        if os.path.exists(cached_path):
                            logging.info(f"ğŸ¯ é€šè¿‡SHA256æ‰¾åˆ°ç°æœ‰ç¼“å­˜: {os.path.basename(video_path)} ({video_sha256[:8]})")
                            # åˆ›å»ºç¼“å­˜æ¡ç›®
                            cache_size = os.path.getsize(cached_path)
                            entry = CacheEntry(
                                video_path=video_path,
                                local_path=cached_path,
                                pre_processing_size=cache_size,
                                download_time=os.path.getmtime(cached_path),
                                last_access=time.time(),
                                is_complete=True,
                                priority=1,
                                access_count=1
                            )
                            self.cache_entries[video_path] = entry
                            self.cache_hits += 1
                            return cached_path
            except Exception as e:
                logging.debug(f"SHA256æŸ¥æ‰¾å¤±è´¥ {os.path.basename(video_path)}: {e}")
            
            # 4ï¸âƒ£ æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†ç¼“å­˜æ–‡ä»¶ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
            video_name = os.path.basename(video_path)
            cache_file_pattern = f"cache_{hashlib.md5(video_path.encode()).hexdigest()[:8]}_{video_name}"
            potential_cache_path = self.cache_dir / cache_file_pattern
            
            if potential_cache_path.exists():
                cache_size = potential_cache_path.stat().st_size
                try:
                    original_size = os.path.getsize(video_path)
                    completion_ratio = cache_size / original_size if original_size > 0 else 0
                    
                    if completion_ratio >= 0.99:  # 99%ä»¥ä¸Šè®¤ä¸ºå®Œæ•´
                        logging.info(f"ğŸ¯ å‘ç°è¿‘ä¼¼å®Œæ•´ç¼“å­˜æ–‡ä»¶: {video_name} ({completion_ratio:.1%})")
                        # åˆ›å»ºæˆ–æ›´æ–°ç¼“å­˜æ¡ç›®
                        entry = CacheEntry(
                            video_path=video_path,
                            local_path=str(potential_cache_path),
                            pre_processing_size=cache_size,
                            download_time=potential_cache_path.stat().st_mtime,
                            last_access=time.time(),
                            is_complete=True,
                            priority=1,
                            access_count=1
                        )
                        self.cache_entries[video_path] = entry
                        self.cache_hits += 1
                        return str(potential_cache_path)
                    elif completion_ratio >= 0.1:  # 10%ä»¥ä¸Šçš„éƒ¨åˆ†æ–‡ä»¶
                        logging.info(f"ğŸ“‚ å‘ç°éƒ¨åˆ†ç¼“å­˜æ–‡ä»¶: {video_name} ({completion_ratio:.1%}) - å¯ç”¨äºæ–­ç‚¹ç»­ä¼ ")
                        # ä¸è¿”å›è·¯å¾„ï¼Œè®©è°ƒç”¨æ–¹çŸ¥é“éœ€è¦ç»§ç»­ä¸‹è½½
                        return None
                except Exception as e:
                    logging.warning(f"æ£€æŸ¥éƒ¨åˆ†ç¼“å­˜æ–‡ä»¶å¤±è´¥ {video_name}: {e}")
            
            self.cache_misses += 1
            return None
    
    def _verify_cache_integrity(self, cache_path: str, original_path: str) -> bool:
        """éªŒè¯ç¼“å­˜æ–‡ä»¶å®Œæ•´æ€§ - ğŸ”’ é€šè¿‡SHA256å’Œæ–‡ä»¶å¤§å°åŒé‡éªŒè¯"""
        try:
            cache_file = Path(cache_path)
            if not cache_file.exists():
                return False
            
            # ğŸ” æ£€æŸ¥æ–‡ä»¶å¤§å°
            try:
                cache_size = cache_file.stat().st_size
                original_size = os.path.getsize(original_path)
                
                if cache_size != original_size:
                    logging.warning(f"ğŸ“ ç¼“å­˜æ–‡ä»¶å¤§å°ä¸åŒ¹é…: ç¼“å­˜{cache_size} vs åŸå§‹{original_size}")
                    return False
            except Exception as e:
                logging.warning(f"æ–‡ä»¶å¤§å°æ£€æŸ¥å¤±è´¥: {e}")
                return False
            
            # ğŸ” SHA256å®Œæ•´æ€§éªŒè¯ï¼ˆä»…å¯¹å°äº1000MBçš„æ–‡ä»¶è¿›è¡ŒSHA256éªŒè¯ä»¥èŠ‚çœæ—¶é—´ï¼‰
            if cache_size < 1000 * 1024 * 1024:  # 1000MB
                try:
                    cache_sha256 = self._calculate_file_sha256(cache_file)
                    original_sha256 = self._calculate_file_sha256(Path(original_path))
                    
                    if cache_sha256 != original_sha256:
                        logging.warning(f"ğŸ” SHA256æ ¡éªŒå¤±è´¥: {os.path.basename(original_path)}")
                        return False
                    
                    logging.debug(f"âœ… SHA256æ ¡éªŒé€šè¿‡: {os.path.basename(original_path)} ({cache_sha256[:8]})")
                    return True
                    
                except Exception as e:
                    logging.warning(f"SHA256æ ¡éªŒè¿‡ç¨‹å¤±è´¥: {e}")
                    # SHA256å¤±è´¥ä½†å¤§å°åŒ¹é…ï¼Œä»è®¤ä¸ºæœ‰æ•ˆ
                    return True
            else:
                # å¤§æ–‡ä»¶ä»…éªŒè¯å¤§å°ï¼Œè·³è¿‡SHA256ä»¥æå‡æ€§èƒ½
                logging.debug(f"ğŸ“ å¤§æ–‡ä»¶è·³è¿‡SHA256æ ¡éªŒï¼Œä»…éªŒè¯å¤§å°: {os.path.basename(original_path)}")
                return True
                
        except Exception as e:
            logging.error(f"ç¼“å­˜å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
            return False
            
    def start_async_download(self, video_path: str, priority: int = 0) -> bool:
        """å¯åŠ¨å¼‚æ­¥ä¸‹è½½"""
        if video_path in self.active_downloads:
            return True  # å·²åœ¨ä¸‹è½½ä¸­
            
        if video_path in self.cache_entries:
            if self.cache_entries[video_path].is_complete:
                return True  # å·²ä¸‹è½½å®Œæˆ
                
        # æ·»åŠ åˆ°ä¸‹è½½é˜Ÿåˆ—
        self.download_queue.put((priority, time.time(), video_path))
        
        # å¯åŠ¨ä¸‹è½½çº¿ç¨‹
        if len(self.active_downloads) < ASYNC_DOWNLOAD_THREADS:
            thread = threading.Thread(target=self._download_worker, daemon=True)
            thread.start()
            
        return True
        
    def _download_worker(self):
        """ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä¸‹è½½ä»»åŠ¡
                try:
                    priority, timestamp, video_path = self.download_queue.get(timeout=60)
                except queue.Empty:
                    break
                    
                if video_path in self.active_downloads:
                    continue
                    
                # æ ‡è®°ä¸ºæ´»è·ƒä¸‹è½½
                self.active_downloads[video_path] = threading.current_thread()
                
                try:
                    self._download_video_to_cache(video_path)
                finally:
                    # æ¸…ç†æ´»è·ƒä¸‹è½½è®°å½•
                    if video_path in self.active_downloads:
                        del self.active_downloads[video_path]
                    self.download_queue.task_done()
                    
            except Exception as e:
                logging.error(f"ä¸‹è½½å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
                
    def _download_video_to_cache(self, video_path: str) -> bool:
        """ä¸‹è½½è§†é¢‘åˆ°æœ¬åœ°ç¼“å­˜ - ğŸš€ å¢å¼ºç‰ˆï¼šæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€ç½‘ç»œä¸‹è½½å’ŒSHA256å»é‡"""
        try:
            video_name = os.path.basename(video_path)
            
            # ğŸ” é¦–å…ˆæ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å¯ç›´æ¥è®¿é—®ï¼ˆæœ¬åœ°æ–‡ä»¶æˆ–å·²æŒ‚è½½çš„ç½‘ç»œé©±åŠ¨å™¨ï¼‰
            source_accessible = False
            try:
                # å°è¯•å¿«é€Ÿè®¿é—®æ–‡ä»¶ï¼ˆ1ç§’è¶…æ—¶ï¼‰
                if os.path.exists(video_path):
                    test_size = os.path.getsize(video_path)
                    if test_size > 0:
                        source_accessible = True
                        logging.debug(f"âœ… æºæ–‡ä»¶å¯ç›´æ¥è®¿é—®: {video_name}")
            except Exception as e:
                logging.debug(f"âŒ æºæ–‡ä»¶ä¸å¯ç›´æ¥è®¿é—®: {video_name} - {e}")
                source_accessible = False
            
            # å…ˆè®¡ç®—æ–‡ä»¶SHA256ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶ï¼ˆä»…å½“æºæ–‡ä»¶å¯è®¿é—®æ—¶ï¼‰
            source_sha256 = ""
            if source_accessible:
                try:
                    source_sha256 = self._calculate_file_sha256(Path(video_path))
                    if source_sha256 in self.cache_sha256_map:
                        existing_cache_path = self.cache_sha256_map[source_sha256]
                        if Path(existing_cache_path).exists():
                            logging.info(f"ğŸ¯ å‘ç°ç›¸åŒæ–‡ä»¶ç¼“å­˜: {video_name} (SHA256: {source_sha256[:8]})")
                            # æ›´æ–°è®¿é—®æ—¶é—´
                            with self.cache_lock:
                                for entry in self.cache_entries.values():
                                    if entry.local_path == existing_cache_path:
                                        entry.last_access = time.time()
                                        entry.access_count += 1
                                        break
                            return True
                except Exception as e:
                    logging.warning(f"SHA256æ£€æµ‹å¤±è´¥ {video_name}: {e}")
                    source_sha256 = ""
            
            # ğŸš€ å¦‚æœæºæ–‡ä»¶ä¸å¯ç›´æ¥è®¿é—®ï¼Œå°è¯•ç½‘ç»œä¸‹è½½æ–¹å¼
            if not source_accessible:
                return self._download_from_network(video_path, video_name)
            
            # ğŸ”„ æºæ–‡ä»¶å¯è®¿é—®ï¼Œç»§ç»­åŸæœ‰çš„æ–‡ä»¶å¤åˆ¶é€»è¾‘
            return self._copy_file_to_cache(video_path, video_name, source_sha256)
            
        except Exception as e:
            logging.error(f"ç¼“å­˜ä¸‹è½½å¤±è´¥ {video_path}: {e}")
            return False
    
    def _download_from_network(self, video_path: str, video_name: str) -> bool:
        """ä»ç½‘ç»œå…±äº«è®¿é—®æ–‡ä»¶åˆ°ç¼“å­˜"""
        try:
            # ğŸ”§ åªæ”¯æŒSMB/UNCç½‘ç»œå…±äº«è®¿é—®ï¼Œä¸æ”¯æŒHTTP/FTPç­‰ç½‘ç»œä¸‹è½½
            if video_path.startswith(('http://', 'https://', 'ftp://')):
                logging.warning(f"âš ï¸ ä¸æ”¯æŒç½‘ç»œä¸‹è½½åè®® {video_path.split('://')[0]}://, è·³è¿‡: {video_name}")
                return False
            else:
                # ğŸ”§ å°è¯•SMB/ç½‘ç»œå…±äº«è®¿é—®
                return self._try_network_share_access(video_path, video_name)
        except Exception as e:
            logging.error(f"ç½‘ç»œè®¿é—®å¤±è´¥ {video_name}: {e}")
            return False
    
    def _try_network_share_access(self, video_path: str, video_name: str) -> bool:
        """å°è¯•è®¿é—®ç½‘ç»œå…±äº«æ–‡ä»¶"""
        try:
            import time
            max_retries = 3
            retry_delay = 2
            
            for attempt in range(max_retries):
                try:
                    logging.info(f"ğŸ”„ å°è¯•ç½‘ç»œè®¿é—® (ç¬¬{attempt+1}/{max_retries}æ¬¡): {video_name}")
                    
                    # ğŸ• è®¾ç½®è¾ƒé•¿è¶…æ—¶æ—¶é—´è¿›è¡Œç½‘ç»œè®¿é—®ï¼ˆWindowså…¼å®¹ï¼‰
                    import threading
                    import time
                    
                    # ä½¿ç”¨çº¿ç¨‹è¶…æ—¶æœºåˆ¶ï¼ˆWindowså…¼å®¹ï¼‰
                    result = [None]
                    exception = [None]
                    
                    def access_file():
                        try:
                            if os.path.exists(video_path):
                                pre_processing_size = os.path.getsize(video_path)
                                if pre_processing_size > 0:
                                    result[0] = True
                        except Exception as e:
                            exception[0] = e
                    
                    # å¯åŠ¨è®¿é—®çº¿ç¨‹
                    access_thread = threading.Thread(target=access_file, daemon=True)
                    access_thread.start()
                    
                    # ç­‰å¾…30ç§’è¶…æ—¶
                    access_thread.join(timeout=30)
                    
                    if access_thread.is_alive():
                        logging.warning(f"â° ç½‘ç»œè®¿é—®è¶…æ—¶ (30ç§’): {video_name}")
                        continue
                    elif exception[0]:
                        raise exception[0]
                    elif result[0]:
                        logging.info(f"âœ… ç½‘ç»œè®¿é—®æˆåŠŸ: {video_name}")
                        return self._copy_file_to_cache(video_path, video_name, "")
                        
                except (TimeoutError, OSError, IOError) as e:
                    logging.warning(f"âš ï¸ ç½‘ç»œè®¿é—®å¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {video_name} - {e}")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    continue
            
            logging.error(f"âŒ ç½‘ç»œè®¿é—®æœ€ç»ˆå¤±è´¥: {video_name}")
            return False
            
        except Exception as e:
            logging.error(f"ç½‘ç»œå…±äº«è®¿é—®å¼‚å¸¸ {video_name}: {e}")
            return False
    
    def _copy_file_to_cache(self, video_path: str, video_name: str, source_sha256: str) -> bool:
        """å°†æ–‡ä»¶å¤åˆ¶åˆ°ç¼“å­˜ç›®å½•"""
        try:
            
            # ç”Ÿæˆæœ¬åœ°ç¼“å­˜è·¯å¾„
            local_path = self.cache_dir / f"cache_{hashlib.md5(video_path.encode()).hexdigest()[:8]}_{video_name}"
            
            # ğŸ”„ æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            resume_from = 0
            if local_path.exists():
                existing_size = local_path.stat().st_size
                original_size = os.path.getsize(video_path)
                
                if existing_size < original_size:
                    resume_from = existing_size
                    completion_ratio = existing_size / original_size
                    logging.info(f"ğŸ“‚ å‘ç°éƒ¨åˆ†æ–‡ä»¶ï¼Œä» {resume_from:,} å­—èŠ‚å¤„ç»­ä¼  ({completion_ratio:.1%})")
                elif existing_size == original_size:
                    # æ–‡ä»¶å¤§å°ç›¸åŒï¼ŒéªŒè¯å®Œæ•´æ€§
                    if self._verify_cache_integrity(str(local_path), video_path):
                        logging.info(f"âœ… æ–‡ä»¶å·²å®Œæ•´ç¼“å­˜: {video_name}")
                        return True
                    else:
                        logging.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½: {video_name}")
                        resume_from = 0
            
            logging.info(f"ğŸ”„ å¼€å§‹ç¼“å­˜ä¸‹è½½: {video_name}")
            start_time = time.time()
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            try:
                pre_processing_size = os.path.getsize(video_path)
            except:
                logging.error(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {video_path}")
                return False
                
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            cache_disk_usage = psutil.disk_usage(str(self.cache_dir))
            if cache_disk_usage.free < pre_processing_size * 1.2:  # ç•™20%ä½™é‡
                logging.warning(f"ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œè·³è¿‡ç¼“å­˜: {video_name}")
                return False
                
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            with self.cache_lock:
                self.cache_entries[video_path] = CacheEntry(
                    video_path=video_path,
                    local_path=str(local_path),
                    pre_processing_size=pre_processing_size,
                    download_time=start_time,
                    last_access=start_time,
                    is_complete=False,
                    priority=0,
                    access_count=1
                )
            
            # ğŸš€ åˆ†å—å¤åˆ¶æ–‡ä»¶ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
            chunk_size = self.network_monitor.get_optimal_chunk_size()
            
            try:
                # æ‰“å¼€æ–‡ä»¶ï¼šæºæ–‡ä»¶ä»æ–­ç‚¹ä½ç½®å¼€å§‹è¯»å–ï¼Œç›®æ ‡æ–‡ä»¶ä»¥è¿½åŠ æ¨¡å¼å†™å…¥
                with open(video_path, 'rb') as src:
                    # ğŸ¯ æ–­ç‚¹ç»­ä¼ ï¼šè·³åˆ°æŒ‡å®šä½ç½®
                    if resume_from > 0:
                        src.seek(resume_from)
                        
                    # æ ¹æ®æ˜¯å¦ç»­ä¼ é€‰æ‹©å†™å…¥æ¨¡å¼
                    mode = 'ab' if resume_from > 0 else 'wb'
                    with open(local_path, mode) as dst:
                        copied = resume_from  # ä»æ–­ç‚¹å¼€å§‹è®¡ç®—
                        last_progress_report = resume_from
                        
                        while True:
                            chunk_start = time.time()
                            try:
                                chunk = src.read(chunk_size)
                                if not chunk:
                                    break
                                    
                                dst.write(chunk)
                                copied += len(chunk)
                                chunk_duration = time.time() - chunk_start
                                
                                # è®°å½•ä¼ è¾“æ€§èƒ½
                                if chunk_duration > 0:
                                    self.network_monitor.record_transfer(len(chunk), chunk_duration)
                                
                                # ğŸš€ æ˜¾ç¤ºå®æ—¶ä¼ è¾“è¿›åº¦ï¼ˆæ¯1%æˆ–æ¯10MBæ˜¾ç¤ºä¸€æ¬¡ï¼‰
                                progress = copied / pre_processing_size * 100
                                
                                # æ›´é¢‘ç¹çš„è¿›åº¦æ˜¾ç¤ºï¼šæ¯1%æˆ–æ¯10MB
                                should_update = (
                                    (copied - last_progress_report >= 10 * 1024 * 1024) or  # æ¯10MB
                                    (progress - (last_progress_report / pre_processing_size * 100) >= 1)  # æ¯1%
                                )
                                
                                if should_update:
                                    stats = self.network_monitor.get_performance_stats()
                                    resume_info = f" (ç»­ä¼ )" if resume_from > 0 else ""
                                    speed_mb = stats['current_speed_mbps']
                                    avg_speed = stats['avg_speed_mbps']
                                    
                                    # ğŸ“Š è®¡ç®—å‰©ä½™æ—¶é—´
                                    remaining_bytes = pre_processing_size - copied
                                    if speed_mb > 0:
                                        eta_seconds = remaining_bytes / (speed_mb * 1024 * 1024)
                                        if eta_seconds < 60:
                                            eta_str = f"{eta_seconds:.0f}s"
                                        elif eta_seconds < 3600:
                                            eta_str = f"{eta_seconds/60:.1f}min"
                                        else:
                                            eta_str = f"{eta_seconds/3600:.1f}h"
                                    else:
                                        eta_str = "âˆ"
                                    
                                    # ğŸ¯ æ˜¾ç¤ºå®Œæ•´çš„ä¸‹è½½è¿›åº¦ä¿¡æ¯
                                    progress_msg = (f"ğŸ“¥ ç¼“å­˜ä¼ è¾“: {video_name} {progress:.1f}%{resume_info} "
                                                  f"[{copied/(1024**2):.1f}MB/{pre_processing_size/(1024**2):.1f}MB] "
                                                  f"é€Ÿåº¦:{speed_mb:.1f}MB/s å‰©ä½™:{eta_str}")
                                    
                                    # åªåœ¨consoleæ˜¾ç¤ºåŠ¨æ€è¿›åº¦ï¼Œä¸è®°å½•åˆ°æ—¥å¿—é¿å…æ··ä¹±
                                    print(f"\r\033[K{progress_msg}", end="", flush=True)
                                    last_progress_report = copied
                                    
                            except Exception as e:
                                self.network_monitor.record_error()
                                logging.error(f"å—å¤åˆ¶é”™è¯¯: {e}")
                                raise
                                
            except Exception as e:
                logging.error(f"æ–‡ä»¶å¤åˆ¶å¤±è´¥ {video_name}: {e}")
                # æ¸…ç†ä¸å®Œæ•´çš„æ–‡ä»¶
                if local_path.exists():
                    try:
                        local_path.unlink()
                    except:
                        pass
                return False
            
            # æ›´æ–°ç¼“å­˜æ¡ç›®å’ŒSHA256æ˜ å°„
            download_time = time.time() - start_time
            with self.cache_lock:
                # æ›´æ–°ç¼“å­˜æ¡ç›®
                entry = CacheEntry(
                    video_path=video_path,
                    local_path=str(local_path),
                    pre_processing_size=pre_processing_size,
                    download_time=download_time,
                    last_access=time.time(),
                    is_complete=True,
                    priority=1,
                    access_count=1
                )
                self.cache_entries[video_path] = entry
                self.total_cache_size += pre_processing_size
                
                # å»ºç«‹SHA256æ˜ å°„
                if source_sha256:
                    self.cache_sha256_map[source_sha256] = str(local_path)
                    logging.debug(f"å»ºç«‹SHA256æ˜ å°„: {source_sha256[:8]} -> {local_path.name}")
                    
            speed_mb = pre_processing_size / (1024 * 1024) / download_time
            
            # ğŸ‰ ç¼“å­˜å®Œæˆï¼Œè¾“å‡ºæœ€ç»ˆçŠ¶æ€
            final_msg = (f"âœ… ç¼“å­˜å®Œæˆ: {video_name} "
                        f"å¤§å°:{pre_processing_size/(1024**3):.2f}GB "
                        f"è€—æ—¶:{download_time:.1f}s "
                        f"å¹³å‡é€Ÿåº¦:{speed_mb:.1f}MB/s")
            
            print(f"\n{final_msg}")  # æ–°è¡Œæ˜¾ç¤ºå®Œæˆä¿¡æ¯ï¼Œé¿å…è¢«è¿›åº¦æ¡è¦†ç›–
            logging.info(final_msg)
            
            return True
            
        except Exception as e:
            logging.error(f"æ–‡ä»¶å¤åˆ¶åˆ°ç¼“å­˜å¤±è´¥ {video_name}: {e}")
            
            # æ¸…ç†å¤±è´¥çš„ç¼“å­˜æ–‡ä»¶
            try:
                if local_path.exists():
                    local_path.unlink()
                with self.cache_lock:
                    if video_path in self.cache_entries:
                        del self.cache_entries[video_path]
            except:
                pass
                
            return False
            
    def preload_videos(self, video_paths: List[str], current_index: int = 0):
        """æ™ºèƒ½é¢„åŠ è½½è§†é¢‘"""
        # é¢„åŠ è½½å½“å‰è§†é¢‘ä¹‹åçš„å‡ ä¸ªè§†é¢‘
        for i in range(PRELOAD_COUNT):
            next_index = current_index + i + 1
            if next_index < len(video_paths):
                video_path = video_paths[next_index]
                if not self.get_cached_path(video_path):
                    self.start_async_download(video_path, priority=i)
                    
    def wait_for_cache(self, video_path: str, timeout: float = 7200) -> Optional[str]:
        """ç­‰å¾…ç¼“å­˜å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            cached_path = self.get_cached_path(video_path)
            if cached_path:
                return cached_path
                
            # å¦‚æœæ²¡æœ‰åœ¨ä¸‹è½½ä¸­ï¼Œå¯åŠ¨ä¸‹è½½
            if video_path not in self.active_downloads:
                self.start_async_download(video_path, priority=-1)  # é«˜ä¼˜å…ˆçº§
                
            time.sleep(1)
            
        logging.warning(f"â° ç¼“å­˜è¶…æ—¶: {os.path.basename(video_path)}")
        return None
        
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        self._ensure_cache_loaded()  # ç¡®ä¿ç¼“å­˜å·²åŠ è½½
        with self.cache_lock:
            return {
                'total_entries': len(self.cache_entries),
                'total_size_gb': self.total_cache_size / (1024**3),
                'cache_hit_ratio': len([e for e in self.cache_entries.values() if e.is_complete]) / max(len(self.cache_entries), 1) * 100,
                'active_downloads': len(self.active_downloads),
                'avg_network_speed_mbps': self.network_monitor.avg_speed / (1024*1024),
                'peak_network_speed_mbps': self.network_monitor.peak_speed / (1024*1024)
            }
    
    def remove_from_cache(self, video_path: str) -> bool:
        """ä»ç¼“å­˜ä¸­ç§»é™¤è§†é¢‘æ–‡ä»¶ï¼ˆç”¨äºæŸ¥é‡åæ¸…ç†ï¼‰"""
        try:
            with self.cache_lock:
                # æŸ¥æ‰¾å¹¶åˆ é™¤ç¼“å­˜æ¡ç›®
                if video_path in self.cache_entries:
                    entry = self.cache_entries[video_path]
                    
                    # åˆ é™¤ç‰©ç†æ–‡ä»¶
                    if os.path.exists(entry.local_path):
                        pre_processing_size = os.path.getsize(entry.local_path)
                        os.remove(entry.local_path)
                        self.total_cache_size -= pre_processing_size
                        logging.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤ç¼“å­˜æ–‡ä»¶: {entry.local_path}")
                    
                    # ä»å†…å­˜ä¸­ç§»é™¤è®°å½•
                    del self.cache_entries[video_path]
                    
                    # ä»SHA256æ˜ å°„ä¸­ç§»é™¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    sha256_to_remove = None
                    for sha256, path in self.cache_sha256_map.items():
                        if path == entry.local_path:
                            sha256_to_remove = sha256
                            break
                    if sha256_to_remove:
                        del self.cache_sha256_map[sha256_to_remove]
                    
                    logging.info(f"âœ… æˆåŠŸæ¸…ç†é‡å¤è§†é¢‘ç¼“å­˜: {os.path.basename(video_path)}")
                    return True
                else:
                    # å°è¯•æŒ‰æ–‡ä»¶ååŒ¹é…æ¸…ç†
                    video_name = os.path.basename(video_path)
                    for cache_file in self.cache_dir.glob(f"*_{video_name}"):
                        if cache_file.is_file():
                            pre_processing_size = cache_file.stat().st_size
                            cache_file.unlink()
                            self.total_cache_size -= pre_processing_size
                            logging.debug(f"ğŸ—‘ï¸ æŒ‰æ–‡ä»¶ååŒ¹é…åˆ é™¤: {cache_file}")
                            return True
                    
                    logging.debug(f"ğŸ“‚ ç¼“å­˜ä¸­æœªæ‰¾åˆ°æ–‡ä»¶: {os.path.basename(video_path)}")
                    return False
                    
        except Exception as e:
            logging.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥ {video_path}: {e}")
            return False

class I9PerformanceOptimizer:
    """i9å¤„ç†å™¨æ€§èƒ½ä¼˜åŒ–å™¨ - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self):
        """i9æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ– - é˜²æ­»é”åŠ å›ºç‰ˆ"""
        try:
            self.cpu_count = multiprocessing.cpu_count()
            self.is_i9 = self._detect_i9_processor()
            self.numa_nodes = self._detect_numa_topology()
            self.cpu_affinity_map = self._create_cpu_affinity_map()
            
            if self.is_i9 and ENABLE_I9_TURBO:
                # ä½¿ç”¨è¶…æ—¶ä¿æŠ¤æ¥é¿å…æ­»é”
                import threading
                def turbo_thread():
                    try:
                        self._enable_i9_turbo()
                    except Exception as e:
                        logging.warning(f"i9ç¿é¢‘ä¼˜åŒ–çº¿ç¨‹å¼‚å¸¸: {e}")
                
                thread = threading.Thread(target=turbo_thread, daemon=True)
                thread.start()
                thread.join(timeout=30)  # æœ€å¤šç­‰å¾…30ç§’
                if thread.is_alive():
                    logging.warning("âš ï¸  i9ç¿é¢‘ä¼˜åŒ–è¶…æ—¶ï¼Œè·³è¿‡ä½†ä¸å½±å“ä¸»ç¨‹åº")
                
            logging.info(f"ğŸ”¥ i9æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
            logging.info(f"   i9å¤„ç†å™¨: {self.is_i9}")
            logging.info(f"   CPUæ ¸å¿ƒæ•°: {self.cpu_count}")
            logging.info(f"   NUMAèŠ‚ç‚¹: {len(self.numa_nodes)}")
            
        except Exception as e:
            logging.error(f"âš ï¸  i9æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logging.info("ğŸ”„ ä½¿ç”¨é»˜è®¤é…ç½®ç»§ç»­è¿è¡Œï¼Œä¸å½±å“æ ¸å¿ƒåŠŸèƒ½")
            # è®¾ç½®é»˜è®¤å€¼ç¡®ä¿ç¨‹åºèƒ½ç»§ç»­è¿è¡Œ
            self.cpu_count = multiprocessing.cpu_count()
            self.is_i9 = False
            self.numa_nodes = [list(range(self.cpu_count))]
            self.cpu_affinity_map = {}
        
    def _detect_i9_processor(self) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºi9æˆ–é«˜ç«¯å¤„ç†å™¨"""
        try:
            cpu_info = platform.processor().lower()
            cpu_count = multiprocessing.cpu_count()
            
            # ç›´æ¥æ£€æµ‹i9
            if 'i9' in cpu_info or 'intel(r) core(tm) i9' in cpu_info:
                return True
            
            # æ ¹æ®æ ¸å¿ƒæ•°åˆ¤æ–­é«˜ç«¯CPU (16æ ¸å¿ƒä»¥ä¸Šè§†ä¸ºé«˜ç«¯)
            if cpu_count >= 16:
                logging.info(f"æ£€æµ‹åˆ°é«˜æ ¸å¿ƒæ•°CPU ({cpu_count}æ ¸å¿ƒ)ï¼Œå¯ç”¨i9çº§ä¼˜åŒ–")
                return True
            
            # æ£€æµ‹å…¶ä»–é«˜ç«¯CPUæ ‡è¯†
            high_end_keywords = [
                'xeon', 'threadripper', 'ryzen 9', 'ryzen 7 5', 'ryzen 7 7',
                'i7-12', 'i7-13', 'i7-14', 'i5-13', 'i5-14'
            ]
            
            for keyword in high_end_keywords:
                if keyword in cpu_info:
                    logging.info(f"æ£€æµ‹åˆ°é«˜ç«¯CPUå…³é”®è¯: {keyword}")
                    return True
                    
            return False
        except Exception as e:
            logging.warning(f"CPUæ£€æµ‹å¤±è´¥: {e}")
            # å¦‚æœæ£€æµ‹å¤±è´¥ä½†æ ¸å¿ƒæ•°è¾ƒå¤šï¼Œä»ç„¶å¯ç”¨ä¼˜åŒ–
            try:
                cpu_count = multiprocessing.cpu_count()
                return cpu_count >= 16
            except:
                return False
    
    def _detect_numa_topology(self) -> List[List[int]]:
        """æ£€æµ‹NUMAæ‹“æ‰‘ç»“æ„"""
        numa_nodes = []
        try:
            if platform.system() == "Windows":
                # Windowsä¸‹çš„NUMAæ£€æµ‹
                cores_per_node = self.cpu_count // 2  # å‡è®¾åŒNUMAèŠ‚ç‚¹
                numa_nodes.append(list(range(0, cores_per_node)))
                numa_nodes.append(list(range(cores_per_node, self.cpu_count)))
            else:
                # Linuxä¸‹çš„NUMAæ£€æµ‹
                numa_nodes = [list(range(self.cpu_count))]
                
        except Exception as e:
            logging.warning(f"NUMAæ£€æµ‹å¤±è´¥: {e}")
            numa_nodes = [list(range(self.cpu_count))]
            
        return numa_nodes
            
        
    def _create_cpu_affinity_map(self) -> Dict[str, List[int]]:
        """åˆ›å»ºCPUäº²å’Œæ€§æ˜ å°„"""
        if not CPU_AFFINITY_OPTIMIZATION:
            return {}
            
        # ä¸ºä¸åŒä»»åŠ¡ç±»å‹åˆ†é…ä¸“ç”¨æ ¸å¿ƒ
        total_cores = self.cpu_count
        
        return {
            'ffmpeg_encoding': list(range(0, total_cores // 2)),  # ç¼–ç ä»»åŠ¡ä½¿ç”¨å‰ä¸€åŠæ ¸å¿ƒ
            'file_io': list(range(total_cores // 2, total_cores // 2 + 2)),  # I/Oä»»åŠ¡ä½¿ç”¨2ä¸ªæ ¸å¿ƒ
            'download': list(range(total_cores // 2 + 2, total_cores // 2 + 4)),  # ä¸‹è½½ä»»åŠ¡ä½¿ç”¨2ä¸ªæ ¸å¿ƒ
            'system': list(range(total_cores // 2 + 4, total_cores))  # ç³»ç»Ÿä»»åŠ¡ä½¿ç”¨å…¶ä½™æ ¸å¿ƒ
        }
        
    def _enable_i9_turbo(self):
        """å¯ç”¨i9ç¿é¢‘ä¼˜åŒ– - é˜²æ­»é”åŠ å›ºç‰ˆ"""
        try:
            if platform.system() == "Windows":
                # Windowsä¸‹è®¾ç½®é«˜æ€§èƒ½ç”µæºè®¡åˆ’ - æ·»åŠ è¶…æ—¶ä¿æŠ¤
                try:
                    result1 = subprocess.run([
                        'powercfg', '/setactive', '8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c'
                    ], capture_output=True, check=False, timeout=10)  # 10ç§’è¶…æ—¶
                    logging.debug(f"ç”µæºè®¡åˆ’è®¾ç½®ç»“æœ: {result1.returncode}")
                except subprocess.TimeoutExpired:
                    logging.warning("ç”µæºè®¡åˆ’è®¾ç½®è¶…æ—¶ï¼Œè·³è¿‡")
                except Exception as e:
                    logging.warning(f"ç”µæºè®¡åˆ’è®¾ç½®å¤±è´¥: {e}")
                
                # è®¾ç½®å¤„ç†å™¨çŠ¶æ€ä¸º100% - æ·»åŠ è¶…æ—¶ä¿æŠ¤
                try:
                    result2 = subprocess.run([
                        'powercfg', '/setacvalueindex', 'scheme_current', 
                        'sub_processor', 'procthrottlemax', '100'
                    ], capture_output=True, check=False, timeout=10)  # 10ç§’è¶…æ—¶
                    logging.debug(f"å¤„ç†å™¨çŠ¶æ€è®¾ç½®ç»“æœ: {result2.returncode}")
                except subprocess.TimeoutExpired:
                    logging.warning("å¤„ç†å™¨çŠ¶æ€è®¾ç½®è¶…æ—¶ï¼Œè·³è¿‡")
                except Exception as e:
                    logging.warning(f"å¤„ç†å™¨çŠ¶æ€è®¾ç½®å¤±è´¥: {e}")
                
                logging.info("ğŸš€ i9ç¿é¢‘ä¼˜åŒ–å·²å¯ç”¨")
                
        except Exception as e:
            logging.warning(f"i9ç¿é¢‘ä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")
            logging.info("âš ï¸  i9ä¼˜åŒ–å¤±è´¥ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œ")
            
    def set_process_affinity(self, process: subprocess.Popen, task_type: str = 'ffmpeg_encoding'):
        """è®¾ç½®è¿›ç¨‹CPUäº²å’Œæ€§ - é˜²æ­»é”åŠ å›ºç‰ˆ"""
        if not CPU_AFFINITY_OPTIMIZATION or task_type not in self.cpu_affinity_map:
            return
            
        try:
            cpu_list = self.cpu_affinity_map[task_type]
            if platform.system() == "Windows":
                # Windowsä¸‹è®¾ç½®äº²å’Œæ€§ - æ·»åŠ å¼‚å¸¸ä¿æŠ¤
                try:
                    import ctypes
                    handle = ctypes.windll.kernel32.OpenProcess(0x0200, False, process.pid)
                    if handle:
                        mask = sum(1 << cpu for cpu in cpu_list)
                        result = ctypes.windll.kernel32.SetProcessAffinityMask(handle, mask)
                        ctypes.windll.kernel32.CloseHandle(handle)
                        if not result:
                            logging.debug(f"CPUäº²å’Œæ€§è®¾ç½®å¤±è´¥ï¼Œä½†ä¸å½±å“åŠŸèƒ½")
                except ImportError:
                    logging.debug("ctypesåº“å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡CPUäº²å’Œæ€§è®¾ç½®")
                except Exception as e:
                    logging.debug(f"Windows CPUäº²å’Œæ€§è®¾ç½®å¼‚å¸¸: {e}")
            else:
                # Linuxä¸‹è®¾ç½®äº²å’Œæ€§
                try:
                    os.sched_setaffinity(process.pid, cpu_list)
                except Exception as e:
                    logging.debug(f"Linux CPUäº²å’Œæ€§è®¾ç½®å¤±è´¥: {e}")
                
            logging.debug(f"å°è¯•è®¾ç½®è¿›ç¨‹ {process.pid} äº²å’Œæ€§åˆ°CPU {cpu_list}")
            
        except Exception as e:
            logging.debug(f"CPUäº²å’Œæ€§è®¾ç½®å¤±è´¥ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰: {e}")
            
    def get_optimal_worker_count(self, task_type: str = 'encoding') -> int:
        """è·å–æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°"""
        if self.is_i9:
            if task_type == 'encoding':
                return min(self.cpu_count - 4, 20)  # i9ç¼–ç ä»»åŠ¡
            elif task_type == 'download':
                return min(8, self.cpu_count // 4)  # ä¸‹è½½ä»»åŠ¡
            else:
                return min(self.cpu_count // 2, 12)  # å…¶ä»–ä»»åŠ¡
        else:
            return min(self.cpu_count // 2, 8)

class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å’Œä¼˜åŒ–ç±» - 2025å¹´NASæé™ä¼˜åŒ–ç‰ˆ"""
    
    # ç±»çº§åˆ«ç¼“å­˜ï¼Œé¿å…é‡å¤æ£€æµ‹
    _gpu_cache = None
    _gpu_cache_time = 0
    _cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜
    
    def __init__(self, log_file_path: str = None):
        self.computer_name = self.get_computer_unique_id()
        self.hardware_info = None
        self.i9_optimizer = I9PerformanceOptimizer()
        self.cache_manager = SmartCacheManager()
        
        # ğŸ”— å¤šç”µè„‘åä½œç»„ä»¶
        self.db_manager = DatabaseManager()
        self.video_record_manager = VideoRecordManager(self.computer_name, self.db_manager, log_file_path)

    def get_computer_unique_id(self) -> str:
        """è·å–ç”µè„‘çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        try:
            mac = uuid.getnode()
            mac_str = f"{mac:012x}"
            hostname = socket.gethostname()
            
            disk_serial = "unknown"
            if platform.system() == "Windows":
                try:
                    result = subprocess.run(['wmic', 'diskdrive', 'get', 'serialnumber'], 
                                          capture_output=True, text=True, timeout=10)
                    if result.returncode == 0:
                        lines = result.stdout.strip().split('\n')
                        for line in lines[1:]:
                            serial = line.strip()
                            if serial and serial != "SerialNumber":
                                disk_serial = serial[:16]
                                break
                except Exception:
                    pass
            
            combined = f"{hostname}_{mac_str}_{disk_serial}"
            unique_hash = hashlib.md5(combined.encode()).hexdigest()[:12]
            return f"{hostname}_{unique_hash}"
        
        except Exception as e:
            logging.warning(f"è·å–ç”µè„‘å”¯ä¸€æ ‡è¯†å¤±è´¥: {e}")
            return f"{platform.node()}_{uuid.uuid4().hex[:8]}"

    def detect_hardware_capabilities(self) -> Dict[str, Any]:
        """ğŸš€ 2025å¹´æé™ç¡¬ä»¶æ£€æµ‹å’Œæ€§èƒ½ä¼˜åŒ–"""
        try:
            cpu_count = multiprocessing.cpu_count()
            cpu_freq = psutil.cpu_freq()
            memory = psutil.virtual_memory()
            
            cpu_info = platform.processor()
            is_i9 = self.i9_optimizer.is_i9
            is_high_end = any(x in cpu_info.lower() for x in ['i9', 'i7', 'ryzen 9', 'ryzen 7'])
            
            gpu_info = self.detect_gpu_capabilities()
            
            # ğŸ”¥ 2025å¹´i9æé™ä¼˜åŒ–å‚æ•°
            if is_i9:
                max_parallel = self.i9_optimizer.get_optimal_worker_count('encoding')
                buffer_size = "200M"  # i9å¤„ç†å™¨å¤§ç¼“å†²
                probe_size = "500M"   # i9å¤„ç†å™¨å¤§æ¢æµ‹
                memory_pool_gb = MEMORY_POOL_SIZE_GB
            elif is_high_end:
                max_parallel = min(cpu_count - 1, 16)
                buffer_size = "100M" 
                probe_size = "200M"
                memory_pool_gb = 6
            else:
                max_parallel = min(cpu_count // 2, 8)
                buffer_size = "50M"
                probe_size = "100M"
                memory_pool_gb = 4
            
            # ğŸ§  å†…å­˜æ™ºèƒ½ä¼˜åŒ–
            memory_gb = memory.total / (1024**3)
            if memory_gb >= 64:  # 64GB+å†…å­˜çš„æé™é…ç½®
                max_parallel = min(max_parallel, int(memory_gb // 2))
                memory_pool_gb = min(memory_pool_gb, 16)
            elif memory_gb >= 32:
                max_parallel = min(max_parallel, int(memory_gb // 1.5))
                memory_pool_gb = min(memory_pool_gb, 12)
            elif memory_gb >= 16:
                max_parallel = min(max_parallel, 12)
                memory_pool_gb = min(memory_pool_gb, 8)
            else:
                max_parallel = min(max_parallel, 6)
                memory_pool_gb = 4
            
            if MAX_PARALLEL_WORKERS_OVERRIDE > 0:
                max_parallel = MAX_PARALLEL_WORKERS_OVERRIDE
            
            # ğŸš€ NASç¼“å­˜ä¼˜åŒ–å‚æ•° - å¿«é€Ÿè·å–åŸºç¡€ä¿¡æ¯
            try:
                # ä½¿ç”¨å¿«é€Ÿæ¨¡å¼è·å–ç¼“å­˜ç»Ÿè®¡ï¼Œé¿å…ç½‘ç»œæ£€æµ‹å»¶è¿Ÿ
                cache_stats = {
                    'total_entries': len(self.cache_manager.cache_entries),
                    'total_size_gb': self.cache_manager.total_cache_size / (1024**3),
                    'cache_hit_ratio': 0.0,
                    'active_downloads': len(self.cache_manager.active_downloads),
                    'avg_network_speed_mbps': 0.0,  # å»¶è¿Ÿæ£€æµ‹
                    'peak_network_speed_mbps': 0.0  # å»¶è¿Ÿæ£€æµ‹
                }
            except Exception:
                cache_stats = {
                    'total_entries': 0,
                    'total_size_gb': 0.0,
                    'cache_hit_ratio': 0.0,
                    'active_downloads': 0,
                    'avg_network_speed_mbps': 0.0,
                    'peak_network_speed_mbps': 0.0
                }
            
            hw_info = {
                'cpu_cores': cpu_count,
                'cpu_freq_max': cpu_freq.max if cpu_freq else 0,
                'memory_gb': memory_gb,
                'memory_pool_gb': memory_pool_gb,
                'is_i9': is_i9,
                'is_high_end': is_high_end,
                'max_parallel': max_parallel,
                'buffer_size': buffer_size,
                'probe_size': probe_size,
                'gpu_info': gpu_info,
                'i9_optimizer': self.i9_optimizer,
                'cache_manager': self.cache_manager,
                'cache_stats': cache_stats,
                'numa_nodes': len(self.i9_optimizer.numa_nodes),
                # ğŸ”— å¤šç”µè„‘åä½œç»„ä»¶
                'db_manager': self.db_manager,
                'video_record_manager': self.video_record_manager,
                'computer_name': self.computer_name
            }
            
            hw_info.update(gpu_info)
            
            logging.info(f"ğŸš€ 2025å¹´æé™ç¡¬ä»¶æ£€æµ‹å®Œæˆ:")
            logging.info(f"   CPU: {cpu_count}æ ¸å¿ƒ (i9={is_i9})")
            logging.info(f"   å†…å­˜: {memory_gb:.1f}GB (æ± ={memory_pool_gb}GB)")
            logging.info(f"   å¹¶è¡Œæ•°: {max_parallel} (ç¼“å†²={buffer_size})")
            logging.info(f"   GPU: {gpu_info.get('encoder_type', 'unknown')}")
            logging.info(f"   ç¼“å­˜: {cache_stats['total_entries']}æ¡ç›® ({cache_stats['total_size_gb']:.1f}GB)")
            logging.info(f"   ç½‘é€Ÿ: å»¶è¿Ÿæ£€æµ‹ä¸­... (æå‡å¯åŠ¨é€Ÿåº¦)")
            
            # å¯åŠ¨åå°ä»»åŠ¡å¼‚æ­¥æ›´æ–°ç½‘ç»œé€Ÿåº¦
            threading.Thread(target=self._update_network_stats_async, args=(hw_info,), daemon=True).start()
            
            self.hardware_info = hw_info
            return hw_info
            
        except Exception as e:
            logging.error(f"æé™ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")
            return self.get_fallback_hardware()
    
    def _update_network_stats_async(self, hw_info: Dict[str, Any]):
        """å¼‚æ­¥æ›´æ–°ç½‘ç»œç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ç»™ç³»ç»Ÿä¸€äº›æ—¶é—´å®Œæˆåˆå§‹åŒ–
            time.sleep(2)
            
            # è·å–å®Œæ•´çš„ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç½‘ç»œé€Ÿåº¦ï¼‰
            cache_stats = self.cache_manager.get_cache_stats()
            
            # æ›´æ–°ç¡¬ä»¶ä¿¡æ¯
            hw_info['cache_stats'] = cache_stats
            
            logging.info(f"ğŸ“¡ ç½‘ç»œé€Ÿåº¦æ£€æµ‹å®Œæˆ: å¹³å‡{cache_stats['avg_network_speed_mbps']:.1f}MB/s, å³°å€¼{cache_stats['peak_network_speed_mbps']:.1f}MB/s")
            
        except Exception as e:
            logging.warning(f"å¼‚æ­¥ç½‘ç»œæ£€æµ‹å¤±è´¥: {e}")

    def detect_gpu_capabilities(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUèƒ½åŠ›å’Œä¼˜åŒ–ç¼–ç å™¨é€‰æ‹© - å¸¦ç¼“å­˜ä¼˜åŒ–"""
        # æ£€æŸ¥ç¼“å­˜
        current_time = time.time()
        if (HardwareDetector._gpu_cache is not None and 
            current_time - HardwareDetector._gpu_cache_time < HardwareDetector._cache_timeout):
            return HardwareDetector._gpu_cache.copy()
        
        gpu_info = {"encoder_type": "software", "encoder": "libx264", "options": {}}
        
        try:
            # å‡å°‘è¶…æ—¶æ—¶é—´ï¼ŒåŠ å¿«æ£€æµ‹é€Ÿåº¦
            result = subprocess.run([FFMPEG_PATH, '-hide_banner', '-encoders'], 
                                  capture_output=True, text=True, encoding='utf-8', timeout=5)
            
            if result.returncode != 0:
                return gpu_info
            
            encoders_output = result.stdout
            
            # NVIDIAæ£€æµ‹ (ä¼˜å…ˆçº§æœ€é«˜)
            nvidia_encoders = ['h264_nvenc', 'hevc_nvenc']
            for encoder in nvidia_encoders:
                if encoder in encoders_output:
                    gpu_info.update({
                        "encoder_type": "nvidia",
                        "encoder": encoder,
                        "options": self.get_nvidia_options(),
                        "max_parallel": 6 if encoder == 'h264_nvenc' else 4
                    })
                    logging.info(f"æ£€æµ‹åˆ°NVIDIAç¼–ç å™¨: {encoder}")
                    return gpu_info
            
            # AMDæ£€æµ‹
            amd_encoders = ['h264_amf', 'hevc_amf']
            for encoder in amd_encoders:
                if encoder in encoders_output:
                    gpu_info.update({
                        "encoder_type": "amd",
                        "encoder": encoder,
                        "options": self.get_amd_options(),
                        "max_parallel": 4
                    })
                    logging.info(f"æ£€æµ‹åˆ°AMDç¼–ç å™¨: {encoder}")
                    return gpu_info
            
            # Intelæ£€æµ‹
            intel_encoders = ['h264_qsv', 'hevc_qsv']
            for encoder in intel_encoders:
                if encoder in encoders_output:
                    gpu_info.update({
                        "encoder_type": "intel",
                        "encoder": encoder,
                        "options": self.get_intel_options(),
                        "max_parallel": 4
                    })
                    logging.info(f"æ£€æµ‹åˆ°Intelç¼–ç å™¨: {encoder}")
                    return gpu_info
            
            # è½¯ä»¶ç¼–ç å™¨ä¼˜åŒ–
            gpu_info.update({
                "encoder_type": "software",
                "encoder": "libx264",
                "options": self.get_software_options(),
                "max_parallel": min(multiprocessing.cpu_count() // 2, 8)
            })
            logging.info("ä½¿ç”¨ä¼˜åŒ–çš„è½¯ä»¶ç¼–ç å™¨")
            
        except Exception as e:
            logging.warning(f"GPUæ£€æµ‹å¤±è´¥: {e}")
        
        # ç¼“å­˜æ£€æµ‹ç»“æœ
        HardwareDetector._gpu_cache = gpu_info.copy()
        HardwareDetector._gpu_cache_time = time.time()
        
        return gpu_info

    def get_nvidia_options(self) -> Dict[str, str]:
        """NVIDIAç¼–ç å™¨ä¼˜åŒ–å‚æ•°"""
        if QUALITY_MODE == 'highest':
            return {'preset': 'p2', 'rc': 'vbr', 'cq': '20'}
        elif QUALITY_MODE == 'high':
            return {'preset': 'p2', 'rc': 'vbr', 'cq': '23'}
        elif QUALITY_MODE == 'balanced':
            return {'preset': 'p4', 'rc': 'vbr', 'cq': '25'}
        else:  # fast
            return {'preset': 'p6', 'rc': 'cbr'}

    def get_amd_options(self) -> Dict[str, str]:
        """AMDç¼–ç å™¨é€‰é¡¹"""
        return {'quality': 'balanced', 'rc': 'vbr_peak_constrained'}
    
    def get_intel_options(self) -> Dict[str, str]:
        """Intelç¼–ç å™¨é€‰é¡¹"""
        return {'preset': 'fast', 'global_quality': '25'}
    
    def get_software_options(self) -> Dict[str, str]:
        """è½¯ä»¶ç¼–ç å™¨ä¼˜åŒ–å‚æ•°"""
        if QUALITY_MODE == 'highest':
            return {'preset': 'slow', 'crf': '20', 'threads': '0'}
        elif QUALITY_MODE == 'high':
            return {'preset': 'medium', 'crf': '23', 'threads': '0'}
        elif QUALITY_MODE == 'balanced':
            return {'preset': 'fast', 'crf': '25', 'threads': '0'}
        else:  # fast
            return {'preset': 'veryfast', 'crf': '28', 'threads': '0'}

    def get_fallback_hardware(self) -> Dict[str, Any]:
        """å›é€€ç¡¬ä»¶é…ç½®"""
        cpu_count = multiprocessing.cpu_count()
        return {
            "cpu_cores": cpu_count,
            "encoder_type": "software",
            "encoder": "libx264",
            "options": self.get_software_options(),
            "max_parallel": min(cpu_count // 2, 4),
            "buffer_size": "30M",
            "probe_size": "50M"
        }

class ROISelector:
    """ROIåŒºåŸŸé€‰æ‹©å™¨ - å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self):
        self.gui_available = self.init_opencv_gui()
        self.roi_16_9_mode = True  # é»˜è®¤å¯ç”¨16:9å¼ºåˆ¶æ¨¡å¼
        
    def init_opencv_gui(self) -> bool:
        """åˆå§‹åŒ–OpenCV GUIåç«¯"""
        try:
            current_version = cv2.__version__
            logging.info(f"OpenCVç‰ˆæœ¬: {current_version}")
            
            test_img = np.zeros((100, 100, 3), dtype=np.uint8)
            cv2.namedWindow("test_window", cv2.WINDOW_NORMAL)
            cv2.imshow("test_window", test_img)
            cv2.waitKey(1)
            cv2.destroyWindow("test_window")
            logging.info("OpenCV GUIåç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logging.warning(f"OpenCV GUIåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def extract_preview_frame(self, video_path: str) -> Optional[np.ndarray]:
        """æå–è§†é¢‘é¢„è§ˆå¸§"""
        try:
            # è·å–è§†é¢‘æ—¶é•¿
            probe_cmd = [FFPROBE_PATH, '-v', 'error', '-show_entries', 'format=duration', 
                        '-of', 'default=noprint_wrappers=1:nokey=1', video_path]
            result = subprocess.run(probe_cmd, capture_output=True, text=True, encoding='utf-8', timeout=15)
            
            if result.returncode != 0 or not result.stdout.strip():
                return None
                
            try:
                duration = float(result.stdout.strip())
            except ValueError:
                duration = 60  # é»˜è®¤å€¼
            
            # æå–ä¸­é—´å¸§ä½œä¸ºé¢„è§ˆ
            temp_preview_path = Path("temp_preview_frame.jpg")
            extract_cmd = [
                FFMPEG_PATH, '-ss', str(duration / 2), '-i', video_path, 
                '-vframes', '1', '-q:v', '2', str(temp_preview_path), '-y'
            ]
            
            result = subprocess.run(extract_cmd, capture_output=True, timeout=30)
            if result.returncode != 0:
                return None
            
            frame = self._load_frame(temp_preview_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if temp_preview_path.exists():
                    temp_preview_path.unlink()
            except Exception:
                pass
                
            return frame
            
        except Exception as e:
            logging.warning(f"æå–é¢„è§ˆå¸§å¤±è´¥ {video_path}: {e}")
            return None

    def _load_frame(self, temp_path: Path) -> Optional[np.ndarray]:
        """åŠ è½½å¸§å›¾åƒ"""
        try:
            if temp_path.exists() and temp_path.stat().st_size > 1024:
                frame = cv2.imread(str(temp_path))
                if frame is not None and frame.size > 0:
                    return frame
            return None
        except Exception as e:
            logging.warning(f"åŠ è½½å¸§å¤±è´¥: {e}")
            return None

    def adjust_roi_to_16_9(self, roi: Tuple[int, int, int, int], 
                          video_width: int, video_height: int) -> Tuple[Tuple[int, int, int, int], bool]:
        """æ™ºèƒ½è°ƒæ•´ROIä¸º16:9æ¯”ä¾‹"""
        x, y, w, h = roi
        original_roi = roi
        was_adjusted = False
        
        # å¤„ç†è¾¹ç•Œé—®é¢˜
        if x < 0:
            w = w + x
            x = 0
        if y < 0:
            h = h + y
            y = 0
        
        if x + w > video_width:
            w = video_width - x
        if y + h > video_height:
            h = video_height - y
        
        # ç¡®ä¿ROIæœ‰æ•ˆ
        if w <= 0 or h <= 0:
            w = min(video_width * 0.8, video_width)
            h = min(video_height * 0.8, video_height)
            x = (video_width - w) // 2
            y = (video_height - h) // 2
            was_adjusted = True
            logging.warning(f"ROIæ— æ•ˆï¼Œé‡ç½®ä¸ºä¸­å¿ƒåŒºåŸŸ: ({x}, {y}, {w}, {h})")
        
        # è°ƒæ•´ä¸º16:9æ¯”ä¾‹
        target_aspect = 16 / 9
        roi_aspect = w / h if h > 0 else target_aspect
        
        if roi_aspect > target_aspect:
            # ROIæ¯”16:9æ›´å®½ï¼Œä»¥é«˜åº¦ä¸ºå‡†
            new_h = h
            new_w = int(h * target_aspect)
            new_x = x + (w - new_w) // 2
            new_y = y
        else:
            # ROIæ¯”16:9æ›´é«˜ï¼Œä»¥å®½åº¦ä¸ºå‡†
            new_w = w
            new_h = int(w / target_aspect)
            new_x = x
            new_y = y + (h - new_h) // 2
        
        # ç¡®ä¿åœ¨è¾¹ç•Œå†…
        new_x = max(0, min(new_x, video_width - new_w))
        new_y = max(0, min(new_y, video_height - new_h))
        
        # æ£€æŸ¥è°ƒæ•´åæ˜¯å¦ä»è¶…å‡ºè¾¹ç•Œ
        if new_x + new_w > video_width or new_y + new_h > video_height:
            max_w = video_width - new_x
            max_h = video_height - new_y
            
            if max_w / max_h > target_aspect:
                new_h = max_h
                new_w = int(max_h * target_aspect)
            else:
                new_w = max_w
                new_h = int(max_w / target_aspect)
            
            was_adjusted = True
        
        # æ£€æŸ¥æ¯”ä¾‹ç¼©å°
        roi_to_video_ratio_w = new_w / video_width
        roi_to_video_ratio_h = new_h / video_height
        
        max_ratio = 0.9
        if roi_to_video_ratio_w > max_ratio or roi_to_video_ratio_h > max_ratio:
            scale_factor = min(max_ratio / roi_to_video_ratio_w, max_ratio / roi_to_video_ratio_h)
            
            scaled_w = int(new_w * scale_factor)
            scaled_h = int(scaled_w / target_aspect)
            
            center_x = new_x + new_w // 2
            center_y = new_y + new_h // 2
            
            new_x = center_x - scaled_w // 2
            new_y = center_y - scaled_h // 2
            
            new_x = max(0, min(new_x, video_width - scaled_w))
            new_y = max(0, min(new_y, video_height - scaled_h))
            
            new_w, new_h = scaled_w, scaled_h
            was_adjusted = True
        
        # ç¡®ä¿æœ€å°å°ºå¯¸
        min_w = 64
        min_h = int(min_w / target_aspect)
        
        if new_w < min_w or new_h < min_h:
            new_w = max(min_w, new_w)
            new_h = int(new_w / target_aspect)
            
            if new_x + new_w > video_width:
                new_x = video_width - new_w
            if new_y + new_h > video_height:
                new_y = video_height - new_h
            
            was_adjusted = True
        
        final_roi = (new_x, new_y, new_w, new_h)
        
        if was_adjusted or final_roi != original_roi:
            was_adjusted = True
            logging.info(f"ROIæ™ºèƒ½è°ƒæ•´ä¸º16:9: {original_roi} -> {final_roi}")
            
            original_area = original_roi[2] * original_roi[3]
            final_area = new_w * new_h
            area_change_percent = (final_area - original_area) / original_area * 100 if original_area > 0 else 0
            final_aspect = new_w / new_h if new_h > 0 else target_aspect
            
            print(f"ğŸ¯ ROIæ™ºèƒ½è°ƒæ•´ä¸º16:9:")
            print(f"   ç”¨æˆ·é€‰æ‹©ROI: {original_roi}")
            print(f"   æœ€ç»ˆ16:9 ROI: {final_roi}")
            print(f"   å®½é«˜æ¯”: {final_aspect:.3f} (ç›®æ ‡: {target_aspect:.3f})")
            print(f"   é¢ç§¯å˜åŒ–: {area_change_percent:+.1f}%")
            print(f"   å è§†é¢‘æ¯”ä¾‹: {new_w/video_width*100:.1f}% Ã— {new_h/video_height*100:.1f}%")
        
        return final_roi, was_adjusted

    def gui_select_roi_16_9(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """16:9å›ºå®šæ¯”ä¾‹æ‹–æ‹½é€‰æ¡†æ¨¡å¼ - ç±»ä¼¼å‰ªæ˜ çš„äº¤äº’ä½“éªŒ"""
        if not self.gui_available:
            return None
            
        try:
            video_height, video_width = frame.shape[:2]
            target_aspect = 16.0 / 9.0
            
            # è°ƒæ•´æ˜¾ç¤ºå°ºå¯¸
            display_height = 800
            scale_factor = display_height / video_height if video_height > 0 else 1
            display_width = int(video_width * scale_factor)
            
            # è®¡ç®—é»˜è®¤16:9åŒºåŸŸ (å±…ä¸­ï¼Œæœ€å¤§åŒ–)
            frame_aspect = video_width / video_height
            if frame_aspect > target_aspect:
                # è§†é¢‘æ¯”16:9æ›´å®½ï¼Œä»¥é«˜åº¦ä¸ºå‡†
                roi_h = int(video_height * 0.8)  # 80%é«˜åº¦
                roi_w = int(roi_h * target_aspect)
                roi_x = (video_width - roi_w) // 2
                roi_y = (video_height - roi_h) // 2
            else:
                # è§†é¢‘æ¯”16:9æ›´é«˜ï¼Œä»¥å®½åº¦ä¸ºå‡†  
                roi_w = int(video_width * 0.8)  # 80%å®½åº¦
                roi_h = int(roi_w / target_aspect)
                roi_x = (video_width - roi_w) // 2
                roi_y = (video_height - roi_h) // 2
                
            # ç¡®ä¿åœ¨è¾¹ç•Œå†…
            roi_x = max(0, min(roi_x, video_width - roi_w))
            roi_y = max(0, min(roi_y, video_height - roi_h))
            
            # æ‹–æ‹½çŠ¶æ€å˜é‡
            dragging = False
            drag_start = None
            drag_offset = (0, 0)
            resizing = False
            resize_corner = None
            
            def mouse_callback(event, x, y, flags, param):
                nonlocal dragging, drag_start, drag_offset, resizing, resize_corner, roi_x, roi_y, roi_w, roi_h
                
                # è½¬æ¢æ˜¾ç¤ºåæ ‡åˆ°åŸå§‹è§†é¢‘åæ ‡
                orig_x = int(x / scale_factor)
                orig_y = int(y / scale_factor)
                
                # è®¡ç®—æ˜¾ç¤ºåæ ‡ä¸‹çš„ROI
                display_roi_x = int(roi_x * scale_factor)
                display_roi_y = int(roi_y * scale_factor)
                display_roi_w = int(roi_w * scale_factor)
                display_roi_h = int(roi_h * scale_factor)
                
                if event == cv2.EVENT_LBUTTONDOWN:
                    # æ£€æŸ¥æ˜¯å¦ç‚¹å‡»åœ¨è§’è½ï¼ˆè°ƒæ•´å¤§å°ï¼‰ - ä¼˜å…ˆæ£€æŸ¥
                    corner_detect_size = 15  # è§’è½æ£€æµ‹èŒƒå›´
                    corners = {
                        'tl': (display_roi_x, display_roi_y),  # å·¦ä¸Š
                        'tr': (display_roi_x + display_roi_w, display_roi_y),  # å³ä¸Š
                        'bl': (display_roi_x, display_roi_y + display_roi_h),  # å·¦ä¸‹
                        'br': (display_roi_x + display_roi_w, display_roi_y + display_roi_h)  # å³ä¸‹
                    }
                    
                    for corner_name, (cx, cy) in corners.items():
                        if abs(x - cx) <= corner_detect_size and abs(y - cy) <= corner_detect_size:
                            resizing = True
                            resize_corner = corner_name
                            drag_start = (orig_x, orig_y)  # ä½¿ç”¨åŸå§‹åæ ‡
                            return
                    
                    # æ£€æŸ¥æ˜¯å¦ç‚¹å‡»åœ¨ROIæ¡†å†…ï¼ˆç§»åŠ¨ï¼‰
                    if (display_roi_x <= x <= display_roi_x + display_roi_w and 
                        display_roi_y <= y <= display_roi_y + display_roi_h):
                        # æ‹–æ‹½ç§»åŠ¨
                        dragging = True
                        drag_start = (orig_x, orig_y)  # ä½¿ç”¨åŸå§‹åæ ‡
                        drag_offset = (orig_x - roi_x, orig_y - roi_y)
                
                elif event == cv2.EVENT_MOUSEMOVE:
                    if dragging and drag_start:
                        # ç§»åŠ¨ROIæ¡†
                        new_x = orig_x - drag_offset[0]
                        new_y = orig_y - drag_offset[1]
                        
                        # è¾¹ç•Œæ£€æŸ¥
                        new_x = max(0, min(new_x, video_width - roi_w))
                        new_y = max(0, min(new_y, video_height - roi_h))
                        
                        roi_x, roi_y = new_x, new_y
                        
                    elif resizing and drag_start and resize_corner:
                        # è°ƒæ•´ROIæ¡†å¤§å°ï¼ˆä¿æŒ16:9æ¯”ä¾‹ï¼‰
                        dx = orig_x - drag_start[0]
                        dy = orig_y - drag_start[1]
                        
                        # ä¿å­˜åŸå§‹å€¼
                        old_x, old_y, old_w, old_h = roi_x, roi_y, roi_w, roi_h
                        
                        # æ ¹æ®æ‹–æ‹½çš„è§’è½è°ƒæ•´å¤§å°
                        if resize_corner == 'br':  # å³ä¸‹è§’
                            new_w = roi_w + dx
                            new_h = roi_h + dy
                        elif resize_corner == 'tr':  # å³ä¸Šè§’
                            new_w = roi_w + dx
                            new_h = roi_h - dy
                            new_y = roi_y + dy
                        elif resize_corner == 'bl':  # å·¦ä¸‹è§’
                            new_w = roi_w - dx
                            new_h = roi_h + dy
                            new_x = roi_x + dx
                        elif resize_corner == 'tl':  # å·¦ä¸Šè§’
                            new_w = roi_w - dx
                            new_h = roi_h - dy
                            new_x = roi_x + dx
                            new_y = roi_y + dy
                        else:
                            new_w, new_h = roi_w, roi_h
                            new_x, new_y = roi_x, roi_y
                        
                        # ä¿æŒ16:9æ¯”ä¾‹ - ä»¥å˜åŒ–é‡å¤§çš„ä¸ºå‡†
                        if abs(dx) > abs(dy):
                            # ä»¥å®½åº¦ä¸ºå‡†è°ƒæ•´é«˜åº¦
                            new_h = int(new_w / target_aspect)
                        else:
                            # ä»¥é«˜åº¦ä¸ºå‡†è°ƒæ•´å®½åº¦
                            new_w = int(new_h * target_aspect)
                        
                        # ç¡®ä¿æœ€å°å°ºå¯¸
                        min_w, min_h = 64, int(64 / target_aspect)
                        if new_w < min_w:
                            new_w = min_w
                            new_h = int(new_w / target_aspect)
                        if new_h < min_h:
                            new_h = min_h
                            new_w = int(new_h * target_aspect)
                        
                        # æ ¹æ®è§’è½é‡æ–°è®¡ç®—ä½ç½®
                        if resize_corner == 'br':  # å³ä¸‹è§’ - å·¦ä¸Šè§’ä¸å˜
                            new_x, new_y = roi_x, roi_y
                        elif resize_corner == 'tr':  # å³ä¸Šè§’ - å·¦ä¸‹è§’ä¸å˜
                            new_x = roi_x
                            new_y = roi_y + roi_h - new_h
                        elif resize_corner == 'bl':  # å·¦ä¸‹è§’ - å³ä¸Šè§’ä¸å˜
                            new_x = roi_x + roi_w - new_w
                            new_y = roi_y
                        elif resize_corner == 'tl':  # å·¦ä¸Šè§’ - å³ä¸‹è§’ä¸å˜
                            new_x = roi_x + roi_w - new_w
                            new_y = roi_y + roi_h - new_h
                        
                        # è¾¹ç•Œæ£€æŸ¥
                        if (new_x >= 0 and new_y >= 0 and 
                            new_x + new_w <= video_width and new_y + new_h <= video_height):
                            roi_x, roi_y, roi_w, roi_h = new_x, new_y, new_w, new_h
                            drag_start = (orig_x, orig_y)  # æ›´æ–°æ‹–æ‹½èµ·ç‚¹
                
                elif event == cv2.EVENT_LBUTTONUP:
                    dragging = False
                    resizing = False
                    drag_start = None
                    resize_corner = None
                
                elif event == cv2.EVENT_RBUTTONDOWN:
                    # å³é”®é‡ç½®åˆ°é»˜è®¤ä½ç½®
                    roi_x = (video_width - roi_w) // 2
                    roi_y = (video_height - roi_h) // 2
            
            # åˆ›å»ºçª—å£å¹¶è®¾ç½®é¼ æ ‡å›è°ƒ
            cv2.namedWindow("16:9 æ‹–æ‹½é€‰æ¡†", cv2.WINDOW_NORMAL)
            cv2.resizeWindow("16:9 æ‹–æ‹½é€‰æ¡†", display_width, display_height)
            cv2.setWindowProperty("16:9 æ‹–æ‹½é€‰æ¡†", cv2.WND_PROP_TOPMOST, 1)
            cv2.setMouseCallback("16:9 æ‹–æ‹½é€‰æ¡†", mouse_callback)
            
            print("\nğŸ¯ 16:9 æ‹–æ‹½é€‰æ¡†çª—å£å·²æ‰“å¼€ï¼")
            print(f"ğŸ“º åŸå§‹è§†é¢‘: {video_width}x{video_height} -> æ˜¾ç¤ºçª—å£: {display_width}x{display_height}")
            print(f"ğŸ“ åˆå§‹16:9åŒºåŸŸ: ({roi_x}, {roi_y}, {roi_w}, {roi_h})")
            print("ğŸ–±ï¸  æ“ä½œæç¤ºï¼š")
            print("   ğŸ–±ï¸  å·¦é”®æ‹–æ‹½æ¡†å†… - ç§»åŠ¨é€‰æ¡†")
            print("   ğŸ”„ æ‹–æ‹½å››ä¸ªè§’ç‚¹ - è°ƒæ•´å¤§å°ï¼ˆä¿æŒ16:9ï¼‰")
            print("   ğŸ–±ï¸  å³é”® - é‡ç½®åˆ°ä¸­å¿ƒä½ç½®")
            print("   âŒ¨ï¸  ENTER - ç¡®è®¤é€‰æ‹©")
            print("   âŒ¨ï¸  ESC - å–æ¶ˆé€‰æ‹©")
            
            while True:
                # åˆ›å»ºæ˜¾ç¤ºå¸§
                display_frame = cv2.resize(frame, (display_width, display_height))
                
                # è®¡ç®—æ˜¾ç¤ºåæ ‡ä¸‹çš„ROI
                display_roi_x = int(roi_x * scale_factor)
                display_roi_y = int(roi_y * scale_factor)
                display_roi_w = int(roi_w * scale_factor)
                display_roi_h = int(roi_h * scale_factor)
                
                # ç»˜åˆ¶åŠé€æ˜è¦†ç›–å±‚
                overlay = display_frame.copy()
                cv2.rectangle(overlay, 
                             (display_roi_x, display_roi_y),
                             (display_roi_x + display_roi_w, display_roi_y + display_roi_h),
                             (0, 255, 0), -1)
                display_frame = cv2.addWeighted(display_frame, 0.7, overlay, 0.3, 0)
                
                # ç»˜åˆ¶è¾¹æ¡† - å‡å°‘åšåº¦
                cv2.rectangle(display_frame,
                             (display_roi_x, display_roi_y),
                             (display_roi_x + display_roi_w, display_roi_y + display_roi_h),
                             (0, 255, 0), 1)  # åšåº¦ä»3æ”¹ä¸º1
                
                # ç»˜åˆ¶è§’è½è°ƒæ•´ç‚¹ - æ›´å¤§æ›´æ˜æ˜¾
                corner_visual_size = 6  # æ˜¾ç¤ºå¤§å°
                corners = [
                    (display_roi_x, display_roi_y),  # å·¦ä¸Š
                    (display_roi_x + display_roi_w, display_roi_y),  # å³ä¸Š
                    (display_roi_x, display_roi_y + display_roi_h),  # å·¦ä¸‹
                    (display_roi_x + display_roi_w, display_roi_y + display_roi_h)  # å³ä¸‹
                ]
                for corner in corners:
                    # ç»˜åˆ¶ç™½è‰²å¤–åœˆ
                    cv2.circle(display_frame, corner, corner_visual_size + 2, (255, 255, 255), -1)
                    # ç»˜åˆ¶ç»¿è‰²å†…åœ†
                    cv2.circle(display_frame, corner, corner_visual_size, (0, 255, 0), -1)
                    # ç»˜åˆ¶é»‘è‰²è¾¹æ¡†
                    cv2.circle(display_frame, corner, corner_visual_size, (0, 0, 0), 1)
                
                # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
                cv2.putText(display_frame, "16:9 æ‹–æ‹½é€‰æ¡† (ä¼˜åŒ–ç‰ˆ)", 
                           (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(display_frame, "æ‹–æ‹½æ¡†å†…ç§»åŠ¨ / æ‹–æ‹½è§’ç‚¹ç¼©æ”¾ / å³é”®é‡ç½®", 
                           (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # æ˜¾ç¤ºROIä¿¡æ¯
                roi_info = f"ROI: {roi_x},{roi_y},{roi_w}x{roi_h} (16:9)"
                cv2.putText(display_frame, roi_info, 
                           (20, display_height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                cv2.imshow("16:9 æ‹–æ‹½é€‰æ¡†", display_frame)
                key = cv2.waitKey(30) & 0xFF
                
                if key == 27:  # ESC
                    cv2.destroyAllWindows()
                    print("âŒ ç”¨æˆ·å–æ¶ˆé€‰æ‹©")
                    return None
                    
                elif key in [13, 10]:  # ENTER
                    cv2.destroyAllWindows()
                    final_roi = (roi_x, roi_y, roi_w, roi_h)
                    print(f"âœ… ç¡®è®¤ä½¿ç”¨16:9åŒºåŸŸ: {final_roi}")
                    return final_roi
            
        except Exception as e:
            logging.warning(f"16:9æ‹–æ‹½é€‰æ¡†å¤±è´¥: {e}")
            return None
    
    def gui_select_roi_manual(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """æ‰‹åŠ¨ROIé€‰æ‹©æ¨¡å¼ï¼ˆåŸç‰ˆåŠŸèƒ½ï¼‰"""
        if not self.gui_available:
            return None
            
        try:
            video_height, video_width = frame.shape[:2]
            
            # è°ƒæ•´æ˜¾ç¤ºå°ºå¯¸
            display_height = 800
            scale_factor = display_height / video_height if video_height > 0 else 1
            display_width = int(video_width * scale_factor)
            display_frame = cv2.resize(frame, (display_width, display_height))
            
            cv2.putText(display_frame, "æ‰‹åŠ¨é€‰æ‹©ROIåŒºåŸŸï¼Œç„¶åæŒ‰'ç©ºæ ¼'æˆ–'å›è½¦'ç¡®è®¤", 
                       (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            cv2.putText(display_frame, "é€‰æ‹©åå°†è‡ªåŠ¨è°ƒæ•´ä¸º16:9æ¯”ä¾‹", 
                       (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
            
            # åˆ›å»ºçª—å£
            cv2.namedWindow("æ‰‹åŠ¨ROIé€‰æ‹©", cv2.WINDOW_NORMAL)
            cv2.resizeWindow("æ‰‹åŠ¨ROIé€‰æ‹©", display_width, display_height)
            cv2.setWindowProperty("æ‰‹åŠ¨ROIé€‰æ‹©", cv2.WND_PROP_TOPMOST, 1)
            
            print("\nğŸ–±ï¸  æ‰‹åŠ¨ROIé€‰æ‹©æ¨¡å¼:")
            print("   1. ç”¨é¼ æ ‡æ‹–æ‹½é€‰æ‹©è£å‰ªåŒºåŸŸ")
            print("   2. æŒ‰ SPACE/ENTER ç¡®è®¤é€‰æ‹©") 
            print("   3. æŒ‰ ESC å–æ¶ˆé€‰æ‹©")
            print("âœ¨ é€‰æ‹©å®Œæˆåä¼šè‡ªåŠ¨è°ƒæ•´ä¸º16:9æ¯”ä¾‹ï¼")
            
            # OpenCV selectROI
            current_version = cv2.__version__
            if current_version.startswith('4.12'):
                print("âš ï¸ æ£€æµ‹åˆ°OpenCV 4.12.0ç‰ˆæœ¬ï¼Œä½¿ç”¨å¢å¼ºROIé€‰æ‹©æ¨¡å¼")
                for attempt in range(3):
                    try:
                        cv2.imshow("æ‰‹åŠ¨ROIé€‰æ‹©", display_frame)
                        cv2.waitKey(100)
                        r = cv2.selectROI("æ‰‹åŠ¨ROIé€‰æ‹©", display_frame, fromCenter=False, showCrosshair=True)
                        break
                    except Exception as e:
                        print(f"ROIé€‰æ‹©å°è¯• {attempt + 1}/3 å¤±è´¥: {e}")
                        if attempt == 2:
                            raise e
                        cv2.waitKey(500)
            else:
                r = cv2.selectROI("æ‰‹åŠ¨ROIé€‰æ‹©", display_frame, fromCenter=False, showCrosshair=True)
            
            cv2.destroyAllWindows()
            
            if r[2] == 0 or r[3] == 0:
                print("âŒ æœªé€‰æ‹©æœ‰æ•ˆåŒºåŸŸæˆ–å·²å–æ¶ˆé€‰æ‹©")
                return None
            
            # è½¬æ¢ä¸ºåŸå§‹åˆ†è¾¨ç‡
            r_original = (
                int(r[0] / scale_factor),
                int(r[1] / scale_factor),
                int(r[2] / scale_factor),
                int(r[3] / scale_factor)
            )
            
            print(f"âœ… ROIæ‰‹åŠ¨é€‰æ‹©æˆåŠŸ: {r_original}")
            return r_original
            
        except Exception as e:
            logging.warning(f"æ‰‹åŠ¨ROIé€‰æ‹©å¤±è´¥: {e}")
            return None

    def gui_select_roi(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """å›¾å½¢åŒ–ROIé€‰æ‹© - ä¼˜å…ˆä½¿ç”¨16:9å¼ºåˆ¶æ¨¡å¼"""
        if self.roi_16_9_mode:
            return self.gui_select_roi_16_9(frame)
        else:
            return self.gui_select_roi_manual(frame)

    def fallback_roi_input(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """å‘½ä»¤è¡Œå›é€€ROIè¾“å…¥"""
        try:
            video_height, video_width = frame.shape[:2]
            
            # ä¿å­˜é¢„è§ˆå›¾
            preview_path = "roi_preview.jpg"
            display_height = 720
            scale_factor = display_height / video_height
            display_width = int(video_width * scale_factor)
            display_frame = cv2.resize(frame, (display_width, display_height))
            cv2.imwrite(preview_path, display_frame)
            
            print(f"\nğŸ–¼ï¸  å·²ç”Ÿæˆé¢„è§ˆå›¾: {preview_path}")
            print(f"ğŸ“º é¢„è§ˆåˆ†è¾¨ç‡: {display_width}x{display_height}")
            print(f"ğŸ“º åŸå§‹åˆ†è¾¨ç‡: {video_width}x{video_height}")
            
            # å°è¯•è‡ªåŠ¨æ‰“å¼€é¢„è§ˆå›¾
            try:
                if platform.system() == "Windows":
                    os.startfile(preview_path)
                elif platform.system() == "Darwin":
                    subprocess.run(['open', preview_path], check=False, timeout=10)
                else:
                    subprocess.run(['xdg-open', preview_path], check=False, timeout=10)
            except Exception:
                pass
            
            print("\nè¯·åŸºäºé¢„è§ˆå›¾è¾“å…¥ROIåæ ‡:")
            print("æ ¼å¼: x y width height (ä»¥ç©ºæ ¼åˆ†éš”)")
            print("ä¾‹å¦‚: 100 50 800 600")
            
            while True:
                try:
                    user_input = input("è¯·è¾“å…¥ROIåæ ‡: ").strip()
                    coords = list(map(int, user_input.split()))
                    
                    if len(coords) != 4:
                        print("âŒ è¯·è¾“å…¥4ä¸ªæ•°å€¼: x y width height")
                        continue
                    
                    x_disp, y_disp, w_disp, h_disp = coords
                    
                    # è½¬æ¢ä¸ºåŸå§‹åˆ†è¾¨ç‡
                    x = int(x_disp / scale_factor)
                    y = int(y_disp / scale_factor)
                    w = int(w_disp / scale_factor)
                    h = int(h_disp / scale_factor)
                    
                    # è¾¹ç•Œæ£€æŸ¥
                    x = max(0, min(x, video_width - 1))
                    y = max(0, min(y, video_height - 1))
                    w = max(1, min(w, video_width - x))
                    h = max(1, min(h, video_height - y))
                    
                    print(f"âœ… ROIè¾“å…¥æˆåŠŸ: ({x}, {y}, {w}, {h})")
                    
                    # æ¸…ç†é¢„è§ˆå›¾
                    try:
                        os.remove(preview_path)
                    except Exception:
                        pass
                    
                    return (x, y, w, h)
                    
                except ValueError:
                    print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥4ä¸ªæ•´æ•°")
                except KeyboardInterrupt:
                    print("\nç”¨æˆ·å–æ¶ˆè¾“å…¥")
                    return None
                except Exception as e:
                    print(f"âŒ è¾“å…¥é”™è¯¯: {e}")
        
        except Exception as e:
            logging.error(f"å‘½ä»¤è¡ŒROIè¾“å…¥å¤±è´¥: {e}")
            return None

    def select_roi_for_video(self, video_path: str) -> Optional[Tuple[int, int, int, int, int, int]]:
        """ä¸ºè§†é¢‘é€‰æ‹©ROIåŒºåŸŸï¼Œè¿”å›(x, y, w, h, base_width, base_height)"""
        frame = self.extract_preview_frame(video_path)
        if frame is None:
            logging.error(f"æ— æ³•æå–é¢„è§ˆå¸§: {video_path}")
            return None
        
        video_height, video_width = frame.shape[:2]
        logging.info(f"åŸºå‡†è§†é¢‘å°ºå¯¸: {video_width}x{video_height}")
        
        # å°è¯•GUIé€‰æ‹©
        roi = self.gui_select_roi(frame)
        if roi is None and self.gui_available:
            print("ğŸ”„ GUIé€‰æ‹©å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å‘½ä»¤è¡Œè¾“å…¥æ¨¡å¼")
        
        # å¦‚æœGUIå¤±è´¥ï¼Œä½¿ç”¨å‘½ä»¤è¡Œè¾“å…¥
        if roi is None:
            roi = self.fallback_roi_input(frame)
        
        if roi is None:
            return None
        
        # è°ƒæ•´ä¸º16:9æ¯”ä¾‹
        adjusted_roi, was_adjusted = self.adjust_roi_to_16_9(roi, video_width, video_height)
        
        if was_adjusted:
            print(f"ğŸ”„ ROIå·²è‡ªåŠ¨è°ƒæ•´ä¸º16:9æ¯”ä¾‹")
        
        # è¿”å›ROI + åŸºå‡†å°ºå¯¸
        return adjusted_roi + (video_width, video_height)

class ProgressManager:
    """ç»Ÿä¸€è¿›åº¦ç®¡ç†å™¨ - å¢å¼ºç‰ˆæœ¬ + å¤šç”µè„‘åä½œ"""
    
    def __init__(self, computer_name: str, video_record_manager: Optional[VideoRecordManager] = None):
        self.computer_name = computer_name
        self.video_record_manager = video_record_manager
        self.progress_folder = os.path.join(PROGRESS_FOLDER, computer_name)
        self.progress_file = os.path.join(self.progress_folder, "unified_progress.json")
        self.temp_file = os.path.join(self.progress_folder, "unified_progress.tmp")
        self.crash_recovery_file = os.path.join(self.progress_folder, "crash_recovery.json")
        
        os.makedirs(self.progress_folder, exist_ok=True)
        self.progress_data = self.load_progress()

    def calculate_file_sha256(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶SHA256å“ˆå¸Œå€¼"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜å ç”¨è¿‡é«˜
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            logging.warning(f"è®¡ç®—SHA256å¤±è´¥ {file_path}: {e}")
            return ""
    
    def get_video_signature(self, video_path: str) -> str:
        """è·å–è§†é¢‘å”¯ä¸€ç­¾åï¼ˆæ–‡ä»¶å+å¤§å°+ä¿®æ”¹æ—¶é—´çš„å“ˆå¸Œï¼‰"""
        try:
            stat = os.stat(video_path)
            signature_data = f"{os.path.basename(video_path)}_{stat.st_size}_{stat.st_mtime}"
            return hashlib.md5(signature_data.encode()).hexdigest()[:16]
        except Exception as e:
            logging.warning(f"è·å–è§†é¢‘ç­¾åå¤±è´¥ {video_path}: {e}")
            return hashlib.md5(os.path.basename(video_path).encode()).hexdigest()[:16]

    def check_crash_recovery(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨å´©æºƒæ¢å¤æ•°æ®"""
        try:
            if os.path.exists(self.crash_recovery_file):
                with open(self.crash_recovery_file, 'r', encoding='utf-8') as f:
                    recovery_data = json.load(f)
                    return recovery_data
        except Exception as e:
            logging.warning(f"è¯»å–å´©æºƒæ¢å¤æ–‡ä»¶å¤±è´¥: {e}")
        return {}

    def save_crash_recovery(self, current_video: str, roi_settings: Optional[Tuple[int, int, int, int]]):
        """ä¿å­˜å´©æºƒæ¢å¤æ•°æ®"""
        try:
            recovery_data = {
                'last_session_time': datetime.now().isoformat(),
                'current_video': current_video,
                'roi_settings': roi_settings,
                'config': {
                    'enable_head_tail_cut': ENABLE_HEAD_TAIL_CUT,
                    'enable_cropping': ENABLE_CROPPING,
                    'head_cut_time': HEAD_CUT_TIME,
                    'tail_cut_time': TAIL_CUT_TIME,
                    'target_resolution': TARGET_RESOLUTION
                }
            }
            
            with open(self.crash_recovery_file, 'w', encoding='utf-8') as f:
                json.dump(recovery_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜å´©æºƒæ¢å¤æ•°æ®å¤±è´¥: {e}")

    def clear_crash_recovery(self):
        """æ¸…é™¤å´©æºƒæ¢å¤æ•°æ®"""
        try:
            if os.path.exists(self.crash_recovery_file):
                os.remove(self.crash_recovery_file)
        except Exception as e:
            logging.warning(f"æ¸…é™¤å´©æºƒæ¢å¤æ–‡ä»¶å¤±è´¥: {e}")
            
    def detect_partial_outputs(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹è¾“å‡ºç›®å½•ä¸­çš„éƒ¨åˆ†å¤„ç†æ–‡ä»¶"""
        partial_files = []
        try:
            if not os.path.exists(OUTPUT_DIR):
                return partial_files
                
            for video_name in os.listdir(OUTPUT_DIR):
                if video_name.lower().endswith('.mp4'):
                    file_path = os.path.join(OUTPUT_DIR, video_name)
                    file_stat = os.stat(file_path)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦å¼‚å¸¸å°ï¼ˆå¯èƒ½æ˜¯éƒ¨åˆ†æ–‡ä»¶ï¼‰
                    if file_stat.st_size < 1024 * 1024:  # å°äº1MB
                        partial_files.append({
                            'path': file_path,
                            'name': video_name,
                            'size': file_stat.st_size,
                            'modified': file_stat.st_mtime
                        })
                        continue
                    
                    # å°è¯•å¿«é€Ÿæ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
                    try:
                        probe_cmd = [FFPROBE_PATH, '-v', 'error', '-select_streams', 'v:0', 
                                   '-show_entries', 'format=duration', '-of', 'csv=p=0', file_path]
                        result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=5)
                        
                        if result.returncode != 0 or not result.stdout.strip():
                            partial_files.append({
                                'path': file_path,
                                'name': video_name,
                                'size': file_stat.st_size,
                                'modified': file_stat.st_mtime,
                                'error': 'probe_failed'
                            })
                    except:
                        # å¦‚æœæ£€æŸ¥è¶…æ—¶ï¼Œå‡è®¾æ–‡ä»¶å¯èƒ½æœ‰é—®é¢˜
                        partial_files.append({
                            'path': file_path,
                            'name': video_name,  
                            'size': file_stat.st_size,
                            'modified': file_stat.st_mtime,
                            'error': 'probe_timeout'
                        })
                        
        except Exception as e:
            logging.warning(f"æ£€æµ‹éƒ¨åˆ†æ–‡ä»¶å¤±è´¥: {e}")
            
        return partial_files

    def load_progress(self) -> Dict[str, Any]:
        """åŠ è½½è¿›åº¦æ•°æ®"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    logging.info(f"åŠ è½½è¿›åº¦è®°å½•: {len(data.get('completed', []))} ä¸ªå·²å®Œæˆ")
                    return data
        except Exception as e:
            logging.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
        
        return {
            'completed': [], 'processing': [], 'failed': [],
            'start_time': None, 'roi_settings': None,
            'video_signatures': {},  # SHA256ç­¾åæ˜ å°„
            'config': {'enable_head_tail_cut': ENABLE_HEAD_TAIL_CUT, 'enable_cropping': ENABLE_CROPPING}
        }

    def save_progress(self):
        """ä¿å­˜è¿›åº¦æ•°æ®"""
        try:
            with open(self.temp_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress_data, f, ensure_ascii=False, indent=2)
            
            if os.path.exists(self.progress_file):
                os.remove(self.progress_file)
            os.rename(self.temp_file, self.progress_file)
        except Exception as e:
            logging.error(f"ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

    def is_completed(self, video_path: str, quick_check: bool = False) -> bool:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦å·²å®Œæˆï¼ˆä¼˜å…ˆæ•°æ®åº“ï¼Œåå¤‡æœ¬åœ°è®°å½•ï¼‰"""
        video_name = os.path.basename(video_path)
        
        # å¿«é€Ÿæ£€æŸ¥æ¨¡å¼ï¼šä»…æ£€æŸ¥æœ¬åœ°è®°å½•å’Œè¾“å‡ºæ–‡ä»¶
        if quick_check:
            # 1. æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            base_name = os.path.splitext(video_name)[0]
            output_path = os.path.join(OUTPUT_DIR, f"{base_name}.mp4")
            if os.path.exists(output_path) and os.path.getsize(output_path) > 1024 * 1024:  # å¤§äº1MB
                logger.debug(f"âœ… è¾“å‡ºæ–‡ä»¶å­˜åœ¨: {video_name}")
                return True
            
            # 2. æ£€æŸ¥æœ¬åœ°ç­¾åè®°å½•ï¼ˆä½¿ç”¨å¿«é€Ÿç­¾åï¼‰
            video_signature = self.get_video_signature(video_path)
            signatures = self.progress_data.get('video_signatures', {})
            if video_signature in signatures:
                logger.debug(f"âœ… æœ¬åœ°ç­¾åå‘½ä¸­: {video_name}")
                return True
            
            # 3. æ£€æŸ¥æ—§ç‰ˆæœ¬è®°å½•ï¼ˆåŸºäºæ–‡ä»¶åï¼‰
            for record in self.progress_data.get('completed', []):
                if isinstance(record, dict) and record.get('name') == video_name:
                    logger.debug(f"âœ… æ—§è®°å½•å‘½ä¸­: {video_name}")
                    return True
                elif record == video_name:
                    logger.debug(f"âœ… æ—§è®°å½•å‘½ä¸­: {video_name}")
                    return True
            
            return False
        
        # å®Œæ•´æ£€æŸ¥æ¨¡å¼ï¼ˆåŸé€»è¾‘ï¼‰
        # ğŸ”— ä¼˜å…ˆæ£€æŸ¥æ•°æ®åº“è®°å½•
        if self.video_record_manager and self.video_record_manager.db_manager.is_available():
            is_processed, db_record = self.video_record_manager.is_video_processed(video_path)
            if is_processed:
                logger.info(f"ğŸ” æ•°æ®åº“å‘ç°å·²å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
                return True
        
        # åå¤‡ï¼šæ£€æŸ¥æœ¬åœ°è®°å½•
        video_signature = self.get_video_signature(video_path)
        
        # æ£€æŸ¥SHA256ç­¾åæ˜ å°„
        signatures = self.progress_data.get('video_signatures', {})
        if video_signature in signatures:
            return True
            
        # å…¼å®¹æ—§ç‰ˆæœ¬è®°å½•ï¼ˆåŸºäºæ–‡ä»¶åï¼‰
        for record in self.progress_data.get('completed', []):
            if isinstance(record, dict):
                if record.get('name') == video_name:
                    # è¿ç§»åˆ°æ–°çš„ç­¾åç³»ç»Ÿ
                    signatures[video_signature] = {
                        'name': video_name,
                        'path': video_path,
                        'migrated_from_old': True
                    }
                    self.progress_data['video_signatures'] = signatures
                    self.save_progress()
                    return True
            else:
                if record == video_name:
                    # è¿ç§»åˆ°æ–°ç³»ç»Ÿ
                    signatures[video_signature] = {
                        'name': video_name,
                        'path': video_path,
                        'migrated_from_old': True
                    }
                    self.progress_data['video_signatures'] = signatures
                    self.save_progress()
                    return True
        return False

    def mark_completed(self, video_path: str, output_path: str = None, processing_time: float = 0.0):
        """æ ‡è®°è§†é¢‘ä¸ºå·²å®Œæˆï¼ˆæ•°æ®åº“+æœ¬åœ°è®°å½•ï¼‰"""
        video_name = os.path.basename(video_path)
        video_signature = self.get_video_signature(video_path)
        
        # ğŸ”— ä¼˜å…ˆæ›´æ–°æ•°æ®åº“è®°å½•
        if self.video_record_manager and self.video_record_manager.db_manager.is_available():
            self.video_record_manager.complete_processing(video_path, output_path, processing_time)
        
        # ç»§ç»­ç»´æŠ¤æœ¬åœ°è®°å½•ï¼ˆå…¼å®¹æ€§ï¼‰
        # ç§»é™¤æ—§è®°å½•ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
        self.progress_data['completed'] = [
            record for record in self.progress_data['completed']
            if not (isinstance(record, dict) and record.get('name') == video_name) and record != video_name
        ]
        
        # æ·»åŠ æ–°è®°å½•åˆ°ä¼ ç»Ÿåˆ—è¡¨ï¼ˆå…¼å®¹æ€§ï¼‰
        completed_record = {
            'name': video_name,
            'path': video_path,
            'output_path': output_path,
            'processing_time': processing_time,
            'completed_time': datetime.now().isoformat(),
            'signature': video_signature,
            'config': {
                'head_tail_cut': ENABLE_HEAD_TAIL_CUT,
                'cropping': ENABLE_CROPPING,
                'head_cut_time': HEAD_CUT_TIME if ENABLE_HEAD_TAIL_CUT else 0,
                'tail_cut_time': TAIL_CUT_TIME if ENABLE_HEAD_TAIL_CUT else 0,
                'target_resolution': TARGET_RESOLUTION if ENABLE_CROPPING else None
            }
        }
        
        self.progress_data['completed'].append(completed_record)
        
        # æ·»åŠ åˆ°æ–°çš„ç­¾åæ˜ å°„ç³»ç»Ÿ
        if 'video_signatures' not in self.progress_data:
            self.progress_data['video_signatures'] = {}
        
        self.progress_data['video_signatures'][video_signature] = {
            'name': video_name,
            'path': video_path,
            'output_path': output_path,
            'processing_time': processing_time,
            'completed_time': datetime.now().isoformat(),
            'config': completed_record['config'].copy()
        }
        
        # ä»å¤„ç†ä¸­ç§»é™¤
        if video_name in self.progress_data['processing']:
            self.progress_data['processing'].remove(video_name)
        
        # ä»å¤±è´¥åˆ—è¡¨ç§»é™¤
        self.progress_data['failed'] = [
            record for record in self.progress_data['failed']
            if not (isinstance(record, dict) and record.get('name') == video_name)
        ]
        
        self.save_progress()

    def mark_processing(self, video_path: str, video_info: Dict = None):
        """æ ‡è®°è§†é¢‘ä¸ºå¤„ç†ä¸­ï¼ˆæ•°æ®åº“+æœ¬åœ°è®°å½•ï¼‰"""
        video_name = os.path.basename(video_path)
        
        # ğŸ”— ä¼˜å…ˆæ›´æ–°æ•°æ®åº“è®°å½•
        if self.video_record_manager and self.video_record_manager.db_manager.is_available():
            self.video_record_manager.start_processing(video_path, video_info=video_info)
        
        # ç»§ç»­ç»´æŠ¤æœ¬åœ°è®°å½•ï¼ˆå…¼å®¹æ€§ï¼‰
        if video_name not in self.progress_data['processing']:
            self.progress_data['processing'].append(video_name)
        self.save_progress()

    def mark_failed(self, video_path: str, error_msg: str = ""):
        """æ ‡è®°è§†é¢‘ä¸ºå¤±è´¥ï¼ˆæ•°æ®åº“+æœ¬åœ°è®°å½•ï¼‰"""
        video_name = os.path.basename(video_path)
        
        # æ¸…ç†é”™è¯¯æ¶ˆæ¯
        clean_error = self.clean_error_message(error_msg)
        
        # ğŸ”— ä¼˜å…ˆæ›´æ–°æ•°æ®åº“è®°å½•
        if self.video_record_manager and self.video_record_manager.db_manager.is_available():
            self.video_record_manager.fail_processing(video_path, clean_error)
        
        # ç»§ç»­ç»´æŠ¤æœ¬åœ°è®°å½•ï¼ˆå…¼å®¹æ€§ï¼‰
        # æ£€æŸ¥æ˜¯å¦å·²åœ¨å¤±è´¥åˆ—è¡¨ä¸­
        failed_names = [f.get('name') if isinstance(f, dict) else f for f in self.progress_data['failed']]
        if video_name not in failed_names:
            self.progress_data['failed'].append({
                'name': video_name,
                'error': clean_error,
                'time': datetime.now().isoformat()
            })
        
        # ä»å¤„ç†ä¸­ç§»é™¤
        if video_name in self.progress_data['processing']:
            self.progress_data['processing'].remove(video_name)
        
        self.save_progress()

    def clean_error_message(self, error_msg: str) -> str:
        """æ¸…ç†é”™è¯¯æ¶ˆæ¯"""
        if not error_msg:
            return "å¤„ç†å¤±è´¥"
        
        if 'frame=' in error_msg or 'fps=' in error_msg:
            lines = error_msg.split('\n')
            clean_lines = []
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                if not any(x in line for x in ['frame=', 'fps=', 'time=', 'bitrate=', 'speed=']):
                    clean_lines.append(line)
            return '\n'.join(clean_lines[:3]) if clean_lines else "FFmpegå¤„ç†å¤±è´¥"
        
        return error_msg

    def get_statistics(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'completed': len(self.progress_data['completed']),
            'processing': len(self.progress_data['processing']),
            'failed': len(self.progress_data['failed'])
        }

def get_video_info(video_path: str) -> Dict[str, Any]:
    """è·å–è§†é¢‘ä¿¡æ¯"""
    try:
        cmd = [FFPROBE_PATH, '-v', 'quiet', '-print_format', 'json', '-show_format', '-show_streams', video_path]
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', timeout=30)
        
        if result.returncode != 0:
            return {}
        
        data = json.loads(result.stdout)
        video_stream = None
        
        for stream in data.get('streams', []):
            if stream.get('codec_type') == 'video':
                video_stream = stream
                break
        
        if not video_stream:
            return {}
        
        format_info = data.get('format', {})
        
        return {
            'width': int(video_stream.get('width', 0)),
            'height': int(video_stream.get('height', 0)),
            'duration': float(video_stream.get('duration', 0)) or float(format_info.get('duration', 0)),
            'fps': eval(video_stream.get('r_frame_rate', '25/1')),
            'bitrate': int(format_info.get('bit_rate', 0)),
            'codec': video_stream.get('codec_name', 'unknown'),
            'pixel_format': video_stream.get('pix_fmt', 'unknown')
        }
        
    except Exception as e:
        logging.warning(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥ {video_path}: {e}")
        return {}

def adjust_roi_by_ratio(roi_with_base: Tuple[int, int, int, int, int, int], current_video_path: str) -> Tuple[int, int, int, int]:
    """æ ¹æ®é¢„è§ˆè§†é¢‘ä¸å½“å‰è§†é¢‘çš„å°ºå¯¸æ¯”ä¾‹è°ƒæ•´ROIå‚æ•°
    
    Args:
        roi_with_base: (x, y, w, h, base_width, base_height) - åŸºäºé¢„è§ˆè§†é¢‘çš„ROIå’Œé¢„è§ˆè§†é¢‘å°ºå¯¸
        current_video_path: å½“å‰è¦å¤„ç†çš„è§†é¢‘è·¯å¾„
    
    Returns:
        è°ƒæ•´åçš„ROI: (x, y, w, h)
    """
    x, y, w, h, base_width, base_height = roi_with_base
    
    try:
        # è·å–å½“å‰è§†é¢‘çš„å®é™…å°ºå¯¸
        current_info = get_video_info(current_video_path)
        if not current_info:
            logging.warning(f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹ROI: {current_video_path}")
            return (x, y, w, h)
        
        current_width = current_info.get('width', 0)
        current_height = current_info.get('height', 0)
        
        if current_width <= 0 or current_height <= 0:
            logging.warning(f"è§†é¢‘å°ºå¯¸æ— æ•ˆï¼Œä½¿ç”¨åŸå§‹ROI: {current_width}x{current_height}")
            return (x, y, w, h)
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        width_ratio = current_width / base_width
        height_ratio = current_height / base_height
        
        # æŒ‰æ¯”ä¾‹è°ƒæ•´ROIå‚æ•°
        new_x = int(x * width_ratio)
        new_y = int(y * height_ratio)
        new_w = int(w * width_ratio)
        new_h = int(h * height_ratio)
        
        # è¾¹ç•Œæ£€æŸ¥å’Œä¿®æ­£
        new_x = max(0, min(new_x, current_width - 1))
        new_y = max(0, min(new_y, current_height - 1))
        new_w = min(new_w, current_width - new_x)
        new_h = min(new_h, current_height - new_y)
        
        # ç¡®ä¿è£å‰ªåŒºåŸŸæœ‰æ•ˆ
        if new_w <= 0 or new_h <= 0:
            logging.error(f"è°ƒæ•´åçš„ROIæ— æ•ˆ: {new_w}x{new_h}")
            return (x, y, w, h)  # è¿”å›åŸå§‹ROI
        
        # è®°å½•è°ƒæ•´ä¿¡æ¯
        if (new_x != x or new_y != y or new_w != w or new_h != h):
            logging.info(f"ğŸ”„ ROIæŒ‰æ¯”ä¾‹è°ƒæ•´: åŸºå‡†({base_width}x{base_height}) -> å½“å‰({current_width}x{current_height})")
            logging.info(f"   ç¼©æ”¾æ¯”ä¾‹: {width_ratio:.3f}x{height_ratio:.3f}")
            logging.info(f"   ROIè°ƒæ•´: ({x},{y},{w},{h}) -> ({new_x},{new_y},{new_w},{new_h})")
        
        return (new_x, new_y, new_w, new_h)
        
    except Exception as e:
        logging.error(f"ROIæ¯”ä¾‹è°ƒæ•´å¤±è´¥: {e}")
        return (x, y, w, h)  # è¿”å›åŸå§‹ROI

def should_skip_low_resolution(video_path: str) -> Tuple[bool, Optional[Tuple[int, int]], str]:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ä½åˆ†è¾¨ç‡è§†é¢‘"""
    if not SKIP_LOW_RESOLUTION_VIDEOS:
        return False, None, ""
    
    video_info = get_video_info(video_path)
    if not video_info:
        return False, None, "æ— æ³•è·å–è§†é¢‘ä¿¡æ¯"
    
    width = video_info.get('width', 0)
    height = video_info.get('height', 0)
    
    if width < MIN_RESOLUTION_WIDTH:
        return True, (width, height), f"åˆ†è¾¨ç‡({width}x{height})ä½äºæœ€å°è¦æ±‚({MIN_RESOLUTION_WIDTH}pxå®½åº¦)"
    
    return False, (width, height), ""

def build_unified_ffmpeg_command(input_file: str, output_file: str, 
                                roi: Optional[Tuple[int, int, int, int, int, int]] = None,
                                hardware_info: Dict[str, Any] = None) -> List[str]:
    """æ„å»ºç»Ÿä¸€çš„FFmpegå‘½ä»¤ï¼Œæ”¯æŒåˆ‡å¤´å°¾+è£å‰ª"""
    # ç¡®ä¿è·¯å¾„ä½¿ç”¨æ­£ç¡®çš„åˆ†éš”ç¬¦å¹¶å¤„ç†ç‰¹æ®Šå­—ç¬¦
    input_file = os.path.normpath(input_file)
    output_file = os.path.normpath(output_file)
    
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„ä»¥é¿å…è·¯å¾„è§£æé—®é¢˜
    if not os.path.isabs(input_file):
        input_file = os.path.abspath(input_file)
    if not os.path.isabs(output_file):
        output_file = os.path.abspath(output_file)
    
    # æä¾›é»˜è®¤ç¡¬ä»¶ä¿¡æ¯
    if hardware_info is None:
        hardware_info = {
            'probe_size': '50M',
            'buffer_size': '1024M',
            'encoder_type': 'software',
            'options': {}
        }
    
    cmd = [FFMPEG_PATH, '-y', '-nostdin']
    
    # è¾“å…¥ä¼˜åŒ–å‚æ•°
    cmd.extend(['-probesize', hardware_info.get('probe_size', '50M')])
    cmd.extend(['-analyzeduration', hardware_info.get('probe_size', '50M')])
    
    # åˆ‡å¤´å°¾æ—¶é—´è®¾ç½®
    if ENABLE_HEAD_TAIL_CUT and HEAD_CUT_TIME > 0:
        cmd.extend(['-ss', str(HEAD_CUT_TIME)])
    
    # ä½¿ç”¨å¼•å·åŒ…å›´è¾“å…¥æ–‡ä»¶è·¯å¾„ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
    cmd.extend(['-i', input_file])
    
    # è®¡ç®—æœ‰æ•ˆæ—¶é•¿ï¼ˆå¦‚æœå¯ç”¨äº†åˆ‡å¤´å°¾ï¼‰
    if ENABLE_HEAD_TAIL_CUT:
        video_info = get_video_info(input_file)
        total_duration = video_info.get('duration', 0)
        effective_duration = max(0, total_duration - HEAD_CUT_TIME - TAIL_CUT_TIME)
        if effective_duration > 0:
            cmd.extend(['-t', str(effective_duration)])
    
    # è§†é¢‘æ»¤é•œè®¾ç½®
    if ENABLE_CROPPING and roi:
        # å¦‚æœROIåŒ…å«åŸºå‡†å°ºå¯¸ä¿¡æ¯ï¼Œåˆ™è¿›è¡Œæ¯”ä¾‹è°ƒæ•´
        if len(roi) == 6:
            # åŒ…å«åŸºå‡†å°ºå¯¸çš„ROIï¼Œéœ€è¦æŒ‰æ¯”ä¾‹è°ƒæ•´
            x, y, w, h = adjust_roi_by_ratio(roi, input_file)
        else:
            # ä¼ ç»Ÿçš„4å‚æ•°ROIï¼Œç›´æ¥ä½¿ç”¨
            x, y, w, h = roi
        
        # è·å–åŸè§†é¢‘å°ºå¯¸è¿›è¡Œè¾¹ç•Œæ£€æŸ¥
        video_info = get_video_info(input_file)
        video_width = video_info.get('width', 0)
        video_height = video_info.get('height', 0)
        
        # è¾¹ç•Œæ£€æŸ¥å’Œè‡ªåŠ¨ä¿®æ­£
        if video_width > 0 and video_height > 0:
            # ç¡®ä¿è£å‰ªåŒºåŸŸä¸è¶…å‡ºè§†é¢‘è¾¹ç•Œ
            max_x = max(0, min(x, video_width - 1))
            max_y = max(0, min(y, video_height - 1))
            max_w = min(w, video_width - max_x)
            max_h = min(h, video_height - max_y)
            
            # å¦‚æœè°ƒæ•´äº†å‚æ•°ï¼Œè®°å½•æ—¥å¿—
            if (max_x != x or max_y != y or max_w != w or max_h != h):
                logging.warning(f"ğŸ”§ è£å‰ªå‚æ•°è¾¹ç•Œè°ƒæ•´: åŸ({x},{y},{w},{h}) -> æ–°({max_x},{max_y},{max_w},{max_h}), è§†é¢‘å°ºå¯¸: {video_width}x{video_height}")
                x, y, w, h = max_x, max_y, max_w, max_h
            
            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿è£å‰ªåŒºåŸŸæœ‰æ•ˆ
            if w > 0 and h > 0:
                filter_complex = f"crop={w}:{h}:{x}:{y},scale={TARGET_RESOLUTION[0]}:{TARGET_RESOLUTION[1]}:force_original_aspect_ratio=disable"
                cmd.extend(['-vf', filter_complex])
            else:
                logging.error(f"âŒ è£å‰ªå‚æ•°æ— æ•ˆ: crop={w}:{h}:{x}:{y}, è§†é¢‘å°ºå¯¸: {video_width}x{video_height}")
                raise ValueError(f"è£å‰ªå‚æ•°è¶…å‡ºè§†é¢‘è¾¹ç•Œï¼Œæ— æ³•å¤„ç†")
        else:
            logging.error(f"âŒ æ— æ³•è·å–è§†é¢‘å°ºå¯¸ä¿¡æ¯: {input_file}")
            raise ValueError(f"æ— æ³•è·å–è§†é¢‘å°ºå¯¸ä¿¡æ¯ï¼Œæ— æ³•éªŒè¯è£å‰ªå‚æ•°")
        cmd.extend(['-s', f'{TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}'])
    
    # ç¼–ç å™¨è®¾ç½®
    cmd.extend(['-c:v', hardware_info['encoder']])
    
    # ç¼–ç å™¨å‚æ•°
    options = hardware_info.get('options', {})
    for key, value in options.items():
        cmd.extend([f'-{key}', str(value)])
    
    # GOPè®¾ç½®
    if hardware_info['encoder_type'] == 'nvidia':
        cmd.extend(['-g', '120', '-keyint_min', '60'])
    elif hardware_info['encoder_type'] in ['amd', 'intel']:
        cmd.extend(['-g', '120', '-keyint_min', '60'])
    else:  # software
        cmd.extend(['-g', '120', '-keyint_min', '60', '-sc_threshold', '40', '-bf', '2'])
    
    # éŸ³é¢‘å¤„ç†
    cmd.extend(['-c:a', 'aac', '-b:a', '192k'])
    
    # è¾“å‡ºä¼˜åŒ–å‚æ•°
    cmd.extend([
        '-movflags', '+faststart',
        '-map_metadata', '-1',
        '-fps_mode', 'cfr',
        '-avoid_negative_ts', 'make_zero',
        '-fflags', '+genpts',
        '-max_muxing_queue_size', hardware_info.get('buffer_size', '2048').replace('M', ''),
        '-f', 'mp4',  # æ˜ç¡®æŒ‡å®šè¾“å‡ºæ ¼å¼ä¸ºmp4
        output_file
    ])
    
    return cmd

def parse_progress(line: str) -> Dict[str, Any]:
    """è§£æFFmpegè¿›åº¦ä¿¡æ¯"""
    info = {}
    patterns = {
        'frame': r'frame=\s*(\d+)',
        'fps': r'fps=\s*([\d\.]+)',
        'time': r'time=\s*(\d+):(\d+):([\d\.]+)',
        'speed': r'speed=\s*([\d\.]+)x',
        'size': r'size=\s*(\d+)kB'
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, line)
        if match:
            if key == 'time':
                info[key] = float(match.group(1)) * 3600 + float(match.group(2)) * 60 + float(match.group(3))
            elif key in ['fps', 'speed']:
                info[key] = float(match.group(1))
            else:
                info[key] = int(match.group(1))
    
    return info

def run_ffmpeg_process(cmd: List[str], expected_duration: float, pbar, video_path: str, 
                      i9_optimizer: Optional[I9PerformanceOptimizer] = None):
    """ğŸš€ è¿è¡ŒFFmpegè¿›ç¨‹å¹¶ç›‘æ§è¿›åº¦ - 2025å¹´æé™ä¼˜åŒ–ç‰ˆ"""
    process = subprocess.Popen(
        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
        universal_newlines=True, encoding='utf-8', errors='ignore', bufsize=1
    )
    
    # ğŸ”¥ i9å¤„ç†å™¨CPUäº²å’Œæ€§ä¼˜åŒ–
    if i9_optimizer:
        i9_optimizer.set_process_affinity(process, 'ffmpeg_encoding')
    
    last_percentage = 0
    last_update_time = time.time()
    stalled_time = 0
    no_progress_count = 0
    
    # å¯¹é•¿è§†é¢‘è°ƒæ•´è¶…æ—¶å‚æ•°
    is_long_video = expected_duration > 3600
    max_stall_time = 300 if is_long_video else 120
    max_no_progress_time = 600 if is_long_video else 300
    
    print(f"ğŸ¬ å¼€å§‹FFmpegå¤„ç†: æ—¶é•¿={expected_duration:.1f}s, é•¿è§†é¢‘æ¨¡å¼={is_long_video}")
    logging.info(f"å¼€å§‹å¤„ç†: æ—¶é•¿={expected_duration:.1f}s, é•¿è§†é¢‘æ¨¡å¼={is_long_video}")
    
    while process.poll() is None:
        line = process.stderr.readline()
        if line:
            progress_info = parse_progress(line)
            if 'time' in progress_info:
                last_update_time = time.time()
                no_progress_count = 0
                
                current_time = progress_info['time']
                if current_time > expected_duration:
                    current_time = expected_duration
                
                # è¿›åº¦è®¡ç®—ï¼š0-95%èŒƒå›´
                if current_time >= expected_duration * 0.95:
                    percentage = 95
                else:
                    percentage = min(95, 10 + current_time * 85 / expected_duration)
                
                if percentage > last_percentage:
                    pbar.update(percentage - last_percentage)
                    last_percentage = percentage
                    
                    postfix = {
                        'FPS': f"{progress_info.get('fps', 0):.1f}",
                        'é€Ÿåº¦': f"{progress_info.get('speed', 0):.1f}x",
                        'æ—¶é—´': f"{current_time:.1f}s/{expected_duration:.1f}s",
                        'è¿›åº¦': f"{current_time/expected_duration*100:.1f}%"
                    }
                    pbar.set_postfix(postfix)
                
                # å¡æ­»æ£€æµ‹
                speed = progress_info.get('speed', 1.0)
                if speed < 0.01:
                    stalled_time += 1
                elif speed < 0.1 and is_long_video:
                    stalled_time += 0.5
                else:
                    stalled_time = 0
                
                if stalled_time > max_stall_time:
                    process.terminate()
                    raise Exception(f"å¤„ç†é€Ÿåº¦è¿‡æ…¢ï¼Œå¯èƒ½å·²å¡æ­» (é€Ÿåº¦: {speed}x)")
        else:
            no_progress_count += 1
            if time.time() - last_update_time > max_no_progress_time:
                process.terminate()
                raise Exception(f"å¤„ç†è¶…æ—¶ï¼Œ{max_no_progress_time}ç§’å†…æ— è¿›åº¦æ›´æ–°")
        
        time.sleep(1 if is_long_video else 0.5)
    
    # æ£€æŸ¥è¿”å›ç 
    if process.returncode != 0:
        remaining_stderr = process.stderr.read()
        error_lines = []
        for line in remaining_stderr.split('\n'):
            if (line.strip() and 
                not line.startswith('frame=') and 
                not line.startswith('size=') and
                'fps=' not in line):
                error_lines.append(line.strip())
        
        filtered_errors = error_lines[-10:] if error_lines else ["æ— å…·ä½“é”™è¯¯ä¿¡æ¯"]
        error_msg = '\n'.join(filtered_errors)
        raise Exception(f"ffmpegå¤„ç†å¤±è´¥ (ä»£ç  {process.returncode}): {error_msg}")
    
    # ç¡®ä¿è¿›åº¦æ¡åˆ°è¾¾95%
    if last_percentage < 95:
        pbar.update(95 - last_percentage)

def process_single_video(video_path: str, output_path: str, roi: Optional[Tuple[int, int, int, int]], 
                        hardware_info: Dict[str, Any], video_idx: int, total_videos: int) -> Tuple[bool, float, Optional[str]]:
    """å¤„ç†å•ä¸ªè§†é¢‘"""
    filename = os.path.basename(video_path)
    start_time = time.time()
    
    # åˆ›å»ºè¿›åº¦æ¡ - ğŸ”§ çº¿ç¨‹å®‰å…¨ä¼˜åŒ–
    safe_position = None if video_idx >= 8 else video_idx + 1  # é™åˆ¶åŒæ—¶æ˜¾ç¤ºçš„è¿›åº¦æ¡æ•°é‡
    show_individual_bar = video_idx < 8  # åªæ˜¾ç¤ºå‰8ä¸ªè§†é¢‘çš„è¿›åº¦æ¡ï¼Œé¿å…å±å¹•æ··ä¹±
    
    pbar = tqdm(
        total=100, 
        desc=f"è§†é¢‘ {video_idx + 1}/{total_videos}: {filename[:25]:<25}",
        position=safe_position,
        leave=False,
        disable=not show_individual_bar,  # è¶…è¿‡8ä¸ªæ—¶ç¦ç”¨ä¸ªåˆ«è¿›åº¦æ¡
        bar_format='{l_bar}{bar:30}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]',
        smoothing=0.3,
        mininterval=1.0,  # ä¸ªåˆ«è¿›åº¦æ¡æ›´æ–°é—´éš”æ›´é•¿
        maxinterval=3.0
    )
    
    try:
        pbar.set_postfix_str("åˆ†æè§†é¢‘ä¿¡æ¯...")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = get_video_info(video_path)
        if not video_info:
            raise Exception("æ— æ³•è·å–è§†é¢‘ä¿¡æ¯")
        
        duration = video_info.get('duration', 0)
        if duration <= 0:
            raise Exception("è§†é¢‘æ—¶é•¿æ— æ•ˆ")
        
        # è®¡ç®—æœ‰æ•ˆæ—¶é•¿ï¼ˆè€ƒè™‘åˆ‡å¤´å°¾ï¼‰
        if ENABLE_HEAD_TAIL_CUT:
            effective_duration = max(0, duration - HEAD_CUT_TIME - TAIL_CUT_TIME)
        else:
            effective_duration = duration
        
        if effective_duration <= 0:
            raise Exception("åˆ‡å¤´å°¾åæ—¶é•¿æ— æ•ˆ")
        
        pbar.set_postfix_str("æ„å»ºå¤„ç†å‘½ä»¤...")
        
        # æ„å»ºFFmpegå‘½ä»¤
        cmd = build_unified_ffmpeg_command(video_path, output_path, roi, hardware_info)
        
        logging.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        pbar.set_postfix_str("å¼€å§‹å¤„ç†è§†é¢‘...")
        
        # ğŸš€ è¿è¡ŒFFmpeg (ä½¿ç”¨i9ä¼˜åŒ–å™¨)
        i9_optimizer = hardware_info.get('i9_optimizer')
        run_ffmpeg_process(cmd, effective_duration, pbar, video_path, i9_optimizer)
        
        # æ›´æ–°æœ€ç»ˆè¿›åº¦ (95-100%)
        if pbar.n < 100:
            pbar.update(100 - pbar.n)
            pbar.set_postfix_str("å¤„ç†å®Œæˆâœ“")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        if not os.path.exists(output_path) or os.path.getsize(output_path) < 1024:
            raise Exception(f"è¾“å‡ºæ–‡ä»¶æ— æ•ˆ: {output_path}")
        
        processing_time = time.time() - start_time
        
        pbar.close()
        print(f"âœ… FFmpegå¤„ç†å®Œæˆ: {filename}, è€—æ—¶: {processing_time:.2f}s")
        logging.info(f"è§†é¢‘å¤„ç†å®Œæˆ: {filename}, è€—æ—¶: {processing_time:.2f}s")
        
        return True, processing_time, None
        
    except Exception as e:
        pbar.set_postfix_str("å¤„ç†å¤±è´¥âœ—")
        pbar.close()
        
        error_msg = str(e)
        print(f"âŒ FFmpegå¤„ç†å¤±è´¥: {filename} - {error_msg}")
        logging.error(f"è§†é¢‘å¤„ç†å¤±è´¥ {filename}: {error_msg}")
        
        return False, time.time() - start_time, error_msg

def find_video_files(directory: str) -> List[str]:
    """æŸ¥æ‰¾ç›®å½•ä¸­æ‰€æœ‰æ”¯æŒçš„è§†é¢‘æ–‡ä»¶"""
    video_files = []
    
    try:
        for format_ext in SUPPORTED_VIDEO_FORMATS:
            pattern = os.path.join(directory, f'*{format_ext}')
            files = [f for f in os.listdir(directory) 
                    if f.lower().endswith(format_ext.lower())]
            video_files.extend([os.path.join(directory, f) for f in files])
        
        video_files = list(set(video_files))  # å»é‡
        video_files.sort()
        
        logging.info(f"åœ¨ç›®å½• {directory} ä¸­æ‰¾åˆ° {len(video_files)} ä¸ªæ”¯æŒçš„è§†é¢‘æ–‡ä»¶")
        
    except Exception as e:
        logging.error(f"æœç´¢è§†é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return video_files

def process_video_batch_with_pipeline(video_paths: List[str], roi: Optional[Tuple[int, int, int, int]], 
                                    hardware_info: Dict[str, Any], progress_manager: ProgressManager) -> Dict[str, Any]:
    """ğŸš€ ä½¿ç”¨æµæ°´çº¿ç®¡ç†å™¨æ‰¹é‡å¤„ç†è§†é¢‘ - é˜²å¡æ­»åŠ å›ºç‰ˆ"""
    
    # è·å–ç»„ä»¶
    cache_manager = hardware_info.get('cache_manager')
    i9_optimizer = hardware_info.get('i9_optimizer')
    
    # åˆ›å»ºæµæ°´çº¿ç®¡ç†å™¨ - 2025å¹´æé™ä¼˜åŒ–ï¼šç¿»å€å¤„ç†æ•ˆç‡
    pipeline_manager = VideoPipelineManager(
        max_concurrent_cache=6,    # 6ä¸ªç¼“å­˜çº¿ç¨‹ï¼ˆç¿»å€ï¼‰
        max_concurrent_check=4,    # 4ä¸ªæŸ¥é‡çº¿ç¨‹ï¼ˆç¿»å€ï¼‰ 
        max_concurrent_process=4   # 4ä¸ªå¤„ç†çº¿ç¨‹ï¼ˆç¿»å€ï¼‰
    )
    
    # åˆ›å»ºè§†é¢‘å¤„ç†å™¨é€‚é…å™¨
    class VideoProcessor:
        def __init__(self, roi, hardware_info, progress_manager):
            self.roi = roi
            self.hardware_info = hardware_info
            self.progress_manager = progress_manager
            
        def process_single_video(self, input_path: str, original_path: str) -> bool:
            """å¤„ç†å•ä¸ªè§†é¢‘"""
            try:
                # è°ƒç”¨åŸæœ‰çš„è§†é¢‘å¤„ç†é€»è¾‘
                return process_single_video_file(
                    input_path=input_path,
                    original_path=original_path,
                    roi=self.roi,
                    hardware_info=self.hardware_info,
                    progress_manager=self.progress_manager
                )
            except Exception as e:
                logging.error(f"è§†é¢‘å¤„ç†å¼‚å¸¸ {os.path.basename(original_path)}: {e}")
                return False
    
    video_processor = VideoProcessor(roi, hardware_info, progress_manager)
    
    # ç»Ÿè®¡å˜é‡
    start_time = time.time()
    
    try:
        logging.info(f"ğŸš€ å¯åŠ¨è§†é¢‘å¤„ç†æµæ°´çº¿...")
        
        # ğŸš€ æ‰¹é‡æ•°æ®åº“å¿«é€Ÿæ£€æŸ¥ - åœ¨æ·»åŠ ä»»åŠ¡å‰è¿‡æ»¤å·²å¤„ç†è§†é¢‘
        videos_to_process, skipped_videos = pipeline_manager.bulk_database_check(video_paths, progress_manager)
        
        if skipped_videos:
            logging.info(f"ğŸ” æ‰¹é‡æ£€æŸ¥è·³è¿‡ {len(skipped_videos)} ä¸ªå·²å¤„ç†è§†é¢‘")
            for skipped in skipped_videos:
                logging.debug(f"  â””â”€ {skipped['video_name']} (å¤„ç†è€…: {skipped['processor']})")
        
        # å¯åŠ¨æµæ°´çº¿
        pipeline_manager.start_pipeline(cache_manager, progress_manager, video_processor)
        
        # æ·»åŠ éœ€è¦å¤„ç†çš„ä»»åŠ¡
        if videos_to_process:
            logging.info(f"ğŸ“ æ·»åŠ  {len(videos_to_process)} ä¸ªä»»åŠ¡åˆ°æµæ°´çº¿...")
            for video_path in videos_to_process:
                pipeline_manager.add_task(video_path)
        else:
            logging.info("ğŸ¯ æ‰€æœ‰è§†é¢‘éƒ½å·²å¤„ç†å®Œæˆï¼Œæ— éœ€æ·»åŠ æ–°ä»»åŠ¡")
        
        # åˆ›å»ºè¿›åº¦ç›‘æ§ 
        actual_task_count = len(videos_to_process) if videos_to_process else 0
        progress_thread = Thread(
            target=_monitor_pipeline_progress,
            args=(pipeline_manager, actual_task_count),
            daemon=True
        )
        progress_thread.start()
        
        # ç­‰å¾…å®Œæˆ
        logging.info("â³ ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ...")
        success = pipeline_manager.wait_completion(timeout=None)  # æ— é™ç­‰å¾…ï¼Œè®©ä»»åŠ¡è‡ªç„¶å®Œæˆ
        
        if not success:
            logging.warning("â° æµæ°´çº¿å¤„ç†è¶…æ—¶")
        else:
            logging.info("âœ… æµæ°´çº¿å¤„ç†æ­£å¸¸å®Œæˆ")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = pipeline_manager.get_statistics()
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡ï¼ˆåŒ…æ‹¬æ‰¹é‡æ£€æŸ¥è·³è¿‡çš„è§†é¢‘ï¼‰
        total_skipped = len(skipped_videos)
        pipeline_completed = stats['completed_tasks']
        pipeline_failed = stats['failed_tasks']
        pipeline_duplicates = stats['duplicate_tasks']
        
        results = {
            'success_count': pipeline_completed + total_skipped,  # åŒ…æ‹¬æ‰¹é‡æ£€æŸ¥è·³è¿‡çš„
            'failed_count': pipeline_failed + pipeline_duplicates,
            'total_count': len(video_paths),  # åŸå§‹æ€»æ•°
            'duplicate_count': pipeline_duplicates + total_skipped,  # åŒ…æ‹¬æ‰¹é‡æ£€æŸ¥å‘ç°çš„é‡å¤
            'pipeline_stats': stats,
            'bulk_check_skipped': total_skipped,
            'actually_processed': pipeline_completed
        }
        
        elapsed_time = time.time() - start_time
        logging.info(f"âœ… æµæ°´çº¿å¤„ç†å®Œæˆ: {results['success_count']}/{results['total_count']} "
                    f"æˆåŠŸ, è€—æ—¶ {elapsed_time:.1f}s")
        
        return results
        
    except Exception as e:
        logging.error(f"âŒ æµæ°´çº¿å¤„ç†å¼‚å¸¸: {e}")
        return {
            'success_count': 0,
            'failed_count': len(video_paths),
            'total_count': len(video_paths),
            'duplicate_count': 0,
            'error': str(e)
        }
    finally:
        # ç¡®ä¿æµæ°´çº¿å…³é—­
        pipeline_manager.shutdown()

def _monitor_pipeline_progress(pipeline_manager: VideoPipelineManager, total_tasks: int):
    """ç›‘æ§æµæ°´çº¿è¿›åº¦ - ç®€åŒ–ç‰ˆ"""
    last_completed = 0
    
    # ç®€åŒ–è¿›åº¦æ¡ - åªæ˜¾ç¤ºå®Œæˆæ•°é‡
    pbar = tqdm(
        total=total_tasks,
        desc="ğŸ“¹ è§†é¢‘å¤„ç†",
        bar_format='{l_bar}{bar:40}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
        position=0,
        leave=True,
        mininterval=0.5
    )
    
    try:
        last_stats_time = time.time()
        
        while not pipeline_manager.shutdown_event.is_set():
            stats = pipeline_manager.get_statistics()
            completed = stats['completed_tasks']
            failed = stats['failed_tasks']
            duplicates = stats['duplicate_tasks']
            total_finished = completed + failed + duplicates
            
            # æ›´æ–°è¿›åº¦æ¡
            if total_finished > last_completed:
                pbar.update(total_finished - last_completed)
                last_completed = total_finished
            
            # æ¯30ç§’æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            current_time = time.time()
            if current_time - last_stats_time >= 30:
                active_by_stage = stats.get('active_by_stage', {})
                queue_sizes = stats.get('queue_sizes', {})
                
                pending = active_by_stage.get('pending', 0)
                caching = active_by_stage.get('caching', 0)
                checking = active_by_stage.get('checking', 0)
                processing = active_by_stage.get('processing', 0)
                cached = active_by_stage.get('cached', 0)
                
                avg_time = stats.get('avg_process_time', 0)
                
                print(f"\nğŸ“Š å½“å‰çŠ¶æ€: å®Œæˆ={completed}, å¤±è´¥={failed}, é‡å¤={duplicates}")
                print(f"ğŸ“Š æ´»åŠ¨ä»»åŠ¡: ç­‰å¾…={pending}, ç¼“å­˜ä¸­={caching}, æ£€æŸ¥ä¸­={checking}, å¤„ç†ä¸­={processing}, å·²ç¼“å­˜={cached}")
                print(f"ğŸ“Š é˜Ÿåˆ—å¤§å°: {queue_sizes}")
                if avg_time > 0:
                    print(f"ğŸ“Š å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.1f}ç§’")
                last_stats_time = current_time
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if total_finished >= total_tasks:
                break
            
            time.sleep(1)
            
    except Exception as e:
        logging.error(f"è¿›åº¦ç›‘æ§å¼‚å¸¸: {e}")
    finally:
        pbar.close()

def process_single_video_file(input_path: str, original_path: str, roi: Optional[Tuple[int, int, int, int]], 
                            hardware_info: Dict[str, Any], progress_manager: ProgressManager) -> bool:
    """å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶"""
    try:
        video_name = os.path.basename(original_path)
        logging.info(f"âš™ï¸ å¼€å§‹å¤„ç†: {video_name}")
        
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        base_name = os.path.splitext(os.path.basename(original_path))[0]
        output_path = os.path.join(OUTPUT_DIR, f"{base_name}.mp4")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            logging.info(f"âœ… è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {video_name}")
            progress_manager.mark_completed(original_path, output_path)
            return True
        
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºè·¯å¾„
        temp_output = output_path + ".tmp"
        
        # æ„å»ºFFmpegå‘½ä»¤ï¼ˆä½¿ç”¨å·²æœ‰å‡½æ•°ï¼‰
        cmd = build_unified_ffmpeg_command(input_path, temp_output, roi, hardware_info)
        
        # æ‰§è¡Œå¤„ç†
        success = _execute_ffmpeg_safe(cmd, video_name)
        
        if success and os.path.exists(temp_output):
            # ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®
            shutil.move(temp_output, output_path)
            logging.info(f"âœ… å¤„ç†å®Œæˆ: {video_name}")
            
            # è®°å½•å®ŒæˆçŠ¶æ€
            progress_manager.mark_completed(original_path, output_path)
            
            # è®¡ç®—å¹¶è®°å½•SHA256ï¼ˆå¦‚æœå¯ç”¨æ•°æ®åº“ï¼‰- ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è·¯å¾„è®¡ç®—å“ˆå¸Œ
            if progress_manager.video_record_manager:
                try:
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è·¯å¾„è€Œä¸æ˜¯ç¼“å­˜è·¯å¾„è®¡ç®—è¾“å…¥å“ˆå¸Œ
                    original_sha256 = progress_manager.video_record_manager.calculate_video_sha256(original_path, quick_mode=False)
                    output_sha256 = progress_manager.video_record_manager.calculate_video_sha256(output_path, quick_mode=False) 
                    
                    if original_sha256 and output_sha256:
                        progress_manager.video_record_manager.record_processing_result(
                            video_path=original_path,
                            input_sha256=original_sha256,
                            output_path=output_path,
                            output_sha256=output_sha256,
                            processing_status='completed',
                            metadata={'roi': roi} if roi else {}
                        )
                        logging.debug(f"ğŸ“ æ•°æ®åº“è®°å½•å·²æ›´æ–° (åŸå§‹è·¯å¾„): {video_name}")
                except Exception as e:
                    logging.warning(f"æ•°æ®åº“è®°å½•å¤±è´¥ {video_name}: {e}")
            
            return True
        else:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_output):
                os.remove(temp_output)
            logging.error(f"âŒ å¤„ç†å¤±è´¥: {video_name}")
            return False
            
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¼‚å¸¸ {os.path.basename(original_path)}: {e}")
        return False

def _execute_ffmpeg_safe(cmd: List[str], video_name: str, timeout: int = 1800) -> bool:
    """å®‰å…¨æ‰§è¡ŒFFmpegå‘½ä»¤ï¼Œå¸¦è¶…æ—¶å’Œé”™è¯¯å¤„ç†"""
    input_file = None
    output_file = None
    
    try:
        # æ£€æŸ¥FFmpegæ˜¯å¦å­˜åœ¨
        if not os.path.exists(cmd[0]):
            logging.error(f"âŒ FFmpegä¸å­˜åœ¨: {cmd[0]}")
            return False
            
        # ä»å‘½ä»¤ä¸­æå–è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if '-i' in cmd:
            input_idx = cmd.index('-i')
            if input_idx + 1 < len(cmd):
                input_file = cmd[input_idx + 1]
        if len(cmd) > 1:
            output_file = cmd[-1]  # é€šå¸¸è¾“å‡ºæ–‡ä»¶æ˜¯æœ€åä¸€ä¸ªå‚æ•°
            
        logging.debug(f"ğŸ”§ æ‰§è¡ŒFFmpeg: {' '.join(cmd[:5])}... (å…±{len(cmd)}ä¸ªå‚æ•°)")
        
        # è¯¦ç»†è®°å½•å®Œæ•´å‘½ä»¤ç”¨äºè°ƒè¯•
        if logging.getLogger().isEnabledFor(logging.DEBUG):
            logging.debug(f"å®Œæ•´FFmpegå‘½ä»¤: {' '.join(cmd)}")
        
        # ä½¿ç”¨subprocess.runè€Œä¸æ˜¯å…¶ä»–æ–¹å¼ï¼Œç¡®ä¿ç¨³å®šæ€§
        # åœ¨Windowsä¸Šè®¾ç½®æ­£ç¡®çš„ç¼–ç å’Œåˆ›å»ºæ ‡å¿—
        startupinfo = None
        if os.name == 'nt':  # Windows
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',  # å¤„ç†ç¼–ç é”™è¯¯
            timeout=timeout,
            check=False,  # ä¸è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸
            startupinfo=startupinfo,
            cwd=os.path.dirname(cmd[0]) if os.path.dirname(cmd[0]) else None  # è®¾ç½®å·¥ä½œç›®å½•
        )
        
        if result.returncode == 0:
            logging.debug(f"âœ… FFmpegæ‰§è¡ŒæˆåŠŸ: {video_name}")
            return True
        else:
            # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
            error_msg = result.stderr if result.stderr else "æœªçŸ¥é”™è¯¯"
            logging.error(f"âŒ FFmpegæ‰§è¡Œå¤±è´¥ {video_name}: è¿”å›ç  {result.returncode}")
            
            # è¾“å‡ºå®Œæ•´å‘½ä»¤ç”¨äºè°ƒè¯•
            logging.error(f"å®Œæ•´å‘½ä»¤: {' '.join(cmd)}")
            
            if error_msg.strip():
                logging.error(f"FFmpegé”™è¯¯è¾“å‡º: {error_msg}")
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯åŸå› 
            if result.returncode == 4294967274 or result.returncode == -22:
                logging.error("å¯èƒ½åŸå› : æ–‡ä»¶è·¯å¾„åŒ…å«ç‰¹æ®Šå­—ç¬¦ã€æƒé™ä¸è¶³æˆ–ç¼–ç å™¨ä¸æ”¯æŒ")
                logging.error(f"è¾“å…¥æ–‡ä»¶å­˜åœ¨: {os.path.exists(input_file) if input_file else 'æœªçŸ¥'}")
                logging.error(f"è¾“å‡ºç›®å½•å­˜åœ¨: {os.path.exists(os.path.dirname(output_file)) if output_file else 'æœªçŸ¥'}")
            elif result.returncode == 1:
                logging.error("FFmpegé€šç”¨é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç¼–ç å‚æ•°é—®é¢˜")
            elif result.returncode < 0:
                logging.error(f"FFmpegè¢«ä¿¡å·ç»ˆæ­¢: {abs(result.returncode)}")
            
            return False
            
    except subprocess.TimeoutExpired:
        logging.error(f"â° FFmpegæ‰§è¡Œè¶…æ—¶ {video_name}: {timeout}ç§’")
        return False
    except Exception as e:
        logging.error(f"âŒ FFmpegæ‰§è¡Œå¼‚å¸¸ {video_name}: {e}")
        return False

def process_video_batch(video_paths: List[str], roi: Optional[Tuple[int, int, int, int]], 
                       hardware_info: Dict[str, Any], progress_manager: ProgressManager) -> Dict[str, Any]:
    """ğŸš€ æ‰¹é‡å¤„ç†è§†é¢‘ - 2025å¹´NASæé™ä¼˜åŒ–ç‰ˆ"""
    
    # ğŸš€ è·å–NASä¼˜åŒ–ç»„ä»¶
    cache_manager = hardware_info.get('cache_manager')
    i9_optimizer = hardware_info.get('i9_optimizer')
    
    # ğŸ’¡ å¯åŠ¨æ™ºèƒ½é¢„åŠ è½½ - ğŸ”§ æ·»åŠ ç·Šæ€¥ä¿®å¾©æª¢æŸ¥
    if cache_manager and ENABLE_CACHE and PRELOAD_COUNT > 0:
        logging.info(f"ğŸš€ å¯åŠ¨æ™ºèƒ½é¢„åŠ è½½ç³»ç»Ÿ...")
        cache_manager.preload_videos(video_paths, 0)
        
        # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
        cache_stats = cache_manager.get_cache_stats()
        logging.info(f"ğŸ“Š ç¼“å­˜çŠ¶æ€: {cache_stats['total_entries']}æ¡ç›®, "
                    f"{cache_stats['total_size_gb']:.1f}GB, "
                    f"å‘½ä¸­ç‡{cache_stats['cache_hit_ratio']:.1f}%")
    else:
        logging.info(f"âš ï¸ ç·©å­˜åŠŸèƒ½å·²ç¦ç”¨ - ç›´æ¥è™•ç†è¦–é »æ–‡ä»¶")
    
    # åˆ›å»ºæ€»è¿›åº¦æ¡
    # ğŸ”§ çº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ¡
    import threading
    progress_lock = threading.Lock()
    
    total_pbar = tqdm(
        total=len(video_paths), 
        desc="ğŸ“ æ€»ä½“è¿›åº¦", 
        position=0, 
        leave=True,
        bar_format='{l_bar}{bar:30}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
        smoothing=0.1,  # å‡å°‘è·³è·ƒ
        mininterval=0.5,  # æœ€å°æ›´æ–°é—´éš”
        maxinterval=2.0   # æœ€å¤§æ›´æ–°é—´éš”
    )
    
    success_count = 0
    failed_count = 0
    
    def update_main_progress():
        nonlocal success_count, failed_count
        with progress_lock:
            try:
                total_pbar.update(1)
                completed = success_count + failed_count
                success_rate = success_count / completed * 100 if completed > 0 else 0
                
                total_pbar.set_postfix({
                    'æˆåŠŸ': success_count,
                    'å¤±è´¥': failed_count,
                    'æˆåŠŸç‡': f"{success_rate:.1f}%"
                })
                
                # å¼ºåˆ¶åˆ·æ–°æ˜¾ç¤º
                total_pbar.refresh()
                
            except Exception as e:
                logging.debug(f"è¿›åº¦æ¡æ›´æ–°å¼‚å¸¸: {e}")
                pass

    def process_single_wrapper(video_info):
        """å¸¦æœ‰å†…å­˜ç®¡ç†çš„è§†é¢‘å¤„ç†åŒ…è£…å™¨"""
        video_path, video_idx = video_info
        
        try:
            # ç¡®ä¿è¾“å‡ºæ–‡ä»¶ä¸º.mp4æ ¼å¼ï¼ˆçº¯è§†é¢‘æ ¼å¼ï¼‰
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            output_path = os.path.join(OUTPUT_DIR, f"{base_name}.mp4")
            
            # ğŸš€ NASç¼“å­˜ä¼˜åŒ–ï¼šè·å–ç¼“å­˜è·¯å¾„æˆ–ç­‰å¾…ç¼“å­˜å®Œæˆ - ğŸ”§ ç·Šæ€¥ä¿®å¾©
            actual_video_path = video_path
            if cache_manager and ENABLE_CACHE:
                cached_path = cache_manager.get_cached_path(video_path)
                if cached_path:
                    actual_video_path = cached_path
                    logging.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜: {os.path.basename(video_path)}")
                else:
                    # ğŸ”§ ç·Šæ€¥ä¿®å¾©ï¼šç¸®çŸ­ç­‰å¾…æ™‚é–“ä¸¦å¢åŠ è¶…æ™‚è™•ç†
                    logging.info(f"â° ç¼“å­˜æœªå°±ç»ªï¼Œé™çº§åˆ°30ç§’å¿«é€Ÿç­‰å¾…: {os.path.basename(video_path)}")
                    cached_path = cache_manager.wait_for_cache(video_path, timeout=30)
                    if cached_path:
                        actual_video_path = cached_path
                    else:
                        logging.warning(f"âš ï¸ ç·©å­˜è¶…æ™‚ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è·¯å¾‘: {os.path.basename(video_path)}")
            else:
                logging.info(f"ğŸ”§ ç·©å­˜å·²ç¦ç”¨ï¼Œç›´æ¥è™•ç†: {os.path.basename(video_path)}")
            
            # ğŸ”„ é¢„åŠ è½½ä¸‹ä¸€ä¸ªè§†é¢‘ - ğŸ”§ ç·Šæ€¥ä¿®å¾©
            if cache_manager and ENABLE_CACHE and PRELOAD_COUNT > 0:
                cache_manager.preload_videos(video_paths, video_idx)
            
            # ğŸ”— è·å–è§†é¢‘ä¿¡æ¯ç”¨äºæ•°æ®åº“è®°å½•
            video_info = get_video_info(actual_video_path)
            
            # ğŸ” ç¼“å­˜åæŸ¥é‡æ£€æŸ¥ï¼ˆé¿å…æµªè´¹å¤„ç†èµ„æºï¼‰- ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è·¯å¾„è®¡ç®—å“ˆå¸Œ
            if progress_manager.video_record_manager and progress_manager.video_record_manager.db_manager.is_available():
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è§†é¢‘è·¯å¾„è®¡ç®—SHA256ï¼Œç¡®ä¿æ•°æ®åº“ä¸€è‡´æ€§
                hash_value = progress_manager.video_record_manager.calculate_video_sha256(video_path, quick_mode=False)
                if hash_value:
                    is_processed, db_record = progress_manager.video_record_manager.is_video_processed(video_path, hash_value)
                    if is_processed:
                        logging.info(f"ğŸš€ ç¼“å­˜åå‘ç°é‡å¤ï¼Œè·³è¿‡å¤„ç†: {os.path.basename(video_path)} "
                                   f"(å¤„ç†è€…: {db_record.get('computer_name', 'unknown')})")
                        
                        # æ¸…ç†ç¼“å­˜æ–‡ä»¶ï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰
                        if cache_manager and actual_video_path != video_path:
                            try:
                                cache_manager.remove_from_cache(video_path)
                                logging.debug(f"ğŸ—‘ï¸ å·²æ¸…ç†é‡å¤æ–‡ä»¶ç¼“å­˜: {os.path.basename(video_path)}")
                            except Exception as e:
                                logging.debug(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
                        
                        # æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆæœ¬åœ°è®°å½•ï¼‰
                        progress_manager.mark_completed(video_path, db_record.get('output_path', ''), 0)
                        return True
                    
                    # æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¢«å…¶ä»–ç”µè„‘å¤„ç†
                    is_processing, processing_info = progress_manager.video_record_manager.is_video_processing(video_path, hash_value)
                    if is_processing:
                        logging.info(f"ğŸš€ ç¼“å­˜åå‘ç°æ­£åœ¨å¤„ç†ï¼Œè·³è¿‡: {os.path.basename(video_path)} "
                                   f"(å¤„ç†è€…: {processing_info.get('computer_name', 'unknown')})")
                        
                        # æ¸…ç†ç¼“å­˜æ–‡ä»¶
                        if cache_manager and actual_video_path != video_path:
                            try:
                                cache_manager.remove_from_cache(video_path)
                            except Exception as e:
                                logging.debug(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
                        
                        return False  # è·³è¿‡å¤„ç†
            
            # æ ‡è®°ä¸ºå¤„ç†ä¸­ï¼ˆä¼ é€’è§†é¢‘ä¿¡æ¯ï¼‰
            progress_manager.mark_processing(video_path, video_info)
            
            # å¤„ç†è§†é¢‘
            success, processing_time, error_msg = process_single_video(
                actual_video_path, output_path, roi, hardware_info, video_idx, len(video_paths)
            )
            
            if success:
                progress_manager.mark_completed(video_path, output_path, processing_time)
                return True
            else:
                progress_manager.mark_failed(video_path, error_msg or "å¤„ç†å¤±è´¥")
                return False
                
        except Exception as e:
            logging.error(f"è§†é¢‘å¤„ç†å¼‚å¸¸ {video_path}: {e}")
            try:
                progress_manager.mark_failed(video_path, str(e))
            except:
                pass
            return False
        finally:
            # ğŸ§¹ å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
            try:
                import gc
                gc.collect()
                
                # æ¸…ç†å±€éƒ¨å˜é‡
                locals().clear()
                
                # å¦‚æœæ˜¯è¿›ç¨‹æ± ï¼Œé¢å¤–æ¸…ç†
                if hasattr(os, 'getpid'):
                    import psutil
                    process = psutil.Process()
                    # é™åˆ¶å†…å­˜ä½¿ç”¨
                    if process.memory_info().rss > 2 * 1024 * 1024 * 1024:  # è¶…è¿‡2GB
                        logging.warning(f"å·¥ä½œè¿›ç¨‹å†…å­˜ä½¿ç”¨è¿‡é«˜: {process.memory_info().rss / 1024 / 1024:.1f}MB")
                        gc.collect()
            except Exception as cleanup_error:
                logging.debug(f"å†…å­˜æ¸…ç†å¼‚å¸¸: {cleanup_error}")
                pass

    # ğŸš€ 2025å¹´æé™å¹¶è¡Œä¼˜åŒ–
    if i9_optimizer:
        max_workers = i9_optimizer.get_optimal_worker_count('encoding')
    else:
        max_workers = hardware_info.get("max_parallel", 4)
        
    # ç¡¬ä»¶ç¼–ç å™¨é™åˆ¶è°ƒæ•´
    if hardware_info["encoder_type"] != "software":
        max_workers = min(max_workers, 8)  # 2025å¹´ç¡¬ä»¶ç¼–ç å™¨æ›´å¼º
    else:
        cpu_cores = hardware_info.get("cpu_cores", 8)
        if hardware_info.get('is_i9', False):
            max_workers = min(max_workers, min(cpu_cores - 2, 24))  # i9æé™é…ç½®
        else:
            max_workers = min(max_workers, cpu_cores // 2)
    
    logging.info(f"ğŸš€ 2025å¹´æé™é…ç½®: {hardware_info['encoder_type']}, å¹¶è¡Œæ•°: {max_workers}, "
                f"i9ä¼˜åŒ–: {hardware_info.get('is_i9', False)}")
    
    # é€‰æ‹©æ‰§è¡Œå™¨ç±»å‹
    executor_class = concurrent.futures.ThreadPoolExecutor if hardware_info["encoder_type"] != "software" else concurrent.futures.ProcessPoolExecutor
    
    with executor_class(max_workers=max_workers) as executor:
        # æ¸…å±å¹¶å¼€å§‹å¤„ç†
        print("\033[2J\033[H", end="")
        
        video_info_list = [(path, idx) for idx, path in enumerate(video_paths)]
        futures = [executor.submit(process_single_wrapper, info) for info in video_info_list]
        
        # ç­‰å¾…å®Œæˆå¹¶æ›´æ–°è¿›åº¦
        processed_count = 0
        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
                if result:
                    success_count += 1
                else:
                    failed_count += 1
            except Exception as e:
                failed_count += 1
                logging.error(f"ä»»åŠ¡å¼‚å¸¸: {e}")
            
            processed_count += 1
            update_main_progress()
            
            # ğŸ§¹ å®šæœŸå†…å­˜æ¸…ç† - æ¯å¤„ç†10ä¸ªè§†é¢‘æ¸…ç†ä¸€æ¬¡
            if processed_count % 10 == 0:
                try:
                    import gc
                    gc.collect()
                    
                    # è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
                    import psutil
                    process = psutil.Process()
                    memory_mb = process.memory_info().rss / 1024 / 1024
                    logging.info(f"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨: {memory_mb:.1f}MB")
                    
                    # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå¼ºåˆ¶æ¸…ç†
                    if memory_mb > 4096:  # è¶…è¿‡4GB
                        logging.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_mb:.1f}MBï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†")
                        gc.collect()
                        gc.collect()  # åŒé‡æ¸…ç†
                        
                except Exception as cleanup_error:
                    logging.debug(f"å†…å­˜æ¸…ç†å¼‚å¸¸: {cleanup_error}")
                    pass
    
    total_pbar.close()
    
    return {
        'success_count': success_count,
        'failed_count': failed_count,
        'total_count': len(video_paths)
    }

def check_database_table_with_retry(max_retries=None, retry_interval=5):
    """æ£€æŸ¥æ•°æ®åº“è¿æ¥ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    if not ENABLE_MULTI_COMPUTER_SYNC:
        return True
    
    retry_count = 0
    while True:
        try:
            print(f"ğŸ”— æ­£åœ¨å°è¯•è¿æ¥æ•°æ®åº“... (ç¬¬{retry_count + 1}æ¬¡)")
            db_manager = DatabaseManager()
            
            if not db_manager.is_available():
                raise Exception("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
                
            with db_manager.get_connection() as connection:
                if not connection or not connection.is_connected():
                    raise Exception("æ•°æ®åº“è¿æ¥å¤±è´¥")
                
                logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
                print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
                return True
                    
        except Exception as e:
            retry_count += 1
            logger.warning(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ (ç¬¬{retry_count}æ¬¡): {e}")
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ (ç¬¬{retry_count}æ¬¡): {e}")
            
            if max_retries and retry_count >= max_retries:
                logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                return False
            
            print(f"â³ {retry_interval}ç§’åé‡è¯•...")
            time.sleep(retry_interval)

def check_database_table():
    """æ£€æŸ¥æ•°æ®åº“è¿æ¥ï¼ˆå‡è®¾è¡¨å·²å­˜åœ¨ï¼‰"""
    if not ENABLE_MULTI_COMPUTER_SYNC:
        return True
        
    try:
        db_manager = DatabaseManager()
        if not db_manager.is_available():
            logger.warning("æ•°æ®åº“ä¸å¯ç”¨ï¼Œå°†ä»¥å•æœºæ¨¡å¼è¿è¡Œ")
            return False
            
        with db_manager.get_connection() as connection:
            if not connection or not connection.is_connected():
                logger.warning("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå°†ä»¥å•æœºæ¨¡å¼è¿è¡Œ")
                return False
            
            logger.info("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
            return True
                
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
        print(f"âŒ æ•°æ®åº“è¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
        return False

def setup_logging():
    """è®¾ç½®æ—¥å¿—å¹¶è¿”å›æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    log_dir = 'logs'
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = f"unified_video_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    log_file_path = os.path.join(log_dir, log_filename)
    
    # è·å–æ—¥å¿—æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    abs_log_path = os.path.abspath(log_file_path)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file_path, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # å‡å°‘ç¬¬ä¸‰æ–¹åº“æ—¥å¿—
    logging.getLogger('PIL').setLevel(logging.WARNING)
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    
    # è®°å½•æ—¥å¿—æ–‡ä»¶åœ°å€
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶åœ°å€: {abs_log_path}")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶åœ°å€: {abs_log_path}")
    
    return abs_log_path

def main():
    """ğŸš€ ä¸»å‡½æ•° - 2025å¹´NASæé™ä¼˜åŒ–ç‰ˆ"""
    print("ğŸ¬ MC_Lçš„ç»ˆææˆ˜æ–—ä»ªè§†é¢‘å¤„ç†å™¨ - NASæé™ä¼˜åŒ–ç‰ˆ")
    print("ğŸš€ 2025å¹´MC_Lçš„ä¼˜åŒ–ç­–ç•¥:")
    print("   ğŸ’¾ æ™ºèƒ½æœ¬åœ°ç¼“å­˜ç³»ç»Ÿ (500TB+ NASä¼˜åŒ–)")
    print("   âš¡ i9å¤„ç†å™¨æè‡´æ€§èƒ½è°ƒä¼˜")
    print("   ğŸŒ å¼‚æ­¥é¢„è¯»å–å’Œæ–­ç‚¹ç»­ä¼ ")
    print("   ğŸ§  å†…å­˜å’Œå­˜å‚¨æ™ºèƒ½ç®¡ç†")
    print("=" * 60)
    
    # è®¾ç½®æ—¥å¿—å¹¶è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file_path = setup_logging()
    
    # ğŸ”— æ£€æŸ¥æ•°æ®åº“è¡¨ï¼ˆå¤šç”µè„‘åä½œï¼‰
    if ENABLE_MULTI_COMPUTER_SYNC:
        print("ğŸ”— æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œè¡¨ç»“æ„...")
        print("ğŸ’¡ æ•°æ®åº“è¿æ¥å¤±è´¥æ—¶å°†è‡ªåŠ¨é‡è¯•ï¼Œç›´åˆ°è¿æ¥æˆåŠŸ...")
        
        # ä½¿ç”¨å¸¦é‡è¯•çš„æ•°æ®åº“è¿æ¥æ£€æŸ¥
        if not check_database_table_with_retry():
            print("âš ï¸ æ•°æ®åº“è¿æ¥é‡è¯•å¤±è´¥ï¼Œå°†ä»¥å•æœºæ¨¡å¼ç»§ç»­è¿è¡Œ")
            # å¯ä»¥é€‰æ‹©ç»§ç»­æˆ–é€€å‡º
            choice = input("æ˜¯å¦ç»§ç»­ä»¥å•æœºæ¨¡å¼è¿è¡Œ? (y/nï¼Œå›è½¦é»˜è®¤æ˜¯): ").strip().lower()
            if choice in ['n', 'no']:
                print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                return
    
    # éªŒè¯é…ç½®
    print("ğŸ” éªŒè¯é…ç½®...")
    if not os.path.exists(FFMPEG_PATH):
        print(f"âŒ FFmpegè·¯å¾„ä¸å­˜åœ¨: {FFMPEG_PATH}")
        return
    else:
        print(f"âœ… FFmpegå·²æ‰¾åˆ°: {FFMPEG_PATH}")
    
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_DIR}")
        return
    else:
        print(f"âœ… è¾“å…¥ç›®å½•: {INPUT_DIR}")
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    else:
        print(f"âœ… è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    if LOCAL_CACHE_DIR and not os.path.exists(LOCAL_CACHE_DIR):
        os.makedirs(LOCAL_CACHE_DIR)
        print(f"âœ… åˆ›å»ºç¼“å­˜ç›®å½•: {LOCAL_CACHE_DIR}")
    elif LOCAL_CACHE_DIR:
        print(f"âœ… ç¼“å­˜ç›®å½•: {LOCAL_CACHE_DIR}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(PROGRESS_FOLDER, exist_ok=True)
    
    print("âœ… é…ç½®éªŒè¯é€šè¿‡")
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    print("\nğŸ“‹ å½“å‰é…ç½®:")
    print(f"  è¾“å…¥ç›®å½•: {INPUT_DIR}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  åˆ‡å¤´å°¾åŠŸèƒ½: {'å¯ç”¨' if ENABLE_HEAD_TAIL_CUT else 'ç¦ç”¨'}")
    if ENABLE_HEAD_TAIL_CUT:
        print(f"    ç‰‡å¤´æ—¶é—´: {HEAD_CUT_TIME}ç§’")
        print(f"    ç‰‡å°¾æ—¶é—´: {TAIL_CUT_TIME}ç§’")
    print(f"  è£å‰ªåŠŸèƒ½: {'å¯ç”¨' if ENABLE_CROPPING else 'ç¦ç”¨'}")
    if ENABLE_CROPPING:
        print(f"    ç›®æ ‡åˆ†è¾¨ç‡: {TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}")
    
    print("\nğŸš€ NASæé™ä¼˜åŒ–é…ç½®:")
    print(f"  æœ¬åœ°ç¼“å­˜ç›®å½•: {LOCAL_CACHE_DIR}")
    print(f"  æœ€å¤§ç¼“å­˜å¤§å°: {MAX_CACHE_SIZE_GB}GB")
    print(f"  é¢„åŠ è½½è§†é¢‘æ•°: {PRELOAD_COUNT}ä¸ª")
    print(f"  å¼‚æ­¥ä¸‹è½½çº¿ç¨‹: {ASYNC_DOWNLOAD_THREADS}ä¸ª")
    print(f"  i9ç¿é¢‘ä¼˜åŒ–: {'å¯ç”¨' if ENABLE_I9_TURBO else 'ç¦ç”¨'}")
    print(f"  CPUäº²å’Œæ€§ä¼˜åŒ–: {'å¯ç”¨' if CPU_AFFINITY_OPTIMIZATION else 'ç¦ç”¨'}")
    print(f"  å†…å­˜æ± å¤§å°: {MEMORY_POOL_SIZE_GB}GB")
    print(f"  ä¸´æ—¶å¤„ç†ç›®å½•: {TEMP_PROCESSING_DIR}")
    
    # åˆå§‹åŒ–ç¡¬ä»¶æ£€æµ‹å™¨ - é˜²æ­»é”åŠ å›ºç‰ˆ
    print("\nğŸ”§ æ£€æµ‹ç¡¬ä»¶é…ç½®...")
    try:
        # ä½¿ç”¨è¶…æ—¶ä¿æŠ¤æ¥é¿å…ç¡¬ä»¶æ£€æµ‹æ­»é”
        import threading
        hardware_detector = None
        hardware_info = None
        
        def detect_hardware_thread():
            nonlocal hardware_detector, hardware_info
            try:
                hardware_detector = HardwareDetector(log_file_path)
                hardware_info = hardware_detector.detect_hardware_capabilities()
                logging.info("âœ… ç¡¬ä»¶æ£€æµ‹å®Œæˆ")
            except Exception as e:
                logging.error(f"ç¡¬ä»¶æ£€æµ‹çº¿ç¨‹å¼‚å¸¸: {e}")
        
        # å¯åŠ¨æ£€æµ‹çº¿ç¨‹ï¼Œè®¾ç½®60ç§’è¶…æ—¶
        detect_thread = threading.Thread(target=detect_hardware_thread, daemon=True)
        detect_thread.start()
        detect_thread.join(timeout=60)
        
        if detect_thread.is_alive():
            print("âš ï¸  ç¡¬ä»¶æ£€æµ‹è¶…æ—¶ï¼Œä½¿ç”¨åŸºç¡€é…ç½®ç»§ç»­è¿è¡Œ")
            logging.warning("ç¡¬ä»¶æ£€æµ‹è¶…æ—¶ï¼Œå›é€€åˆ°åŸºç¡€ç¡¬ä»¶æ£€æµ‹")
            # åˆ›å»ºåŸºç¡€é…ç½®ï¼Œé¿å…å¡åœ¨HardwareDetectoråˆå§‹åŒ–
            hardware_detector = None
            hardware_info = {
                'encoder_type': 'software',
                'encoder': 'libx264', 
                'cpu_cores': multiprocessing.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / (1024**3),
                'is_i9': multiprocessing.cpu_count() >= 16,
                'max_parallel': min(multiprocessing.cpu_count() // 2, 8),
                'numa_nodes': 1,
                'options': {},
                'computer_name': f"fallback_{socket.gethostname()}"
            }
        elif hardware_info is None:
            print("âš ï¸  ç¡¬ä»¶æ£€æµ‹å¼‚å¸¸ï¼Œä½¿ç”¨åŸºç¡€é…ç½®ç»§ç»­è¿è¡Œ")
            logging.warning("ç¡¬ä»¶æ£€æµ‹å¼‚å¸¸ï¼Œå›é€€åˆ°åŸºç¡€ç¡¬ä»¶æ£€æµ‹")
            hardware_detector = None
            hardware_info = {
                'encoder_type': 'software',
                'encoder': 'libx264',
                'cpu_cores': multiprocessing.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / (1024**3),
                'is_i9': multiprocessing.cpu_count() >= 16,
                'max_parallel': min(multiprocessing.cpu_count() // 2, 8),
                'numa_nodes': 1,
                'options': {},
                'computer_name': f"fallback_{socket.gethostname()}"
            }
            
    except Exception as e:
        print(f"âš ï¸  ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")
        logging.error(f"ç¡¬ä»¶æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        # ç¡®ä¿ç¨‹åºèƒ½ç»§ç»­è¿è¡Œçš„æœ€å°é…ç½®
        hardware_detector = None
        hardware_info = {
            'encoder_type': 'software',
            'encoder': 'libx264',
            'cpu_cores': multiprocessing.cpu_count(),
            'memory_gb': 8.0,
            'is_i9': False,
            'max_parallel': 4,
            'numa_nodes': 1,
            'options': {},
                'computer_name': f"error_{socket.gethostname()}"
        }
    
    print(f"âœ… 2025å¹´æé™ç¡¬ä»¶æ£€æµ‹å®Œæˆ:")
    print(f"  ç¼–ç å™¨ç±»å‹: {hardware_info['encoder_type']}")
    print(f"  ç¼–ç å™¨: {hardware_info['encoder']}")
    print(f"  CPUæ ¸å¿ƒæ•°: {hardware_info.get('cpu_cores', 'N/A')} (i9: {hardware_info.get('is_i9', False)})")
    print(f"  å†…å­˜: {hardware_info.get('memory_gb', 0):.1f}GB (æ± : {hardware_info.get('memory_pool_gb', 0)}GB)")
    print(f"  æœ€å¤§å¹¶è¡Œæ•°: {hardware_info.get('max_parallel', 'N/A')}")
    print(f"  NUMAèŠ‚ç‚¹: {hardware_info.get('numa_nodes', 'N/A')}")
    
    # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
    cache_stats = hardware_info.get('cache_stats', {})
    print(f"  ç¼“å­˜çŠ¶æ€: {cache_stats.get('total_entries', 0)}æ¡ç›® ({cache_stats.get('total_size_gb', 0):.1f}GB)")
    print(f"  ç½‘ç»œæ€§èƒ½: å¹³å‡{cache_stats.get('avg_network_speed_mbps', 0):.1f}MB/s, å³°å€¼{cache_stats.get('peak_network_speed_mbps', 0):.1f}MB/s")
    
    # è·å–è®¡ç®—æœºåç§°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¤„ç†ç¡¬ä»¶æ£€æµ‹å™¨ä¸ºNoneçš„æƒ…å†µï¼‰
    computer_name = COMPUTER_NAME if COMPUTER_NAME else hardware_info.get('computer_name', f"fallback_{socket.gethostname()}")
    
    # åˆå§‹åŒ–å¢å¼ºçš„æ•°æ®åº“ç³»ç»Ÿç®¡ç†å™¨
    db_system_manager = None
    if ENABLE_MULTI_COMPUTER_SYNC:
        print("\nğŸ”§ åˆå§‹åŒ–æ•°æ®åº“ç³»ç»Ÿ...")
        db_system_manager = DatabaseSystemManager(computer_name, log_file_path)
        if db_system_manager.initialize_system():
            print("âœ… æ•°æ®åº“ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            # æ›¿æ¢ç¡¬ä»¶ä¿¡æ¯ä¸­çš„video_record_manager
            hardware_info['video_record_manager'] = db_system_manager.record_manager
        else:
            print("âš ï¸ æ•°æ®åº“ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä»¥å•æœºæ¨¡å¼è¿è¡Œ")
            db_system_manager = None
    
    # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨ï¼ˆé›†æˆå¤šç”µè„‘åä½œï¼‰
    video_record_manager = hardware_info.get('video_record_manager')
    progress_manager = ProgressManager(computer_name, video_record_manager)
    
    # ğŸ”§ é…ç½®æ™ºèƒ½æŸ¥é‡ç³»ç»Ÿ
    if video_record_manager:
        # è®¾ç½®é‡å¤è§†é¢‘è¾“å‡ºè·¯å¾„ï¼ˆå¯é…ç½®ï¼‰
        duplicate_output_path = os.path.join(os.path.dirname(OUTPUT_DIR), "é‡å¤è§†é¢‘")
        video_record_manager.set_duplicate_output_path(duplicate_output_path)
        print(f"âœ… é‡å¤è§†é¢‘å°†ç§»åŠ¨åˆ°: {duplicate_output_path}")
        
        # åˆ›å»ºé‡å¤è§†é¢‘è¾“å‡ºç›®å½•
        if not os.path.exists(duplicate_output_path):
            os.makedirs(duplicate_output_path, exist_ok=True)
    
    # ğŸ”— æ˜¾ç¤ºå¤šç”µè„‘åä½œçŠ¶æ€
    if db_system_manager and db_system_manager.db_manager.is_available():
        print(f"ğŸ”— å¤šç”µè„‘åä½œ: å·²å¯ç”¨ (å¢å¼ºç‰ˆ)")
        print(f"   æ•°æ®åº“: {MYSQL_CONFIG['host']}:{MYSQL_CONFIG.get('port', 3306)}")
        print(f"   ç”µè„‘åç§°: {hardware_info.get('computer_name', 'unknown')}")
        print(f"   å¤„ç†äºº: {PROCESSOR_NAME}")
        
        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€æŠ¥å‘Š
        try:
            status_report = db_system_manager.get_system_status_report()
            basic_health = status_report.get('basic_health', False)
            performance_metrics = status_report.get('performance_metrics', {})
            
            print(f"   ç³»ç»Ÿå¥åº·: {'âœ… æ­£å¸¸' if basic_health else 'âš ï¸ å¼‚å¸¸'}")
            if performance_metrics:
                print(f"   æ•°æ®åº“è¿æ¥: {performance_metrics.get('active_connections', 'N/A')}")
                print(f"   ç¼“å†²æ± ä½¿ç”¨ç‡: {performance_metrics.get('buffer_pool_usage_percent', 'N/A')}%")
            
            # æ˜¾ç¤ºåä½œç»Ÿè®¡
            table_stats = status_report.get('table_statistics', {})
            if table_stats:
                total_records = table_stats.get('total_records', 0)
                success_count = table_stats.get('successful_processes', 0)
                today_count = table_stats.get('today_processes', 0)
                print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æ€»è®¡{total_records}, æˆåŠŸ{success_count}, ä»Šæ—¥{today_count}")
                
                # æ˜¾ç¤ºå„ç”µè„‘ç»Ÿè®¡
                computer_stats = table_stats.get('computer_statistics', [])
                if computer_stats:
                    print(f"   åä½œç”µè„‘ï¼ˆå‰5å°ï¼‰:")
                    for computer in computer_stats[:5]:
                        print(f"     {computer['computer_name']}: {computer['success_count']}/{computer['total_count']} "
                              f"({computer['success_rate']}%)")
                        
            # æ˜¾ç¤ºå»ºè®®
            recommendations = status_report.get('recommendations', [])
            if recommendations:
                print(f"ğŸ’¡ ç³»ç»Ÿå»ºè®®:")
                for rec in recommendations[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                    print(f"   - {rec}")
                    
        except Exception as e:
            logger.debug(f"è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Šå¤±è´¥: {e}")
            # å›é€€åˆ°åŸºæœ¬ç»Ÿè®¡æ˜¾ç¤º
            try:
                stats = video_record_manager.get_processing_statistics()
                if stats.get('computer_stats'):
                    print(f"ğŸ“Š åä½œç»Ÿè®¡ï¼ˆè¿‘7å¤©ï¼‰:")
                    for computer in stats['computer_stats'][:5]:
                        success_rate = computer['completed'] / computer['total'] * 100 if computer['total'] > 0 else 0
                        print(f"   {computer['computer_name']}: {computer['completed']}/{computer['total']} "
                              f"({success_rate:.1f}%)")
            except Exception as e2:
                logger.debug(f"è·å–åŸºæœ¬ç»Ÿè®¡ä¹Ÿå¤±è´¥: {e2}")
    else:
        print(f"âš ï¸  å¤šç”µè„‘åä½œ: å·²ç¦ç”¨ï¼ˆå•æœºæ¨¡å¼ï¼‰")
    
    # æ£€æŸ¥å´©æºƒæ¢å¤
    print("ğŸ”„ æ£€æŸ¥æ˜¯å¦éœ€è¦å´©æºƒæ¢å¤...")
    recovery_data = progress_manager.check_crash_recovery()
    
    should_resume = False
    resume_roi = None
    
    if recovery_data:
        print(f"ğŸ”¥ æ£€æµ‹åˆ°ä¸Šæ¬¡ä¼šè¯å¼‚å¸¸ç»ˆæ­¢ ({recovery_data.get('last_session_time', 'unknown')})")
        
        # æ£€æµ‹éƒ¨åˆ†è¾“å‡ºæ–‡ä»¶
        partial_files = progress_manager.detect_partial_outputs()
        if partial_files:
            print(f"âš ï¸  å‘ç° {len(partial_files)} ä¸ªå¯èƒ½çš„éƒ¨åˆ†å¤„ç†æ–‡ä»¶:")
            for pf in partial_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                size_mb = pf['size'] / (1024*1024)
                print(f"   - {pf['name']} ({size_mb:.1f}MB)")
            if len(partial_files) > 5:
                print(f"   ... è¿˜æœ‰ {len(partial_files)-5} ä¸ªæ–‡ä»¶")
                
        # è¯¢é—®æ˜¯å¦ç»§ç»­å¤„ç†
        print("\nğŸ¤” å‘ç°ä¸Šæ¬¡å¤„ç†å¯èƒ½è¢«ä¸­æ–­ï¼Œæ‚¨å¸Œæœ›:")
        print("   [1] ç»§ç»­æ–­ç‚¹ç»­ä¼  (æ¨è)")
        print("   [2] é‡æ–°å¼€å§‹å¤„ç† (æ¸…ç†éƒ¨åˆ†æ–‡ä»¶)")
        print("   [3] é€€å‡ºç¨‹åº")
        
        while True:
            try:
                choice = input("\nè¯·é€‰æ‹© (1-3ï¼Œå›è½¦é»˜è®¤é€‰1): ").strip()
                if choice == '' or choice == '1':
                    should_resume = True
                    resume_roi = recovery_data.get('roi_settings')
                    if resume_roi:
                        resume_roi = tuple(resume_roi)
                    print("âœ… é€‰æ‹©æ–­ç‚¹ç»­ä¼ æ¨¡å¼")
                    break
                elif choice == '2':
                    print("ğŸ§¹ æ¸…ç†éƒ¨åˆ†æ–‡ä»¶å¹¶é‡æ–°å¼€å§‹...")
                    for pf in partial_files:
                        try:
                            os.remove(pf['path'])
                            print(f"   åˆ é™¤: {pf['name']}")
                        except Exception as e:
                            print(f"   åˆ é™¤å¤±è´¥ {pf['name']}: {e}")
                    progress_manager.clear_crash_recovery()
                    should_resume = False
                    print("âœ… æ¸…ç†å®Œæˆï¼Œé‡æ–°å¼€å§‹å¤„ç†")
                    break
                elif choice == '3':
                    print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                    return
                else:
                    print("âŒ è¯·è¾“å…¥ 1ã€2 æˆ– 3")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return
    else:
        progress_manager.clear_crash_recovery()
    
    # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶ï¼ˆå¿«é€Ÿæ‰«æï¼Œä¸åšæ·±åº¦æ£€æŸ¥ï¼‰
    print(f"\nğŸ“‚ æ‰«æè§†é¢‘æ–‡ä»¶: {INPUT_DIR}")
    video_paths = find_video_files(INPUT_DIR)
    
    if not video_paths:
        print("âŒ æœªæ‰¾åˆ°æ”¯æŒçš„è§†é¢‘æ–‡ä»¶")
        return
    else:
        print(f"âœ… æ‰¾åˆ° {len(video_paths)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # ROIé€‰æ‹©ï¼ˆä»…åœ¨å¯ç”¨è£å‰ªåŠŸèƒ½æ—¶ï¼‰
    roi = None
    if ENABLE_CROPPING:
        print(f"\nğŸ¯ ROIåŒºåŸŸé€‰æ‹©...")
        
        # ä¼˜å…ˆä½¿ç”¨æ–­ç‚¹ç»­ä¼ çš„ROIè®¾ç½®
        if should_resume and resume_roi:
            roi = resume_roi
            print(f"âœ… ä½¿ç”¨æ–­ç‚¹ç»­ä¼ çš„ROIè®¾ç½®: {roi}")
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„ROIè®¾ç½®
            saved_roi = progress_manager.progress_data.get('roi_settings')
            if saved_roi:
                print(f"å‘ç°ä¿å­˜çš„ROIè®¾ç½®: {saved_roi}")
                use_saved = input("ä½¿ç”¨ä¿å­˜çš„ROIè®¾ç½®? (y/nï¼Œå›è½¦é»˜è®¤æ˜¯): ").strip().lower()
                if use_saved in ['y', 'yes', '']:
                    roi = tuple(saved_roi)
                    print(f"âœ… ä½¿ç”¨ä¿å­˜çš„ROIè®¾ç½®: {roi}")
            
            if roi is None:
                print("ğŸ¯ å¼€å§‹16:9æ™ºèƒ½ROIé€‰æ‹©...")
                roi_selector = ROISelector()
                
                # é€‰æ‹©ç¬¬ä¸€ä¸ªè§†é¢‘ä½œä¸ºé¢„è§ˆ
                preview_video = video_paths[0]
                print(f"ğŸ“º ä½¿ç”¨è§†é¢‘è¿›è¡ŒROIé¢„è§ˆ: {os.path.basename(preview_video)}")
                
                roi = roi_selector.select_roi_for_video(preview_video)
                if roi is None:
                    print("âŒ ROIé€‰æ‹©å¤±è´¥")
                    return
                
                # ä¿å­˜ROIè®¾ç½®ï¼ˆåŒ…å«åŸºå‡†å°ºå¯¸ï¼‰
                progress_manager.progress_data['roi_settings'] = roi
                progress_manager.save_progress()
                
                if len(roi) == 6:
                    x, y, w, h, base_w, base_h = roi
                    print(f"âœ… ROIé€‰æ‹©å®Œæˆ: è£å‰ªåŒºåŸŸ({x},{y},{w},{h}) åŸºå‡†å°ºå¯¸({base_w}x{base_h})")
                else:
                    print(f"âœ… ROIé€‰æ‹©å®Œæˆ: {roi}")
        
        print(f"ğŸ“ ä½¿ç”¨ROIåŒºåŸŸ: {roi}")
    else:
        print("â„¹ï¸  è£å‰ªåŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡ROIé€‰æ‹©")
    
    print(f"\nğŸ’¡ ç®€åŒ–è¿‡æ»¤åŠŸèƒ½å·²å–æ¶ˆï¼Œå› ä¸ºç¼“å­˜åæœ‰æ•°æ®åº“æŸ¥é‡æ“ä½œ")
    print(f"ğŸ“ å…±æ‰¾åˆ° {len(video_paths)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°†ç›´æ¥è¿›å…¥ç¼“å­˜é˜¶æ®µ")
    
    if not video_paths:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶ï¼")
        return
    
    # ğŸ“‹ ROIé€‰æ‹©å®Œæˆåï¼Œç›´æ¥è¯»å–è¿›åº¦æ–‡ä»¶è·³è¿‡å·²å®Œæˆè§†é¢‘
    print(f"\nğŸ“‹ æ­£åœ¨è¯»å–è¿›åº¦æ–‡ä»¶æ£€æŸ¥å·²å®Œæˆçš„è§†é¢‘...")
    filtered_video_paths = []
    skipped_count = 0
    
    # éå†æ‰€æœ‰è§†é¢‘ï¼Œä½¿ç”¨è¿›åº¦ç®¡ç†å™¨çš„å¿«é€Ÿæ£€æŸ¥æ¨¡å¼
    for video_path in video_paths:
        video_name = os.path.basename(video_path)
        
        # ä½¿ç”¨è¿›åº¦ç®¡ç†å™¨çš„å¿«é€Ÿæ£€æŸ¥æ¨¡å¼ï¼ˆæ£€æŸ¥è¾“å‡ºæ–‡ä»¶ã€æœ¬åœ°è®°å½•ã€æ—§ç‰ˆæœ¬è®°å½•ï¼‰
        if progress_manager.is_completed(video_path, quick_check=True):
            print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {video_name} (è¿›åº¦æ–‡ä»¶è®°å½•)")
            skipped_count += 1
        else:
            filtered_video_paths.append(video_path)
    
    print(f"ğŸ“Š è¿›åº¦æ–‡ä»¶è¿‡æ»¤ç»“æœ:")
    print(f"   åŸå§‹è§†é¢‘æ•°é‡: {len(video_paths)}")
    print(f"   å·²å®Œæˆè·³è¿‡: {skipped_count}")
    print(f"   å¾…å¤„ç†æ•°é‡: {len(filtered_video_paths)}")
    
    # ä½¿ç”¨è¿‡æ»¤åçš„è§†é¢‘åˆ—è¡¨
    video_paths = filtered_video_paths
    
    if not video_paths:
        print("âœ… æ‰€æœ‰è§†é¢‘éƒ½å·²å®Œæˆå¤„ç†ï¼")
        return
    
    # å¼€å§‹æ‰¹é‡å¤„ç†
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†...")
    print(f"å¤„ç†æ¨¡å¼: ", end="")
    if ENABLE_HEAD_TAIL_CUT and ENABLE_CROPPING:
        print("å®Œæ•´å¤„ç† (åˆ‡å¤´å°¾ + è£å‰ª)")
    elif ENABLE_HEAD_TAIL_CUT:
        print("ä»…åˆ‡å¤´å°¾")
    elif ENABLE_CROPPING:
        print("ä»…è£å‰ª")
    else:
        print("âŒ æœªå¯ç”¨ä»»ä½•å¤„ç†åŠŸèƒ½")
        return
    
    start_time = time.time()
    
    # ä¿å­˜å´©æºƒæ¢å¤ä¿¡æ¯
    if video_paths:
        progress_manager.save_crash_recovery(video_paths[0], roi)
    
    try:
        # ğŸš€ ä½¿ç”¨æ–°çš„æµæ°´çº¿ç®¡ç†å™¨å¤„ç†
        results = process_video_batch_with_pipeline(video_paths, roi, hardware_info, progress_manager)
        
        # ğŸ”„ æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åº“æ’å…¥å¤±è´¥çš„è§†é¢‘éœ€è¦é‡è¯•
        if results['failed_count'] > 0 and ENABLE_MULTI_COMPUTER_SYNC:
            print(f"\nğŸ”„ æ£€æµ‹åˆ° {results['failed_count']} ä¸ªå¤„ç†å¤±è´¥çš„è§†é¢‘ï¼Œå¼€å§‹é‡è¯•æ•°æ®åº“æ’å…¥...")
            
            max_retry_rounds = 3  # æœ€å¤šé‡è¯•3è½®
            retry_delay = 60  # æ¯è½®é‡è¯•é—´éš”60ç§’
            
            for retry_round in range(max_retry_rounds):
                print(f"\nğŸ”„ ç¬¬ {retry_round + 1}/{max_retry_rounds} è½®é‡è¯•...")
                
                # é‡æ–°æ‰«æå¤±è´¥çš„è§†é¢‘ï¼Œå°è¯•æ’å…¥æ•°æ®åº“
                failed_videos = []
                for video_path in video_paths:
                    if progress_manager.video_record_manager:
                        # æ£€æŸ¥è§†é¢‘æ˜¯å¦åœ¨æ•°æ®åº“ä¸­
                        video_name = os.path.basename(video_path)
                        try:
                            video_info = progress_manager.video_record_manager.get_video_info_fast(video_path)
                            if video_info:
                                hash_value = progress_manager.video_record_manager.calculate_video_sha256(video_path, quick_mode=True)
                                success = progress_manager.video_record_manager.insert_video_record(
                                    video_path, video_name, hash_value, video_info
                                )
                                if not success:
                                    failed_videos.append(video_path)
                                else:
                                    print(f"âœ… é‡è¯•æˆåŠŸæ’å…¥: {video_name}")
                        except Exception as e:
                            logger.warning(f"é‡è¯•æ£€æŸ¥è§†é¢‘å¤±è´¥ {video_name}: {e}")
                            failed_videos.append(video_path)
                
                if not failed_videos:
                    print("âœ… æ‰€æœ‰è§†é¢‘æ•°æ®åº“æ’å…¥é‡è¯•æˆåŠŸï¼")
                    break
                elif retry_round < max_retry_rounds - 1:
                    print(f"â³ ä»æœ‰ {len(failed_videos)} ä¸ªè§†é¢‘æ’å…¥å¤±è´¥ï¼Œ{retry_delay}ç§’åè¿›è¡Œä¸‹ä¸€è½®é‡è¯•...")
                    time.sleep(retry_delay)
                else:
                    print(f"âŒ ç»è¿‡ {max_retry_rounds} è½®é‡è¯•ï¼Œä»æœ‰ {len(failed_videos)} ä¸ªè§†é¢‘æ— æ³•æ’å…¥æ•°æ®åº“")
        
        elapsed_time = time.time() - start_time
        
        # ğŸš€ æ˜¾ç¤º2025å¹´æé™ä¼˜åŒ–æœ€ç»ˆç»Ÿè®¡
        cache_manager = hardware_info.get('cache_manager')
        final_cache_stats = cache_manager.get_cache_stats() if cache_manager else {}
        
        print(f"\nğŸ¬ NASæé™ä¼˜åŒ–å¤„ç†å®Œæˆ! ğŸš€ğŸ“Š")
        print("=" * 50)
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸå¤„ç†:    {results['success_count']} ä¸ª ({results['success_count']/results['total_count']*100:.1f}%)")
        print(f"   âŒ å¤„ç†å¤±è´¥:     {results['failed_count']} ä¸ª")
        print(f"   ğŸ“ æ€»è®¡æ–‡ä»¶:   {results['total_count']} ä¸ª")
        print("-" * 50)
        print(f"âš¡ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   ğŸ• æ‰¹é‡æ€»è€—æ—¶: {elapsed_time:.1f} ç§’")
        if results['success_count'] > 0:
            print(f"   ğŸ“Š å¹³å‡å¤„ç†é€Ÿåº¦: {elapsed_time/results['success_count']:.1f} ç§’/ä¸ª")
        print("-" * 50)
        print(f"ğŸš€ 2025å¹´NASæé™ä¼˜åŒ–ç»Ÿè®¡:")
        print(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {final_cache_stats.get('cache_hit_ratio', 0):.1f}%")
        print(f"   ğŸ“¥ ç¼“å­˜æ€»å¤§å°: {final_cache_stats.get('total_size_gb', 0):.1f}GB")
        print(f"   ğŸŒ å¹³å‡ç½‘é€Ÿ: {final_cache_stats.get('avg_network_speed_mbps', 0):.1f}MB/s")
        print(f"   âš¡ å³°å€¼ç½‘é€Ÿ: {final_cache_stats.get('peak_network_speed_mbps', 0):.1f}MB/s")
        print(f"   ğŸ’¨ i9ä¼˜åŒ–: {'å·²å¯ç”¨' if hardware_info.get('is_i9', False) else 'æœªå¯ç”¨'}")
        print("-" * 50)
        
        # æ˜¾ç¤ºæ•°æ®åº“æ’å…¥ç»Ÿè®¡
        video_record_manager = hardware_info.get('video_record_manager')
        if video_record_manager and ENABLE_MULTI_COMPUTER_SYNC:
            db_stats = video_record_manager.get_db_insert_statistics()
            print(f"ğŸ—„ï¸ æ•°æ®åº“æ“ä½œç»Ÿè®¡:")
            print(f"   âœ… æˆåŠŸæ’å…¥æ•°æ®åº“: {db_stats['success_count']} ä¸ªè§†é¢‘ ({db_stats['success_rate']:.1f}%)")
            print(f"   âŒ æ’å…¥å¤±è´¥:       {db_stats['failed_count']} ä¸ªè§†é¢‘")
            print(f"   ğŸ“Š æ€»è®¡å°è¯•:       {db_stats['total_count']} æ¬¡æ“ä½œ")
        else:
            print(f"ğŸ—„ï¸ æ•°æ®åº“æ“ä½œç»Ÿè®¡: æ•°æ®åº“åŠŸèƒ½æœªå¯ç”¨æˆ–ä¸å¯ç”¨")
        print("=" * 50)
        
        # æ˜¾ç¤ºå¤±è´¥æ–‡ä»¶è¯¦æƒ…
        failed_stats = progress_manager.get_statistics()
        if failed_stats['failed'] > 0:
            print(f"\nâŒ å¤±è´¥æ–‡ä»¶è¯¦æƒ…:")
            for fail_info in progress_manager.progress_data['failed']:
                if isinstance(fail_info, dict):
                    print(f"  - {fail_info.get('name', 'Unknown')}: {fail_info.get('error', 'Unknown error')}")
        
        # æ¸…ç†å´©æºƒæ¢å¤æ–‡ä»¶ï¼ˆæ­£å¸¸å®Œæˆï¼‰
        progress_manager.clear_crash_recovery()
        print("ğŸ”„ å·²æ¸…ç†å´©æºƒæ¢å¤æ•°æ®")
        
    except KeyboardInterrupt:
        print(f"\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        progress_manager.save_crash_recovery("", roi)  # ä¿å­˜ä¸­æ–­çŠ¶æ€
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logging.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
        progress_manager.save_crash_recovery("", roi)  # ä¿å­˜é”™è¯¯çŠ¶æ€
    finally:
        # æ¸…ç†æ•°æ®åº“ç³»ç»Ÿèµ„æº
        if db_system_manager:
            print("ğŸ”§ æ­£åœ¨æ¸…ç†æ•°æ®åº“ç³»ç»Ÿèµ„æº...")
            db_system_manager.cleanup_resources()

if __name__ == '__main__':
    main()